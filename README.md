# TIL
- 기록은 모든것의 기본이다. 보고 배운것과 해본것들을 꾸준히 남기도록 한다.
# Principles
- 양이 적더라도 매일 업데이트 하려고 노력하자. 꾸준함이 중요하다.  
- 다시 보았을 때 불편함이 없도록 명료하게 작성한다.
- 이론본다고 코드공부도 게을리하지 않기~!




# 정리필요 목록 : Last updated 2022/01/11
```
(2021) StyTr^2: Unbiased Image Style Transfer with Transformers  
(2021) GPEN : GAN Prior Embedded Network for Blind Face Restoration in the Wild
(2020) NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis  
  
(2020) Neural Head Reenactment with Latent Pose Descriptors
(2020) First Order Model for Image Animation
(2020) One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing 
(2020) DataAugmentation : Fair Attribute Classification through Latent Space De-biasing
  
Face swap/reenactment
(2018) RSGAN: Face Swapping and Editing using Face and Hair Representation in Latent Spaces
(2018) ReenactGAN: Learning to Reenact Faces via Boundary Transfer
(2016) Face2Face: Real-time Face Capture and Reenactment of RGB Videos

(2016) Loss Functions for Image Restoration with Neural Networks ; L1 vs L2 vs SSIM family
```

## Neural Rendering
```
(2020) State of the Art on Neural Rendering
```
## 3DMM
```
(2020) StyleRig : Rigging StyleGAN for 3D Control over Portrait Images
(1999) A Morphable Model For The Synthesis Of 3D Faces
```

## Anomaly Detection
```
(2019) OCGAN: One-class Novelty Detection Using GANs with Constrained Latent
(2018) DeepAnT: A Deep Learning Approach for Unsupervised Anomaly Detection in Time Series
(2018) GANomaly : Semi-Supervised Anomaly Detection via Adversarial Training
(2017) AnoGAN : Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery  
```
## Battery
```
(2019) Data-driven health estimation and lifetime prediction of lithium-ion batteries: A review
```
## CAM
```
(2020) Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias
(2016) Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization
(2015) CAM : Learning Deep Features for Discriminative Localization
```
## Classification
```
(2020) How Much Position Information Do Convolutional Neural Networks Encode?
(2018) ArcFace: Additive Angular Margin Loss for Deep Face Recognition
```
## Colorization
```
(2017) RTUG : Real-Time User-Guided Image Colorization with Learned Deep Priors
```
## Data Augmentation
```
(2021) StyleMix : Separating Content and Style for Enhanced Data Augmentation
```
## FaceSwap
```
(2021) HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping
(2021) SimSwap: An Efficient Framework For High Fidelity Face Swapping
(2020) DeepFaceLab: Integrated, flexible and extensible face-swapping framework
(2019) FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping
(2019) FSGAN: Subject Agnostic Face Swapping and Reenactment

(2017) On Face Segmentation, Face Swapping, and Face Perception
```
## Generative Model
```
(2014) Generative Adversarial Networks  
```
## I2I translation
```
(2021) Not just Compete, but Collaborate: Local Image-to-Image Translation via Cooperative Mask Prediction
(2020) AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks
(2020) GANHopper : Multi-Hop GAN for Unsupervised Image-to-Image Translation
(2019) U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for I2I Translation
(2019) StarGAN v2: Diverse Image Synthesis for Multiple Domains
(2019) AMGAN : Attribute Manipulation Generative Adversarial Networks for Fashion Images
(2018) StarGAN : Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation
(2018) Ganimorph : Improving Shape Deformation in Unsupervised I2I Translation
(2017) CycleGAN : Unpaired Image-to-Image Translation using Cycle-Consistent
```
## Image Synthesis
```
(2021) StyleGAN v3 : Alias-Free Generative Adversarial Networks
(2021) StyleMapGAN : Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing : TBD
(2020) A U-Net Based Discriminator for Generative Adversarial Networks
(2020) StyleGAN v2 : Analyzing and Improving the Image Quality of StyleGAN
(2019) StyleGAN v1 : A Style-Based Generator Architecture for Generative Adversarial Networks
(2019) MSGAN : Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis
(2019) MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks 
(2018) PGGAN : Progressive Growing of GANs for Improved Quality, Stability, and Variation
(2016) Improved Techniques for Training GANs : **TBD**
```
## LypSync
```
(2017) Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion : TBD
```
## Normalization
```
(2016) Layer Normalization
```
## 3D Human Pose Estimation
```
(2022) MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video
(2021) Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation
(2021) MoVNect : Lightweight 3D Human Pose Estimation Network Training Using Teacher-Student Learning
(2017) VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera
(2006) Recovering 3D Human Pose from Monocular Images

```
## Self Attention
```
(2018) CBAM: Convolutional Block Attention Module
(2018) BAM: Bottleneck Attention Module” , in BMVC 2018
```
## Syle Transfer
```
(2021) StyTr^2: Unbiased Image Style Transfer with Transformers
(2017) Arbitrary Style Transfer in Real-time with Adaptive Instance
(2016) Image Style Transfer Using Convolutional Neural Networks
(2001) Image Analogies
```
## Time Series
```
(2019) TimeGAN : Time-series Generative Adversarial Networks
(2017) RCGAN : REAL-VALUED (MEDICAL) TIME SERIES GENERATION WITH RECURRENT CONDITIONAL GANS
```
## WSSS
```
(2020) Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering
```

## Attention
```
(2019) Stand-Alone Self-Attention in Vision Models
```


## Object Detection
```
(2021) Dynamic Head: Unifying Object Detection Heads with Attentions
```

## Dataset
```
(2017) VGGFace2: A dataset for recognising faces across pose and age
```

## Animation
```
(2020) First Order Motion Model for Image Animation
```

## Novel View Synthesis
```
(2020) NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
```





# 2. 그외 이론및 모델

[Graphical Model?][b_link001] : 상태를 가지는 모델에서 directed/indirected graphical model의 개념이 자주 등장한다.  
[Restricted Boltzmann Machine][b_link002] : 깊은 신경망에서 학습이 잘 되지 않는 문제를 해결하기 위해 Geoffrey Hinton 교수님이 제안하신 방법론  
Gradient vanishing을 사전학습으로 풀어낸다. 이를 통해 DL이 다시 활기를 되찾았다. Generative 계열을 이해하기 위해서는 이해 필수
[MCMC(Monte Carlo Markov Chain)][b_link003] : 샘플링 방법론
[Pytorch Manual][b_link004] : 파이토치 사용매뉴얼
 # Text book 
 [Machine Learning : A Probabilistic Perspective][t_link001] : ML의 바이블이라고 생각하는 책이다. 언젠간 보고 정리해야겠다고 생각했는데, 언제 다볼 수 있을지... 

# 3. Coursera
<p>
 <img src="https://user-images.githubusercontent.com/40943064/147326092-656e97b0-c871-4a7a-a54d-caab6241c2a7.png" width=350 />
 <img src="https://user-images.githubusercontent.com/40943064/147326779-0ef9aa86-9573-4d5c-b8cc-12353b5ed655.png" width=352 />
</p>


[j_link001]: <https://arxiv.org/pdf/1508.06576.pd>
[j_link002]: <https://ieeexplore.ieee.org/document/8581424>
[j_link003]: <https://ieeexplore.ieee.org/document/9171158>
[j_link004]: <https://www.technicaljournalsonline.com/ijeat/VOL%20V/IJAET%20VOL%20V%20ISSUE%20I%20JANUARY%20MARCH%202014/IJAETVol%20V%20Issue%20I%20Article%207.pdf>
[j_link005]: <https://ieeexplore.ieee.org/document/1542519>
[j_link006]: <https://arxiv.org/abs/1711.04322>
[j_link007]: <https://github.com/kyugorithm/TIL/blob/main/journal/PG_GAN.md>
[j_link008]: <https://github.com/kyugorithm/TIL/blob/main/journal/J006_cycleGAN.md>
[j_link008]: <https://dl.acm.org/doi/abs/10.1145/3414685.3417803>
[j_link009]: <https://arxiv.org/pdf/2004.00121.pdf>
[j_link010]: <https://github.com/kyugorithm/TIL/blob/main/journal/J007_RTUG.md>
[b_link001]: <https://medium.com/@chullino/graphical-model%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80%EC%9A%94-2d34980e6d1f>
[b_link002]: <https://github.com/kyugorithm/TIL/blob/main/Theory/RestrictedBoltzmannMachine.md>
[b_link003]: <https://github.com/kyugorithm/TIL/blob/main/Theory/MCMC.md>
[b_link003]: <https://github.com/kyugorithm/TIL/blob/main/ML_APP.md>
[b_link004]: <https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#nn-module>

[nam]: <https://github.com/namjunemy/TIL#%EC%9E%91%EC%84%B1-%EA%B7%9C%EC%B9%99>
[VL_낭독체_001.zip](https://github.com/user-attachments/files/16045170/VL_._001.zip)



해당 문서의 내용을 개선하여 더 명확하고 구조화된 형태로 재작성해드리겠습니다:

# Metrics of Episode Detection

## 배경
우리는 콘텐츠 운영팀의 자동화 워크플로우를 개선하기 위해 ML 작업을 수행합니다. Eyeball QC를 대체할 수단으로서 Episode matching과 Media Operation 작업을 일부 자동화하는 데 초점을 맞추고 있습니다. 이러한 ML 작업의 성과를 정확하게 평가하기 위해서는 적절한 평가 지표(metrics)가 필수적입니다.

## 현재 문제점과 해결 방안
1. **비디오 내 에피소드 감지 문제**
   - 현재는 burn-in된 에피소드 번호와 비디오 콘텐츠 내 실제 에피소드가 불일치하는 경우가 있음
   - ML 모델을 활용하여 비디오 콘텐츠의 이미지와 텍스트를 종합적으로 분석
   - 텍스트 기반 에피소드 번호 예측과 비교 검증 수행

2. **시스템 평가 방법**
   - 시스템 성능에 따라 결과를 부분적으로 신뢰할 수 있음
   - 다양한 케이스에서 비디오 내 텍스트 추출 정확도 검증 필요
   - 차단된 것으로 확인된 결과는 재검토가 필요하지 않음

3. **메타정보 검증 프로세스**
   - 주어진 메타정보의 최초 정보와 유사한지의 관계 검증
   - 독립성을 가진 검증 단계 구축:
     a. 동영상 내의 검출된 최차정보
     b. 과거의 최차정보
     c. 두개 정보의 일치 여부
   - 두 가지 방법에는 나눠진 차이가 있을 수 있으므로 순차적 개발 접근

## 평가 지표 활용
위에서 설명한 방법들의 성능을 정확하게 측정하고 비교하기 위해 명확한 평가 지표를 사용하며, 이를 통해 시스템의 신뢰성과 효율성을 지속적으로 모니터링합니다.

이렇게 구조화하고 내용을 보완하면 독자들이 더 쉽게 이해할 수 있으며, 각 섹션별로 중요 포인트를 명확하게 파악할 수 있습니다. 전문 용어들도 좀 더 일관성 있게 사용하였습니다. 필요하다면 특정 섹션에 대해 더 자세한 설명을 추가할 수 있습니다.​​​​​​​​​​​​​​​​


다음은 지금까지 논의한 내용을 바탕으로 포스터 이미지 검출 모델에 대한 평가 방식을 체계적으로 정리한 것입니다.

1. 데이터 선정 방식

	•	테스트셋 조건: 비디오에서 전문가가 5장 이상의 이미지를 선택한 경우에만 해당 비디오를 테스트셋에 포함합니다.
	•	테스트셋 구성: 5장 이상의 정답 이미지가 있는 비디오로만 테스트셋을 구성하여, 평가의 공정성과 일관성을 유지합니다.

2. 정답 산출 방식

	•	전문가 기준 정답 이미지 세트:
	•	각 비디오에 대해 전문가가 선택한 이미지들을 정답 이미지 세트로 설정합니다.
	•	정답 이미지 수는 비디오마다 다를 수 있지만, 최소 5장 이상의 이미지가 포함됩니다.
	•	모델 이미지 추출:
	•	각 비디오에서 모델이 최대 TOP-10 이미지를 추출하여 평가합니다.
	•	모델이 추출하는 이미지는 모든 비디오에서 동일하게 10장으로 고정하여 평가의 일관성을 확보합니다.
	•	매칭 기준:
	•	모델이 선택한 이미지와 전문가 정답 이미지 간의 시간적 근접성과 유사도 기준을 모두 만족해야 정답으로 평가합니다.
	•	시간적 근접성: 모델 이미지와 전문가 이미지가 앞뒤 3초 이내에 위치해야 합니다.
	•	이미지 유사도: 모델 이미지와 전문가 이미지의 코사인 유사도가 0.5 이상이어야 합니다.

3. 리콜 계산 방식 (TOP-K Recall)

	•	TOP-10 Recall 정의:
	•	모델이 각 비디오에서 추출한 TOP-10 이미지 중 적어도 한 장이 전문가 정답 이미지 세트와 매칭되는 경우 해당 비디오를 정답으로 평가합니다.
	•	TOP-10 Recall 계산:
	•	테스트셋 내 모든 비디오에 대해 모델이 정답으로 평가된 비디오 수를 전체 비디오 수로 나누어 TOP-10 Recall을 계산합니다.
	•	TOP-10 Recall = (정답으로 평가된 비디오 수) / (테스트셋 전체 비디오 수)

요약

	1.	데이터 선정: 전문가가 5장 이상의 이미지를 선택한 비디오로 테스트셋을 구성.
	2.	정답 산출: 모델은 모든 비디오에서 TOP-10 이미지를 예측하고, 시간적 근접성(3초 이내)과 유사도(코사인 유사도 0.5 이상) 기준을 모두 만족하는 이미지를 정답으로 인정.
	3.	리콜 계산: TOP-10 Recall을 사용하여, 각 비디오에서 모델이 한 장이라도 정답과 매칭되는 경우 정답으로 평가.

이 평가 방식은 비디오의 편차를 최소화하면서 모델의 포스터 이미지 검출 성능을 신뢰성 있게 평가할 수 있는 기준을 제공합니다.

