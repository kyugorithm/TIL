# TIL
- 기록은 모든것의 기본이다. 보고 배운것과 해본것들을 꾸준히 남기도록 한다.
# Principles
- 양이 적더라도 매일 업데이트 하려고 노력하자. 꾸준함이 중요하다.  
- 다시 보았을 때 불편함이 없도록 명료하게 작성한다.
- 이론본다고 코드공부도 게을리하지 않기~!




# 정리필요 목록 : Last updated 2022/01/11
```
(2021) StyTr^2: Unbiased Image Style Transfer with Transformers  
(2021) GPEN : GAN Prior Embedded Network for Blind Face Restoration in the Wild
(2020) NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis  
  
(2020) Neural Head Reenactment with Latent Pose Descriptors
(2020) First Order Model for Image Animation
(2020) One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing 
(2020) DataAugmentation : Fair Attribute Classification through Latent Space De-biasing
  
Face swap/reenactment
(2018) RSGAN: Face Swapping and Editing using Face and Hair Representation in Latent Spaces
(2018) ReenactGAN: Learning to Reenact Faces via Boundary Transfer
(2016) Face2Face: Real-time Face Capture and Reenactment of RGB Videos

(2016) Loss Functions for Image Restoration with Neural Networks ; L1 vs L2 vs SSIM family
```

## Neural Rendering
```
(2020) State of the Art on Neural Rendering
```
## 3DMM
```
(2020) StyleRig : Rigging StyleGAN for 3D Control over Portrait Images
(1999) A Morphable Model For The Synthesis Of 3D Faces
```

## Anomaly Detection
```
(2019) OCGAN: One-class Novelty Detection Using GANs with Constrained Latent
(2018) DeepAnT: A Deep Learning Approach for Unsupervised Anomaly Detection in Time Series
(2018) GANomaly : Semi-Supervised Anomaly Detection via Adversarial Training
(2017) AnoGAN : Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery  
```
## Battery
```
(2019) Data-driven health estimation and lifetime prediction of lithium-ion batteries: A review
```
## CAM
```
(2020) Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias
(2016) Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization
(2015) CAM : Learning Deep Features for Discriminative Localization
```
## Classification
```
(2020) How Much Position Information Do Convolutional Neural Networks Encode?
(2018) ArcFace: Additive Angular Margin Loss for Deep Face Recognition
```
## Colorization
```
(2017) RTUG : Real-Time User-Guided Image Colorization with Learned Deep Priors
```
## Data Augmentation
```
(2021) StyleMix : Separating Content and Style for Enhanced Data Augmentation
```
## FaceSwap
```
(2021) HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping
(2021) SimSwap: An Efficient Framework For High Fidelity Face Swapping
(2020) DeepFaceLab: Integrated, flexible and extensible face-swapping framework
(2019) FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping
(2019) FSGAN: Subject Agnostic Face Swapping and Reenactment

(2017) On Face Segmentation, Face Swapping, and Face Perception
```
## Generative Model
```
(2014) Generative Adversarial Networks  
```
## I2I translation
```
(2021) Not just Compete, but Collaborate: Local Image-to-Image Translation via Cooperative Mask Prediction
(2020) AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks
(2020) GANHopper : Multi-Hop GAN for Unsupervised Image-to-Image Translation
(2019) U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for I2I Translation
(2019) StarGAN v2: Diverse Image Synthesis for Multiple Domains
(2019) AMGAN : Attribute Manipulation Generative Adversarial Networks for Fashion Images
(2018) StarGAN : Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation
(2018) Ganimorph : Improving Shape Deformation in Unsupervised I2I Translation
(2017) CycleGAN : Unpaired Image-to-Image Translation using Cycle-Consistent
```
## Image Synthesis
```
(2021) StyleGAN v3 : Alias-Free Generative Adversarial Networks
(2021) StyleMapGAN : Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing : TBD
(2020) A U-Net Based Discriminator for Generative Adversarial Networks
(2020) StyleGAN v2 : Analyzing and Improving the Image Quality of StyleGAN
(2019) StyleGAN v1 : A Style-Based Generator Architecture for Generative Adversarial Networks
(2019) MSGAN : Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis
(2019) MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks 
(2018) PGGAN : Progressive Growing of GANs for Improved Quality, Stability, and Variation
(2016) Improved Techniques for Training GANs : **TBD**
```
## LypSync
```
(2017) Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion : TBD
```
## Normalization
```
(2016) Layer Normalization
```
## 3D Human Pose Estimation
```
(2022) MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video
(2021) Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation
(2021) MoVNect : Lightweight 3D Human Pose Estimation Network Training Using Teacher-Student Learning
(2017) VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera
(2006) Recovering 3D Human Pose from Monocular Images

```
## Self Attention
```
(2018) CBAM: Convolutional Block Attention Module
(2018) BAM: Bottleneck Attention Module” , in BMVC 2018
```
## Syle Transfer
```
(2021) StyTr^2: Unbiased Image Style Transfer with Transformers
(2017) Arbitrary Style Transfer in Real-time with Adaptive Instance
(2016) Image Style Transfer Using Convolutional Neural Networks
(2001) Image Analogies
```
## Time Series
```
(2019) TimeGAN : Time-series Generative Adversarial Networks
(2017) RCGAN : REAL-VALUED (MEDICAL) TIME SERIES GENERATION WITH RECURRENT CONDITIONAL GANS
```
## WSSS
```
(2020) Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering
```

## Attention
```
(2019) Stand-Alone Self-Attention in Vision Models
```


## Object Detection
```
(2021) Dynamic Head: Unifying Object Detection Heads with Attentions
```

## Dataset
```
(2017) VGGFace2: A dataset for recognising faces across pose and age
```

## Animation
```
(2020) First Order Motion Model for Image Animation
```

## Novel View Synthesis
```
(2020) NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
```





# 2. 그외 이론및 모델

[Graphical Model?][b_link001] : 상태를 가지는 모델에서 directed/indirected graphical model의 개념이 자주 등장한다.  
[Restricted Boltzmann Machine][b_link002] : 깊은 신경망에서 학습이 잘 되지 않는 문제를 해결하기 위해 Geoffrey Hinton 교수님이 제안하신 방법론  
Gradient vanishing을 사전학습으로 풀어낸다. 이를 통해 DL이 다시 활기를 되찾았다. Generative 계열을 이해하기 위해서는 이해 필수
[MCMC(Monte Carlo Markov Chain)][b_link003] : 샘플링 방법론
[Pytorch Manual][b_link004] : 파이토치 사용매뉴얼
 # Text book 
 [Machine Learning : A Probabilistic Perspective][t_link001] : ML의 바이블이라고 생각하는 책이다. 언젠간 보고 정리해야겠다고 생각했는데, 언제 다볼 수 있을지... 

# 3. Coursera
<p>
 <img src="https://user-images.githubusercontent.com/40943064/147326092-656e97b0-c871-4a7a-a54d-caab6241c2a7.png" width=350 />
 <img src="https://user-images.githubusercontent.com/40943064/147326779-0ef9aa86-9573-4d5c-b8cc-12353b5ed655.png" width=352 />
</p>


[j_link001]: <https://arxiv.org/pdf/1508.06576.pd>
[j_link002]: <https://ieeexplore.ieee.org/document/8581424>
[j_link003]: <https://ieeexplore.ieee.org/document/9171158>
[j_link004]: <https://www.technicaljournalsonline.com/ijeat/VOL%20V/IJAET%20VOL%20V%20ISSUE%20I%20JANUARY%20MARCH%202014/IJAETVol%20V%20Issue%20I%20Article%207.pdf>
[j_link005]: <https://ieeexplore.ieee.org/document/1542519>
[j_link006]: <https://arxiv.org/abs/1711.04322>
[j_link007]: <https://github.com/kyugorithm/TIL/blob/main/journal/PG_GAN.md>
[j_link008]: <https://github.com/kyugorithm/TIL/blob/main/journal/J006_cycleGAN.md>
[j_link008]: <https://dl.acm.org/doi/abs/10.1145/3414685.3417803>
[j_link009]: <https://arxiv.org/pdf/2004.00121.pdf>
[j_link010]: <https://github.com/kyugorithm/TIL/blob/main/journal/J007_RTUG.md>
[b_link001]: <https://medium.com/@chullino/graphical-model%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80%EC%9A%94-2d34980e6d1f>
[b_link002]: <https://github.com/kyugorithm/TIL/blob/main/Theory/RestrictedBoltzmannMachine.md>
[b_link003]: <https://github.com/kyugorithm/TIL/blob/main/Theory/MCMC.md>
[b_link003]: <https://github.com/kyugorithm/TIL/blob/main/ML_APP.md>
[b_link004]: <https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#nn-module>

[nam]: <https://github.com/namjunemy/TIL#%EC%9E%91%EC%84%B1-%EA%B7%9C%EC%B9%99>
[VL_낭독체_001.zip](https://github.com/user-attachments/files/16045170/VL_._001.zip)





import random
from itertools import product

def generate_episode_texts(num_samples):
    # 시즌 범위 확장
    season = random.randint(1, 30)
    
    # 더 다양한 접두사 패턴
    basic_prefixes = ["", "제", "에피소드 ", "Episode ", "Ep. ", "Epi ", "EP", "E", "#"]
    season_formats = [
        f"S{season}E",
        f"Season {season} Episode ",
        f"시즌{season} 에피소드 ",
        f"시즌 {season} ",
        f"{season}기 ",
        f"시즌 {season:02d} ",
        f"Season {season:02d} Ep. ",
        f"S{season:02d}E",
        f"{season}시즌 ",
        f"제{season}시즌 "
    ]
    prefixes = basic_prefixes + season_formats
    
    # 숫자 범위 및 형식 확장
    numbers = []
    for i in range(1, 201):  # 확장된 에피소드 범위
        numbers.extend([
            str(i),
            f"{i:02d}",  # 01, 02, ...
            f"{i:03d}",  # 001, 002, ...
            str(i) + "화",
            f"{i:02d}화",
            f"{i:03d}화"
        ])
    
    # 접미사 확장
    suffixes = [
        "화", "편", "", 
        "번째 이야기", "번째 에피소드", "번째 편", "편째",
        "회", "부", "절", "장",
        " Part", " Story", " Chapter",
        "-1", "-A", " (상)", " (하)",
        " - 첫번째", " - 두번째"
    ]
    
    # 특별편 포맷 추가
    special_formats = [
        "특별편 ",
        "Special ",
        "SP ",
        "외전 ",
        "Prologue ",
        "Epilogue ",
        "OVA ",
        "예고편 "
    ]
    
    episode_texts = []
    for _ in range(num_samples):
        if random.random() < 0.1:  # 10% 확률로 특별편 형식 사용
            episode_text = random.choice(special_formats) + str(random.randint(1, 10))
        else:
            prefix = random.choice(prefixes)
            number = random.choice(numbers)
            suffix = random.choice(suffixes)
            
            # 다양한 구분자 추가
            separator = random.choice(["", " ", "-", ".", "_"])
            episode_text = f"{prefix}{separator}{number}{suffix}"
            
            # 부가 정보 추가 (20% 확률)
            if random.random() < 0.2:
                additional_info = random.choice([
                    " (재방송)",
                    " (본방송)",
                    " (신작)",
                    " (완결편)",
                    " [HD]",
                    " [4K]",
                    " [자막]"
                ])
                episode_text += additional_info
        
        episode_texts.append(episode_text)
    
    return episode_texts

def generate_rating_texts(num_samples):
    age_ratings = ["전체", "7세", "12세", "15세", "18세", "19금"]
    content_warnings = [
        "폭력성",
        "선정성",
        "언어사용",
        "공포",
        "약물",
        "차별",
        "모방위험"
    ]
    
    detail_phrases = [
        "시청가능",
        "이용가",
        "관람가",
        "등급",
        "허용",
        "권장"
    ]
    
    descriptors = [
        "다소",
        "매우",
        "경미한",
        "심각한",
        "일부",
        "포함"
    ]
    
    rating_texts = []
    for _ in range(num_samples):
        if random.random() < 0.3:  # 단순 등급
            text = random.choice(age_ratings)
        else:
            # 복합 등급 설명 생성
            age = random.choice(age_ratings)
            
            if random.random() < 0.5:  # 기본 등급 표현
                phrase = random.choice(detail_phrases)
                text = f"{age} {phrase}"
            else:  # 상세 경고 포함
                warnings = random.sample(content_warnings, random.randint(1, 3))
                descriptor = random.choice(descriptors)
                warning_text = ", ".join(warnings)
                text = f"{age} 이상 - {descriptor} {warning_text} 요소 포함"
                
                # 부가 설명 추가 (30% 확률)
                if random.random() < 0.3:
                    additional = random.choice([
                        "보호자 시청 권장",
                        "주의 필요",
                        "시청 전 확인 필요",
                        "보호자와 함께 시청 권장"
                    ])
                    text += f" ({additional})"
        
        rating_texts.append(text)
    
    return rating_texts

def generate_previous_transition_texts(num_samples):
    korean_prefixes = [
        "이전", "지난", "전편", "앞편", "저번",
        "직전", "바로 전", "이전의", "지난번"
    ]
    
    english_prefixes = [
        "Previous", "Last Time", "Previously On",
        "Last Episode", "Earlier", "Before",
        "Prior Episode", "Preceding"
    ]
    
    episode_terms = [
        "Episode", "Ep.", "Story", "Part", "편",
        "화", "이야기", "에피소드", "회차", "파트"
    ]
    
    time_expressions = [
        "지난주", "저번주", "일주일 전", "전주",
        "지난달", "저번달", "한달 전", "전월",
        "작년", "작시즌", "이전 시즌"
    ]
    
    transition_texts = []
    for _ in range(num_samples):
        if random.random() < 0.4:  # 단순 표현
            text = random.choice(korean_prefixes + english_prefixes)
        else:
            # 복합 표현 생성
            if random.random() < 0.5:
                prefix = random.choice(korean_prefixes + english_prefixes)
                term = random.choice(episode_terms)
                text = f"{prefix} {term}"
            else:
                time_expr = random.choice(time_expressions)
                term = random.choice(episode_terms)
                text = f"{time_expr} {term}"
            
            # 부가 정보 추가 (20% 확률)
            if random.random() < 0.2:
                additional = random.choice([
                    "요약", "하이라이트", "다시보기",
                    "Recap", "Summary", "Review"
                ])
                text += f" ({additional})"
        
        transition_texts.append(text)
    
    return transition_texts

def generate_next_transition_texts(num_samples):
    korean_prefixes = [
        "다음", "이어서", "계속", "후편", "다음편",
        "뒤편", "차회", "다음번", "이후"
    ]
    
    english_prefixes = [
        "Next", "Coming Up", "Up Next",
        "Following", "Continues", "Next Time",
        "Later", "Upcoming"
    ]
    
    episode_terms = [
        "Episode", "Ep.", "Story", "Part", "편",
        "화", "이야기", "에피소드", "회차", "파트"
    ]
    
    preview_terms = [
        "예고", "미리보기", "스포일러",
        "Preview", "Teaser", "Sneak Peek"
    ]
    
    time_hints = [
        "곧", "잠시후", "다음주", "다음달",
        "Soon", "Shortly", "Next Week",
        "내일", "모레", "다음 방송"
    ]
    
    transition_texts = []
    for _ in range(num_samples):
        if random.random() < 0.3:  # 단순 표현
            text = random.choice(korean_prefixes + english_prefixes)
        else:
            # 복합 표현 생성
            components = []
            
            if random.random() < 0.6:
                components.append(random.choice(time_hints))
            
            components.append(random.choice(korean_prefixes + english_prefixes))
            
            if random.random() < 0.7:
                components.append(random.choice(episode_terms))
            
            if random.random() < 0.3:
                components.append(random.choice(preview_terms))
            
            text = " ".join(components)
            
            # 부가 정보 추가 (25% 확률)
            if random.random() < 0.25:
                additional = random.choice([
                    "기대해주세요",
                    "Don't Miss It",
                    "놓치지 마세요",
                    "특별편",
                    "최종화"
                ])
                text += f" ({additional})"
        
        transition_texts.append(text)
    
    return transition_texts
