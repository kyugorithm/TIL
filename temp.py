Hello, I have made adjustments in poster validation to address color discrepancies between the original source images and newly generated images. Poster images often use the Adobe RGB color space, whereas the standard is typically the sRGB color space. This mismatch in color spaces tends to make colors appear darker. To resolve this issue, I implemented a process to read the color profile from the image. If it uses a color space like Adobe RGB, I applied a transformation to convert it properly to sRGB, processed the entire image, and then reverted it back to Adobe RGB through inverse transformation. This ensures the image retains its original color space and displays correctly.

Regarding episode detection, I have shared the test results with the MO team and am running batch tasks. I am also working on video quality assessment using a Video Quality Assessment (VQA) model to evaluate the quality of video shots. The model has two branches: one for assessing aesthetic quality and the other for evaluating technical quality. For the technical quality branch, I extract features, perform dimensionality reduction using an autoencoder, and analyze the trends in these features. This analysis will help investigate pixel errors and other issues provided by the MO team.

Additionally, an idea suggested by Rohee was to use FFmpeg for video transcoding, intentionally downscaling the quality to synthesize a test dataset. Using this method, I plan to synthesize test sets and evaluate whether the issues can be detected accurately.
Thatâ€™s all for now.
