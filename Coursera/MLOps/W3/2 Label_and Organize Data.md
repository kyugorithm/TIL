### 순서 : 효율적 데이터 취득 방법 > 데이터 파이프라인 구축 > 

## 1. Obtaining data
**효율적으로 데이터를 취득해라!**  
얼마나 많은 시간을 데이터 취득에 들일 것인가?  
데이터 취득에 매우 많은 시간을 투자 할 것으로 생각하지만 그보다는 모델 개발 과정을 빠르게 반복하는 것이 좋다.  
(초기 모델을 학습하기 위해 상당히 많은 시간을 데이터 취득에 투자하는 경우가 대부분이지만 비효율적이다.)  
특정 갯수의 샘플을 정하고 그에 따른 기간을 추산하기 보다는 취득기간을 먼저 정의하고 반복을 여러번 수행하라  
과거에 유사 프로젝트를 수행해 보았다면 특정 규모의 데이터를 취득하기 위해 시간을 쏟는것이 가치 있다.  
![image](https://user-images.githubusercontent.com/40943064/147309157-ba40cafe-ae81-44ce-9c32-602e2f82e01b.png)  

**어디에서 데이터를 취득할 것인가?**  
가장 효율적으로 취득할 수 있는 방법을 고민해라  
가능한 방법들을 나열하고 우선순위를 정해라  
![image](https://user-images.githubusercontent.com/40943064/147316061-0ada305e-60ad-4f6c-bede-ce6018067290.png)  

**라벨링은 누가 어떻게 할 것인가?**

데이터 유형에 따라 효율적으로 데이터를 처리할 수 있는 전문회사가 있을 수 있다. 
일반적으로 ML 엔지니어가 라벨링하면 비용이 많이들고 효율성이 떨어지기 때문에, 보통 짧은 시간만 할애하는 경우가 많다.  
특정 시점을 넘으서면서 확장 가능한 라벨링 프로세스를 획득하고자 할 수있다.  
  
라벨링을 더 잘하는 특정 유형을 구분할  수도 있다.  
음성 인식의 경우 유창한 화자가 더 잘 할 수 있고 다양한 언어의 경우 특정 언어를 잘 구사하는 사람이 더 잘 할 수 있고  
공장, 의료 이미지의 경우 전문화된 어플리케이션에 해당하며 SME 즉, 주제관련 전문가가 필요하다.  
어떤 경우에는 라벨링 자체가 어려운 경우 추천시스템을 활용할 수도 있다. 
즉, 라벨링을 잘 할 수 있는 유형을 고민하고 선정하는것이 중요하다.  

데이터를 확장 할 때, 10배 이상의 데이터를 획득하기 위해 자원을 할애하는것은 비효율 적이다.  
앞서 이야기 한것 처럼 모델 개발 프로세스를 빠르게 반복하는것이 더 효율적이다.  
다만 이 상한선 안에서 자유롭게 합리적인 수준으로 데이터를 증가시키도록 한다.  

## 2. Data pipeline
**재현성에 대하여 어디에 얼마나 투자 해야하는가?**  
사용자 정보 데이터를 활용하는 상황에 대한 데이터 파이프라인 사례(사용자 정보 -> 사용자가 직장을 찾고 있는지 여부)  
![image](https://user-images.githubusercontent.com/40943064/147318617-ad7dc20b-f0f4-4e5b-b47a-27fc3f73844c.png)  

ML을 학습 시키기 위해 데이터를 처리하는데 그 과정에는  spam 아이디를 처리하고 uer ID를 통합하는 순서로 이루어 질 수 있다.  
위 두개의 작업이 script를 통해 구현되었다고 해보자.  
이 때 발생할 수 있는 문제는 생산 시스템에 이 프로그램을 적용시 발생할 수 있을 재현성 이슈이다.  
![image](https://user-images.githubusercontent.com/40943064/147319408-a1b85d09-c17b-4423-9783-3ba62c3edac8.png)

ML 입력 분포가 개발 데이터와 생산 데이터에 동일한지 확인하기 위해 스트립을 어떻게 재현할 것인가?  
이러한 재현성을 달성하기 위해 필요한 노력은 프로젝트의 유형에 따라 매우 다르며  
항상 100% 복제가 가능해야한다고 말하는 것은 매우 비효율 적이다.  
꽤 많은 경우 PoC 프로젝트를 수행하며 단지 가능성을 확인하는 경우도 많다.  
이때, 목표는 단지 prototype을 얻는것이다.  
이 경우 수작업으로 데이터를 처리해도 괜찮으며 단지 과정을 통해 얻은 결과를 잘 정리하는것이 매우 가치있는 일이다.  
우선 다음단계로 넘어갈지에 대한 결정을 한 후 진짜 재현성을 달성하기 위한 노력이 필요하다.  
이때, Tensorflow Transform, Apache beam, Airflow등과 같은 가치있는 도구를 사용할 수 있다.  

![image](https://user-images.githubusercontent.com/40943064/147319421-69614dbb-cc11-4904-a281-d2798dddfde3.png)  

## 3. Meta-data, data provenance and lineage
메타 데이타, 데이터 출처, 계통을 획득하는것이 과제따라 큰 도움이 될 수 있다.  
![image](https://user-images.githubusercontent.com/40943064/147322972-98fb4500-a47e-47ee-8f1b-b90d10ecb27c.png)  
데이터 파이프라인은 위 그림처럼 application에 따라 매우 경우가 대부분이다.  
이때, 데이터셋에서 스팸 list가 여러 이유로 잘못 구성되는 경우 모든 문제의 흐름 안에서 그 원인을 찾기 매우 어려울 수 있다.  
이 때, 데이터의 출처(provenance)와 계통(lineage)를 관리하면 이러한 문제를 발견하는데 큰 도움이 된다.  
고등화된 (Tenforflow Transform과 같은) 툴을 활용할 수 있다.  
문제를 매우 쉽게 만들기 위해 메타 데이터를 사용한다.  
![image](https://user-images.githubusercontent.com/40943064/147323204-4f994711-aa17-45c9-83ed-1e12e9eeac01.png)

### 요약 
유지 관리해야 할 복잡한 대규모 ML 시스템에 대해 데이터 출처 및 계보를 추적하면 작업이 훨씬 쉬워질 수 있다. 
시스템 구축의 일환으로 메타데이터 추적을 고려함으로써 데이터 출처 추적과 오류 분석에 도움이 될 수 있습니다.  

## 4. Balanced train/dev/test splits
데이터의 사이즈가 매우 작은 경우 (약 100개인 경우와 같이)
train/dev/test 셋의 비율이 원본 데이터 셋의 분포를 벗어나도록 샘플링 되는 경우가 있다.  
이러한 경우는 문제를 야기 할 수 있으므로 균형잡힌 분할이 필요하다.  
![image](https://user-images.githubusercontent.com/40943064/147323762-1af40268-8e46-4edd-97e2-eabbe589d91f.png)
