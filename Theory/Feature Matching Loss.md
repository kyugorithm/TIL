Note (**Feature matching loss != Perceptual loss**)


# Feature matching loss
  
## Summary
In GAN-related papers that perform image to image translation tasks, feature matching loss term is often used.  
The cases and explanations used in several papers are summarized to help understand Feature Matching Loss.  
  
---
  
### (ECCV 2019) FUNIT : Few-Shot Unsupervised Image-to-Image Translation
Explanation : The feature matching loss **regularizes** the training.  
Process :  
1. **Df** : Remove the last layer of D and construct feature extractor   
2. Use **Df** to extract features from the translation output **xÂ¯** and the class images {y1, ..., yK} and minimize  

<img src="https://user-images.githubusercontent.com/40943064/142168442-9d1faff8-5e98-4541-a854-0ff4d38114af.png" width="400" height="55">  


Unlike other papers, the target class is set to K, not one.  
Therefore, for each target image, the value obtained through the **Df** is forced to be equal to the average of all other classes's feature.  
Furthermore other papers utilize feature matching loss by using feature values of all layers of D 
This paper's contribution is to extend feature matching loss's use to the more challenging and novel few-shot unsupervised image-to-image translation setting.  
  
---
  
### (CVPR 2018) pix2pix HD : High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
This **stabilizes the training** as **G has to produce natural statistics at multiple scales**.  
Extract features from **multiple layers of D** and learn to match these intermediate representations from the real and the fake.  
For ease of presentation, we denote the ith-layer feature extractor of Dk as D(i)k (from input to the ith layer of Dk).  
The feature matching loss is then calculated as:  
<img src="https://user-images.githubusercontent.com/40943064/142188950-6eb5c6f9-40f0-48f2-8854-65ad7910f5db.png" width="430" height="55">  
To enhance image quality, proposed paper used additional perceptual loss term and this slightly enhanced performance
<img src="https://user-images.githubusercontent.com/40943064/142192229-5592aea4-825d-411e-b445-9c48a475e1fd.png" width="350" height="35">  

---
  
***** Original Paper *****
### (NIPS 2016) Improved Techniques for Training GANs

**Addresses the instability of GANs by specifying a new objective for G** that prevents it from overtraining on the current D.  
Instead of directly maximizing the output of D, this requires G to generate data that **matches the statistics of the real data**,  
where we use D only to specify the statistics that we think are worth matching.  
Specifically, we train G to match the expected value of the **features on an intermediate layer of D**.  
This is a natural choice of statistics for G to match,  
since by training D we ask it to find those features that are most discriminative of real data versus data generated by the current model.  
  
<img src="https://user-images.githubusercontent.com/40943064/142196665-c453c4b0-d29e-4df3-a334-7a449719b7a6.png" width="350" height="35">  
  

--- 
# Perceptual loss
## Summary
It should be understood that Perceptual loss is different in that it extracts loss through other natural networks such as VGG-16.

### (2016) Generating Images with Perceptual Similarity Metrics based on Deep Networks  
Named loss Perceptual Similarity Metrics  
Instead of image space distances, compute distances between features extracted by deep neural networks.  
**C** is feature extractor pretrained for classification task.  
This paper used **L2 loss** for the loss term.  
<img src="https://user-images.githubusercontent.com/40943064/142176306-368c09d2-b1a0-467e-b9f4-6bf897d7b915.png" width="430" height="300">  
<img src="https://user-images.githubusercontent.com/40943064/142176421-152742b1-c882-459e-8533-848f22bbc879.png" width="430" height="150">  

This loss term alone does not provide a good loss for training. It is known (Mahendran & Vedaldi, 2015) that optimizing just  
for similarity in the feature space typically leads to **high-frequency artifacts**.  
This is because for each natural image there are many non-natural images mapped to the same feature vector.  
Therefore, a natural image prior is necessary to constrain the generated images to the manifold of natural image.  
