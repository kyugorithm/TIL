# Feature matching loss
  
## Introduction
In GAN-related papers that perform image to image translation tasks, feature matching loss term is often used.  
The cases and explanations used in several papers are summarized to help understand Feature Matching Loss.  

## (2016) Generating Images with Perceptual Similarity Metrics based on Deep Networks
This paper named feature-matching loss as **Perceptual Similarity Metrics**  
Instead of image space distances, compute distances between image features extracted by deep neural networks.  
![image](https://user-images.githubusercontent.com/40943064/142176306-368c09d2-b1a0-467e-b9f4-6bf897d7b915.png)  
**C** is feature extractor to compare images between real/fake images.  
Furthermore **C** is pretrained for classification task.  
![image](https://user-images.githubusercontent.com/40943064/142176421-152742b1-c882-459e-8533-848f22bbc879.png)  
Alexey Dosovitskiy et al used loss term only for last output.  



## (ECCV 2018) MUNIT : Multimodal Unsupervised Image-to-Image Translation

## (ECCV 2019) FUNIT : Few-Shot Unsupervised Image-to-Image Translation
Explanation : The feature matching loss **regularizes** the training.  
Process :  
1. **Df** : Remove the last layer of D and construct feature extractor   
2. Use **Df** to extract features from the translation output **xÂ¯** and the class images {y1, ..., yK} and minimize  
  
![image](https://user-images.githubusercontent.com/40943064/142168442-9d1faff8-5e98-4541-a854-0ff4d38114af.png)  

Unlike other papers, the target class is set to K, not one.  
Therefore, for each target image, the value obtained through the **Df** is forced to be equal to the average of all other classes's feature.  
Furthermore other papers utilize feature matching loss by using feature values of all layers of Discriminator 
This paper's contribution is to extend feature matching loss's use to the more challenging and novel few-shot unsupervised image-to-image translation setting.  

19
29
37
50
