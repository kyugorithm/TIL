## Abstract. 
인물 오디오 -> (오디오에 싱크가 맞는) 인물 비디오
오디오가 입력되면 Expression(3D face model space)을 뽑고 reenactment를 수행
3D 표현을 통해 모델은 본질적으로 시간 안정성을 학습하면서 rended 마스크로 인물 비디오 프레임을 만드는데 사용도 한다.  
임의 인물에 적용 가능하고 표준 TTS가 생성한 임의 목소리를 적용할 수 있다.

#### Application
- audio-driven video avatars
- video dubbing
- text-driven video synthesis of a talking head

## 1. Introduction
시각적 기반으로 실제 사람의 짧은 대상 비디오를 활용합니다. 
우리 방법의 핵심 구성 요소는 입력 오디오에 맞는 입술 움직임을 추정하고 대상 사람의 모습을 설득력 있는 방식으로 렌더링하는 것입니다. 
오디오-프레임 매핑은 대상 비디오(정렬된 실제 오디오 및 이미지 데이터)에서 수집할 수 있는 실측 정보를 사용하여 훈련됩니다. 
우리는 Neural Voice Puppetry를 사용하기 쉬운 오디오-비디오 번역 도구로 설계했으며 단일 대상 비디오 또는 수동 사용자 입력의 방대한 양의 비디오 장면이 필요하지 않습니다.
실험에서 대상 비디오는 비교적 짧으므로(2-3분) 인터넷에서 다운로드할 수 있는 많은 양의 비디오 장면을 작업할 수 있습니다. 
새 동영상에 쉽게 적용할 수 있도록 파이프라인의 특정 부분을 일반화합니다. 
구체적으로, 우리는 여러 사람 사이에서 일반화된 latent expression space을 계산한다. 
이것은 또한 다양한 오디오 입력을 처리할 수 있는 기능을 보장합니다.
디지털 에이전트의 시각적 모습 생성 외에도 우리의 방법은 오디오 기반 얼굴 재연으로도 사용할 수 있습니다. 
얼굴 재연은 대상 영상을 소스 배우의 표정으로 사진처럼 사실적으로 재연하는 과정입니다. 
소비자 수준의 원격 회의에서 사실적인 가상 아바타를 통해 비디오 더빙과 같은 영화 제작 응용 프로그램에 이르기까지 다양한 응용 프로그램을 사용할 수 있습니다. 
최근에 여러 저자가 얼굴 재연을 위해 오디오 신호를 활용하기 시작했습니다. 
이것은 예를 들어 가려진 얼굴, 노이즈, 왜곡된 보기 등으로 인해 시각적 신호가 신뢰할 수 없을 때 시각적 기반 접근 방식의 실패를 피할 수 있는 가능성이 있습니다. 
그러나 이러한 접근 방식 중 다수는 얼굴 이미지(잘린 얼굴, 정면 얼굴)의 정규화된 공간에서 작동하여 머리 움직임에 구애받지 않기 때문에 비디오 사실성이 부족합니다. 
예외는 오디오 신호에서 합성할 수 있는 오바마 대통령의 사실적인 비디오를 보여준 Suwajanakorn의 작업입니다. 
그러나 이 접근 방식은 교육을 위해 매우 많은 양의 데이터(오바마 대통령 주간 연설의 17시간)가 필요하므로 다른 ID에 대한 적용 및 일반화가 제한됩니다. 
반면에 우리의 방법은 대상 비디오의 2-3분만 있으면 사람별 말투와 외모를 학습할 수 있습니다.
기본 잠재 3D 모델 공간은 본질적으로 3D 일관성과 시간적 안정성을 학습하여 자연스러운 풀 프레임 이미지를 생성할 수 있습니다. 
특히 얼굴 표정에서 경직된 머리 움직임을 풀어줍니다. 

#### Contribution
– Audio2ExpressionNet이라는 시간적 네트워크 아키텍처는 오디오 스트림을 사람별 대화 스타일을 나타낼 수 있는 3D 블렌드 셰이프 기반으로 매핑하기 위해 제안됩니다. 
사전 훈련된 음성-텍스트 네트워크의 feature를 활용하여 News-speaker 데이터 세트에서 Audio2ExpressionNet을 일반화합니다.

– 짧은 대상 비디오 시퀀스(2-3분)를 기반으로, 우리의 목표는 재연 중에 대상 비디오의 대화 스타일을 유지하는 것이므로 개인별 대화 스타일 표현을 추출합니다.

– Neural textures를 사용하는 새로운 경량 신경 렌더링 네트워크를 제시하여 사람의 외모를 재현하는 사실적인 비디오 콘텐츠를 생성할 수 있습니다. 
최신 신경 렌더링 방법의 품질과 속도를 능가합니다.

