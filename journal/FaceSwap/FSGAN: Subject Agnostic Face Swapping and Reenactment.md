# FSGAN: Subject Agnostic Face Swapping and Reenactment

## Abstract
Face swapping과 reenactment를 위한 FSGAN을 제시한다. 과거 작업과는 달리 FSGAN은 불가지론적이고 해당 얼굴에 대한 학습을 요구하지 않고 얼굴 쌍에 대해 적용할 수 있다. 결론적으로, 여러 기술적 기여를 설명한다. 자세와 표정 변화 모두에 맞게 조정되고 단일 이미지 또는 비디오 시퀀스에 적용될 수 있는 face reenactment를 위한 새로운 RNN 기반 접근법을 도출한다. 비디오 시퀀스는 reenactment, Delaunay 삼각 측량 및 중입자 좌표를 기반으로 얼굴 뷰의 연속 보간을 도입한다. 폐색된 얼굴 영역은 얼굴 완성 NN에 의해 처리된다. 마지막으로, 얼굴 블렌딩 네트워크를 사용하여 두 얼굴을 원활하게 블렌딩하는 동시에 대상 피부 색상과 조명 조건을 보존한다. 이 네트워크는 poisson optimization과 perceptual loss를 결합한 새로운 poisson blending loss를 사용한다.

## 1. Inroduction
Face swapping은 source에서 target 이미지로 얼굴을 전송하여 target에 나타나는 얼굴을 잘 대체하고 사실적 결과를 산출하는 작업이다(그림 1 왼쪽). Face reenactment(aka face transfer or puppeteering)은 한 비디오에서 제어 얼굴의 얼굴 움직임과 표정 변형을 사용하여 비디오 또는 이미지에 나타나는 얼굴의 움직임과 형성을 가이드한다(그림 1 오른쪽). 엔터테인먼트[1, 21, 48], 개인 정보 보호[6, 26, 32] 및 학습 데이터 생성 분야에서 응용되기 때문에 상당한 연구 관심을 끌고 있다. 이전 연구에서 swapping 또는 reenactment를 제안했지만 둘 다 제안하지는 않았다. 이전 방법은 얼굴 외모를 전달하거나 통제하기 위해 기본 3D 얼굴 표현[46]에 의존했다. 얼굴 모양은 입력 이미지 [44, 42, 35]에서 추정되었거나 [35] 고정되었다. 그런 다음 3D 형상을 입력 영상[10]과 정렬하고 강도를 전송하거나 표정 및 시야를 제어할 때 프록시로 사용했다. 최근 얼굴 조작 작업을 위해 DNN 기반 방법이 제안되었다. 예를 들어, GAN은 가짜 얼굴의 사실적인 이미지를 성공적으로 생성한다. cGAN(cGAN)을 사용하여 한 도메인에서 다른 도메인으로 실제 데이터를 묘사하는 이미지를 변환하고 다중 얼굴 재현 체계를 영감을 주었다[37, 50, 40]. 마지막으로 DeepFake 프로젝트[12]는 비디오의 페이스 스왑을 위해 cGAN을 활용하여 비전문가가 스왑을 광범위하게 이용할 수 있도록 했으며 상당한 대중의 관심을 받았다. 이러한 방법은 기존의 그래픽 파이프라인을 대체하여 사실적인 얼굴 이미지를 생성할 수 있다. 그러나 모두 implicit하게 3D 얼굴 표현을 사용한다. 일부 방법은 latent featrue 공간 도메인 분리에 의존한다[45, 34, 33]. 이러한 방법은 나머지 feature에서 얼굴의 ID 구성 요소를 분해하고 ID를 latent feature 벡터의 표현으로 인코딩하므로 상당한 정보 손실이 발생하고 합성 이미지의 품질이 제한된다. 주제별 방법[42, 12, 50, 22]은 각 주제 또는 주제 쌍에 대해 학습 하므로 합리적인 결과를 얻기 위해 값비싼 주제별 데이터(일반적으로 수천 개의 얼굴 이미지)가 필요하고 잠재적 사용이 제한된다. 마지막으로, 이전의 얼굴 합성 방식, 특히 3D 기반 방법이 공유하는 주요 관심사는 부분적으로 가려진 얼굴을 처리할 때 모두 특별한 주의가 필요하다는 것이다. 이미지 및 비디오에서 얼굴 교환 및 재현에 대한 DL 기반 접근법을 제안한다. 이전 연구와 달리, 우리의 접근 방식은 주제 불가지론적이다. 주제별 학습 없이 다양한 주체의 얼굴에 적용할 수 있다. FSGAN은 end-to-end 학습이 가능하며 사실적이고 일시적으로 일관성 있는 결과를 생성한다. 다음과 같은 기여를 한다.  
- **Subject agnostic swapping and reenactment.** 사람별 또는 쌍별 학습을 요구하지 않고 자세, 표정 및 ID를 동시에 조작하는 최초의 방법이며 고품질과 일시적으로 일관성 있는 결과를 산출  
 
- **Multiple view interpolation.** reenactment, Delaunay 삼각 측량 및 무게 중심 좌표를 기반으로 연속적인 방식으로 동일한 얼굴의 여러 뷰 사이를 보간하는 새로운 방식을 제공  



- **New loss functions**. 1) 작은 단계로 얼굴 reenactment를 점진적으로 학습하기 위한 stepwise consistency loss와 2) source를 새로운 context에 매끄럽게 통합하도록 얼굴 혼합 네트워크를 학습하기 위한 poisson blending loss 도입

## 2. Related work
특히 face swapping과 reenactment를 위한 얼굴 이미지의 모양을 조작하는 방법은 거의 20년으로 거슬러 올라가는 오랜 역사를 가지고 있다. 이러한 방법은 원래 개인 정보 보호 문제로 인해 제안되었지만[6, 26, 32] 레크리에이션[21]이나 오락[1, 48]에 점점 더 많이 사용된다.  
**3D based methods.**  
초기 스와핑 방법은 수동 개입이 필요했다[6]. 몇 년 후 자동 방법이 제안되었다[4]. 최근에는 Face2Face가 soruce에서 target으로 표정을 전송했다[44]. 트랜스퍼는 3DMM[5, 7, 11]을 양쪽 얼굴에 맞춘 다음 내부 입 영역에 주의를 기울여 한 얼굴의 표현 구성 요소를 다른 얼굴에 적용하여 수행된다. Suwajanakorn의 reenactment 방법. [42]는 Obama의 재구성된 3D 모델을 사용하여 얼굴 랜드마크를 안내하고 Face2Face에서와 유사한 얼굴 내부 채우기 전략을 사용하여 얼굴의 입 부분을 합성했다. 정면 얼굴의 표현은 AverbuchElor에 의해 조작되었다. [3] 2D 랩 및 얼굴 랜드마크를 사용하여 source에서 target 이미지로 입 내부를 전송한다.  
  
마지막으로, Nirkin[35]은 현실적인 face swap을 위해 3D 얼굴 모양 추정이 필요하지 않음을 보여주는 face swap 방법을 제안했다. 대신 고정된 3D 얼굴 모양을 프록시로 사용했다[14, 29]. 우리와 마찬가지로 얼굴 분할 방법을 제안했지만 그들의 작업은 end-to-end 학습이 불가능했고 폐색에 특별한 주의가 필요했다.  

**GAN-based methods.**  
GAN[13]은 target 도메인과 동일한 분포로 가짜 이미지를 생성하는 것으로 나타났다. 사실적인 모양을 생성하는 데는 성공했지만 GAN 교육은 불안정할 수 있으며 저해상도 이미지에 대한 적용을 제한할 수 있다. 그러나 후속 방법은 학습 과정의 안정성을 향상시켰다[28, 2]. Karras[20] 낮은 이미지 해상도에서 높은 이미지 해상도까지 프로그레시브 멀티스케일 방식을 사용하여 GAN을 학습한다. CycleGAN[52]은 cycle consistency loss를 제안하여 서로 다른 도메인 간에 unsupervised 일반 변환을 훈련할 수 있다. L1 손실이 있는 cGAN은 Isola에 의해 적용되었다. [17] pix2pix 방법을 유도하고 모서리를 면으로 변환하는 것과 같은 응용 프로그램에 대해 매력적인 합성 결과를 생성하는 것으로 나타났다.

**Facial manipulation using GANs.**  
Pix2pixHD[47]는 다중 스케일 cGAN 아키텍처를 적용하고 perceptual loss를 추가하여 고해상도 이미지 대 이미지 변환에 GAN을 사용했다[18]. GANimation[37]은 attention map을 생성하는 감정 행동 단위를 조건으로 하는 이중 생성기 cGAN을 제안했다. 이 맵은 배경을 보존하기 위해 재연된 이미지와 원본 이미지 사이를 보간하는 데 사용되었다. GANnotation[40]은 얼굴 랜드마크에 의해 구동되는 깊은 얼굴 reenactment를 제안했다. triple consistency loss를 사용하여 점진적으로 이미지를 생성한다. 먼저 랜드마크를 사용하여 이미지를 정면화한 다음 정면 얼굴을 처리한다.  
  
Kim[22]은 최근에 하이브리드 3D/딥 방법을 제안했다. 고전적인 그래픽 파이프라인을 사용하여 특정 주제의 재구성된 3DMM을 렌더링한다. 그런 다음 렌더링된 이미지는 각 피사체의 synthetic view를 사실적인 이미지에 매핑하도록 학습된 생성기 네트워크에 의해 처리된다.  
    
마지막으로 얼굴 조작을 위한 수단으로 feature disentanglement가 제안되었다. RSGAN[34]은 얼굴과 머리카락의 잠재된 표현을 분리하는 반면 FSNet[33]은 얼굴 포즈 및 표정과 같은 기하학적 구성 요소와 정체성을 분리하는 잠재 공간을 제안했다.  

## 3. Face swapping GAN
In this work we introduce the Face Swapping GAN (FSGAN), illustrated in Fig. 2. Let Is be the source and It the target images of faces Fs ∈ Is and Ft ∈ It, respectively. We aim to create a new image based on It, where Ft is replaced by Fs while retaining the same pose and expression. FSGAN consists of three main components. The first, detailed in Sec. 3.2 (Fig. 2(a)), consists of a reenactment generator Gr and a segmentation CNN Gs. Gr is given a heatmaps encoding the facial landmarks of Ft, and generates the reenacted image Ir, such that Fr depicts Fs at the same pose and expression of Ft. It also computes Sr: the segmentation mask of Fr. Component Gs computes the face and hair segmentations of Ft. The reenacted image, Ir, may contain missing face parts, as illustrated in Fig. 2 and Fig. 2(b). We therefore apply the face inpainting network, Gc, detailed in Sec. 3.4 using the segmentation St, to estimate the missing pixels. The final part of the FSGAN, shown in Fig. 2(c) and Sec. 3.5, is the blending of the completed face Fc into the target image It to derive the final face swapping result. The architecture of our face segmentation network, Gs, is based on U-Net [38], with bilinear interpolation for upsampling. All our other generators—Gr, Gc, and Gb— are based on those used by pix2pixHD [47], with coarseto-fine generators and multi-scale discriminators. Unlike pix2pixHD, our global generator uses a U-Net architecture with bottleneck blocks [15] instead of simple convolutions and summation instead of concatenation. As with the segmentation network, we use bilinear interpolation for upsampling in both global generator and enhancers. The actual number of layers differs between generators.
Following others [50], training subject agnostic face reenactment is non-trivial and might fail when applied to unseen face images related by large poses. To address this challenge, we propose to break large pose changes into small manageable steps and interpolate between the closest available source images corresponding to a target’s pose. These steps are explained in the following sections.  
  
### 3.1. Training losses
**Domain specific perceptual loss.**   
To capture fine facial details we adopt the perceptual loss [18], widely used in recent work for face synthesis [40], outdoor scenes [47], and super resolution [25]. Perceptual loss uses the feature maps of a pretrained VGG network, comparing high frequency details using a Euclidean distance. We found it hard to fully capture details inherent to face images, using a network pretrained on a generic dataset such as ImageNet. Instead, our network is trained on the target domain: We therefore train multiple VGG-19 networks [41] for face recognition and face attribute classification. Let Fi ∈ R Ci×Hi×Wi be the feature map of the i-th layer of our network, the perceptual loss is given by  
![image](https://user-images.githubusercontent.com/40943064/147675589-5de62522-a872-422f-97a2-60ae02164d49.png)  

**Reconstruction loss.**  
While the perceptual loss of Eq. (1) captures fine details well, generators trained using only that loss, often produce images with inaccurate colors, corresponding to reconstruction of low frequency image content. We hence also applied a pixelwise L1 loss to the generators:
![image](https://user-images.githubusercontent.com/40943064/147675636-d84e98b4-f3d9-4c18-996b-97c4e1538928.png)  
  
The overall loss is then given by  
![image](https://user-images.githubusercontent.com/40943064/147675660-24dccd83-70e9-40cb-b5e1-dd8ec838b070.png)  
The loss in Eq. (3) was used with all our generators.  
  
Adversarial loss. To further improve the realism of our generated images we use an adversarial objective [47]. We utilized a multi-scale discriminator consisting of multiple discriminators, D1, D2, ..., Dn, each one operating on a different image resolution. For a generator G and a multi-scale discriminator D, our adversarial loss is defined by:  

![image](https://user-images.githubusercontent.com/40943064/147675735-d41f9c23-eeeb-4a98-a7d6-5590932d8cde.png)


### 3.2. Face reenactment and segmentation
Given an image I ∈ R 3×H×W and a heatmap representation H(p) ∈ R 70×H×W of facial landmarks, p ∈ R 70×2 , we define the face reenactment generator, Gr, as the mapping Gr :  R 3×H×W , R 70×H×W → R 3×H×W . Let vs, vt ∈ R 70×3 and es, et ∈ R 3 , be the 3D landmarks and Euler angles corresponding to Fs and Ft. We generate intermediate 2D landmark positions pj by interpolating between es and et, and the centroids of vs and vt, using intermediate points for which we project vs back to Is. We define the reenactment output recursively for each iteration 1 ≤ j ≤ n as  
![image](https://user-images.githubusercontent.com/40943064/147675789-52466c4b-1e20-453a-9176-de13bd9ab17f.png)  
Similar to others [37], the last layer of the global generator and each of the enhancers in Gr is split into two heads: the first produces the reenacted image and the second the segmentation mask. In contrast to binary masks used bu others [37], we consider the face and hair regions separately. The binary mask implicitly learned by the reenactment network captures most of the head including the hair, which we segment separately. Moreover, the additional hair segmentation also improves the accuracy of the face segmentation. The face segmentation generator Gs is defined as Gr : R 3×H×W → R 3×H×W , where given an RGB image it output a 3-channels segmentation mask encoding the background, face, and hair.  
Training. Inspired by the triple consistency loss [40], we propose a stepwise consistency loss. Given an image pair (Is, It) of the same subject from a video sequence, let Irn be the reenactment result after n iterations, and Iet, Iern be the same images with their background removed using the segmentation masks St and Srj , respectively. The stepwise consistency loss is defined as: Lrec(Iern , Iet). The final objective for the Gr: L(Gr) =λstepwiseLrec(Iern , Iet) + λrecLrec(Ier, Iet) + λadvLadv + λsegLpixel(Sr, St). (7) For the objective of Gs we use the standard crossentropy loss, Lce, with additional guidance from Gr: L(Gs) = Lce + λreenactmentLpixel(St, St r ), (8) where S t r is the segmentation mask result of Gr(It; H(pt)) and pt is the 2D landmarks corresponding to It. We train both Gr and Gs together, in an interleaved fashion. We start with training Gs for one epoch followed by the training of Gr for an additional epoch, increasing λreenactment as the training progresses. We have found that training Gr and Gs together helps filtering noise learned from coarse face and hair segmentation labels. 

### 3.3. Face view interpolation
Standard computer graphics pipelines project textured mesh polygons onto a plane for seamless rendering [16]. We propose a novel, alternative scheme for continuous interpolation between face views. This step is an essential phase of our method, as it allows using the entire source video sequence, without training our model on a particular video frame, making it subject agnostic. Given a set of source subject images, {Is1 , . . . , Isn }, and Euler angles, {e1, . . . , en}, of the corresponding faces {Fs1 , . . . , Fsn }, we construct the appearance map of the source subject, illustrated in Fig. 3(a). This appearance map embeds head poses in a triangulated plane, allowing head poses to follow continuous paths. We start by projecting the Euler angles {e1, . . . , en} onto a plane by dropping the roll angle. Using a k-d tree data structure [16], we remove points in the angular domain that are too close to each other, prioritizing the points for which the corresponding Euler angles have a roll angle closer to zero. We further remove motion blurred images. Using the remaining points, {x1, . . . , xm}, and the four boundary points, yi ∈ [−75, 75] × [−75, 75], we build a mesh, M, in the angular domain by Delaunay Triangulation. For a query Euler angle, et, of a face, Ft, and its corresponding projected point, xt, we find the triangle T ∈ M that contains xt. Let xi1 , xi2 , xi3 be the vertices of T and Isi1 , Isi2 , Isi3 be the corresponding face views. We calculate the barycentric coordinates, λ1, λ2, λ3 of xt, with respect to xi1 , xi2 , xi3 . The interpolation result Ir is then 
![image](https://user-images.githubusercontent.com/40943064/147675884-8dc8c5ec-dc8d-46c5-8a89-006c3ae83872.png)  

where pt are the 2D landmarks of Ft. If any vertices of the triangle are boundary points, we exclude them from the interpolation and normalize the weights, λi , to sum to one. A face view query is illustrated in Fig. 3(b,c). To improve interpolation accuracy, we use a horizontal flip to fill in views when the appearance map is one-sided with respect to the yaw dimension, and generate artificial views using Gr when the appearance map is too sparse.
### 3.4. Face inpainting
Occluded regions in the source face Fs cannot be rendered on the target face, Ft. Nirkin et al. [35] used the segmentations of Fs and Ft to remove occluded regions, rendering (swapping) only regions visible in both source and target faces. Large occlusions and different facial textures can cause noticeable artifacts in the resulting images. To mitigate such problems, we apply a face inpainting generator, Gc (Fig. 2(b)). Gc renders face image Fs such that the resulting face rendering ˜Ir covers entire segmentation mask St (of Ft), thereby resolving such occlusion. Given the reenactment result, Ir, its corresponding segmentation, Sr, and the target image with its background removed, ˜It, all drawn from the same identity, we first augment Sr by simulating common face occlusions due to hair, by randomly removing ellipse-shaped parts, in various sizes and aspect ratios from the border of Sr. Let ˜Ir be Ir with its background removed using the augmented version of Sr, and Ic the completed result from applying Gc on ˜Ir. We define our inpainting generator loss as
![image](https://user-images.githubusercontent.com/40943064/147676018-9e56f529-c133-4221-8ae5-cd651e5da573.png)  
where Lrec and Ladv are the reconstruction and adversarial losses of Sec. 3.1  

### 3.5. Face blending
The last step of the proposed face swapping scheme is blending of the completed face Fc with its target face Ft (Fig. 2(c)). Any blending must account for, among others, different skin tones and lighting conditions. Inspired by previous uses of Poisson blending for inpainting [51] and blending [49], we propose a novel Poisson blending loss. Let It be the target image, I t r the image of the reenacted face transferred onto the target image, and St the segmentation mask marking the transferred pixels. Following [36], we define the Poisson blending optimization as  
![image](https://user-images.githubusercontent.com/40943064/147676041-e2e31359-6614-4234-9a42-2b4ad40f6567.png)  
where ∇ (·) is the gradient operator. We combine the Poisson optimization in Eq. (11) with the perceptual loss. The Poisson blending loss is then L(Gb)  
![image](https://user-images.githubusercontent.com/40943064/147676056-fc1775bd-dad5-4f3d-8547-fed79ebc6720.png)  

## 4. Datasets and training
We use the video sequences of the IJB-C dataset [30] to train our generator, Gr, for which we automatically ex- tracted the frames depicting particular subjects. IJB-C contains ∼11k face videos, of which we used 5,500 which were in high definition. Similar to the frame pruning approach of Sec. 3.3, we prune the face views that are too close together as well as motion-blurred frames. We apply the segmentation CNN, Gs, to the frames, and prune the frames for which less than 15% of the pixels in the face bounding box were classified as face pixels. We used dlib’s face verification1 to group frames according to the subject identity, and limit the number of frames per subject to 100, by choosing frames with the maximal variance in 2D landmarks. In each training iteration, we choose the frames Is and It from two randomly chosen subjects. We trained VGG-19 CNNs for the perceptual loss on the VGGFace2 dataset [9] for face recognition and the CelebA [27] dataset for face attribute classification. The VGGFace2 dataset contains 3.3M images depicting 9,131 identities, whereas CelebA contains 202,599 images, annotated with 40 binary attributes. We trained the segmentation CNN, Gs, on data used by others [35], consisting of ∼10k face images labeled with face segmentations. We also used the LFW Parts Labels set [19] with ∼3k images labeled for face and hair segmentations, removing the neck regions using facial landmarks. We used additional 1k images and corresponding hair segmentations from the Figaro dataset [43]. Finally, FaceForensics++ [39] provides 1000 videos, from which they generated 1000 synthetic videos on random pairs using DeepFakes [12] and Face2Face [44].
### 4.1. Datasets and processing

### 4.2. Training details
We train the proposed generators from scratch, where the weights were initialized randomly using a normal distribution. We use Adam optimization [24] (β1 = 0.5, β2 = 0.999) and a learning rate of 0.0002. We reduce this rate by half every ten epochs. The following parameters were used for all the generators: λperc = 1, λpixel = 0.1, λadv = 0.001, λseg = 0.1, λrec = 1, λstepwise = 1, where λreenactment is linearly increased from 0 to 1 during training. All of our networks were trained on eight NVIDIA Tesla V100 GPUs and an Intel Xeon CPU. Training of Gs required six hours to converge, while the rest of the networks converged in two days. All our networks, except for Gs, were trained using a progressive multi scale approach, starting with a resolution of 128×128 and ending at 256×256. Inference rate is ∼30fps for reenactment and ∼10fps for swapping on one NVIDIA Tesla V100 GPU.
## 5. Experimental results
We performed extensive qualitative and quantitative experiments to verify the proposed scheme. We compare our 1Available: http://dlib.net/ method to two previous face swapping methods: DeepFakes [12] and Nirkin et al. [35], and the Face2Face reenactment scheme [44]. We conduct all our experiments on videos from FaceForensics++ [39], by running our method on the same pairs they used. We further report ablation studies showing the importance of each component in our pipeline.
### 5.1. Qualitative face reenactment results
Fig. 4 shows our raw face reenactment results, without background removal. We chose examples of varying ethnicity, pose, and expression. A specifically interesting example can be seen in the rightmost column, showing our method’s ability to cope with extreme expressions. To show the importance of iterative reenactment, Fig 5 provides reenactments of the same subject for both small and large angle differences. As evident from the last column, for large angle differences, the identity and texture are better preserved using multiple iterations.
### 5.2. Qualitative face swapping results
Fig. 6 offers face swapping examples taken from FaceForensics++ videos, without training our model on these videos. We chose examples that represent different poses and expression, face shapes, and hair occlusions. Because Nirkin et al. [35] is an image-to-image face swapping method, to be fair in our comparison, for each frame in the target video we select the source frame with the most similar pose. To compare FSGAN in a video-to-video scenario, we use our face view interpolation described in Sec. 3.3.
### 5.3. Comparison to Face2Face
We compare our method to Face2Face [44] on the expression only reenactment problem. Given a pair of faces Fs ∈ Is and Ft ∈ It the goal is to transfer the expression from Is to It. To this end, we modify the corresponding 2D landmarks of Ft by swapping in the mouth points of the 2D landmarks of Fs, similarly to how we generate the intermediate landmarks in Sec. 3.2. The reenactment result is then given by Gr(It; H(ˆpt)), where pˆt are the modified landmarks. The examples are shown in Fig. 7.
### 5.4. Quantitative results
We report quantitative results, conforming to how we defined the face swapping problem: we validate how well methods preserve the source subject identity, while retaining the same pose and expression of the target subject. To this end, we first compare the face swapping result, Fb, of each frame to its nearest neighbor in pose from the subject face views. We use the dlib [23] face verification method to compare identities and the structural similarity index method (SSIM) to compare their quality. To measure pose accuracy, we calculate the Euclidean distance between the Euler angles of Fb to the original target image, It. Similarly, the accuracy of the expression is measured as the Euclidean distance between the 2D landmarks. Pose error is measured in degrees and the expression error is measured in pixels. We computes the mean and variance of those measurements on the first 100 frames of the first 500 videos in FaceForensics++, averaging them across the videos. As baselines, we use Nirkin et al. [35] and DeepFakes [12]. Evident from the first two columns of Table 1, our approach preserves identity and image quality similarly to previous methods. The two rightmost metrics in Table 1 show that our method retains pose and expression much better than its baselines. Note that the human eye is very sensitive to artifacts on faces. This should be reflected in the quality score but those artifacts usually capture only a small part of the image and so the SSIM score does not reflect them well.
### 5.5. Ablation study
We performed ablation tests with four configurations of our method: Gr only, Gr + Gc, Gr + Gb, and our full pipeline. The segmentation network, Gs, is used in all configurations. Qualitative results are provided in Fig. 8. Quantitative ablation results are reported in Table 2. Verification scores show that source identities are preserved across all pipeline networks. From Euler and landmarks scores we see that target poses and expressions are best retained with the full pipeline. Error differences are not extreme, suggesting that the inpainting and blending generators, Gc and Gb, respectively, preserve pose and expression similarly well. There is a slight drop in the SSIM, due to the additional networks and processing added to the pipeline.


## 6. Conclusion
Limitations. Fig. 5 shows our reenactment results for different facial yaw angles. Evidently, the larger the angular differences, the more identity and texture quality degrade. Moreover, too many iterations of the face reenactment generator blur the texture. Unlike 3DMM based methods, e.g., Face2Face [44], which warp textures directly from the image, our method is limited to the resolution of the training data. Another limitation arises from using a sparse landmark tracking method that does not fully capture the complexity of facial expressions. Discussion. Our method eliminates laborious, subjectspecific, data collection and model training, making face swapping and reenactment accessible to non-experts. We feel strongly that it is of paramount importance to publish such technologies, in order to drive the development of technical counter-measures for detecting such forgeries, as well as compel law makers to set clear policies for addressing their implications. Suppressing the publication of such methods would not stop their development, but rather make them available to select few and potentially blindside policy makers if it is misused.
