# Layer Normalization
## Abstract
SOTA 신경망을 훈련하는 데는 계산 비용이 많이 듭니다. 훈련 시간을 줄이는 한 가지 방법은 뉴런의 activation을 Nomalize하는 것입니다.  
최근에 도입된 batch normalization는 train sample 의 mini_batch를 통해 뉴런에 대한 합산 입력 분포를 사용하여 평균과 분산을 계산한 다음  
각 학습샘플에서 해당 뉴런에 대한 합산 입력을 정규화하는 데 사용됩니다.  
이것은 feedforward network에서 학습시간을 크게 줄입니다.  
그러나 배치 정규화의 효과는 mini-batch 크기에 따라 달라지며 RNN에 적용하는 방법이 명확하지 않습니다.  
이 논문에서 우리는 단일 학습 케이스의 레이어에 있는 모든 합산된 입력에서 정규화에 사용되는 평균과 분산을 계산하여 배치 정규화를 레이어 정규화로 전치합니다.  
batch norm.과 마찬가지로 각 뉴런에 정규화 후 비선형성 전에 적용되는 자체 적응 편향과 이득을 제공합니다.  
batch norm.과 달리 layer norm.은 훈련 및 테스트 시간에 정확히 동일한 계산을 수행합니다.  
각 시간 단계에서 정규화 통계를 별도로 계산하여 RNN에 적용하는 것도 간단합니다.  
계층 정규화는 순환 네트워크에서 숨겨진 상태 역학을 안정화하는 데 매우 효과적입니다.  
경험적으로, 우리는 Layer norm이 이전에 발표된 기술에 비해 훈련 시간을 상당히 줄일 수 있음을 보여줍니다.  

## 1. Introduction
일부 버전의 SGD로 훈련된 DNN은 컴퓨터 비전 및 음성 처리의 다양한 지도 학습 작업에 대한 이전 접근 방식을 훨씬 능가하는 것으로 나타났습니다.  
그러나 최첨단 심층 신경망은 긴 학습이 필요합니다.  
다른 기계에서 학습 사례의 다른 하위 집합에 대한 기울기를 계산하거나 신경망 자체를 여러 기계로 분할하여 학습 속도를 높이는 것이 가능하지만  
많은 통신과 복잡한 소프트웨어가 필요할 수 있습니다.  
또한 병렬화 정도가 증가함에 따라 수익이 급격히 감소하는 경향이 있습니다.  
직교 접근 방식은 학습을 더 쉽게 하기 위해 신경망의 정방향 패스에서 수행되는 계산을 수정하는 것입니다.  
최근에는 심층 신경망에 정규화 단계를 추가로 포함하여 훈련 시간을 줄이기 위해 배치 정규화가 제안되었습니다.  
정규화는 평균과 훈련 데이터의 표준 편차를 사용하여 합산된 각 입력을 표준화합니다.  
Batch norm.을 사용하여 훈련된 피드포워드 신경망은 간단한 SGD로도 더 빠르게 수렴됩니다.  
훈련 시간 개선 외에도 배치 통계의 확률성은 훈련 중 정규화 역할을 합니다.  

단순함에도 불구하고 배치 정규화에는 합산된 입력 통계의 평균이 필요합니다.  
고정 깊이의 피드포워드 네트워크에서는 각 은닉층에 대해 별도로 통계를 저장하는 것이 간단합니다.  
그러나 RNN의 순환 뉴런에 대한 합산 입력은 시퀀스의 길이에 따라 달라지는 경우가 많으므로  
RNN에 일괄 정규화를 적용하려면 다른 시간 단계에 대해 다른 통계가 필요한 것처럼 보입니다.  
또한 배치 정규화는 온라인 학습 작업이나 미니 배치가 작아야 하는 매우 큰 분산 모델에는 적용할 수 없습니다.  
본 논문에서는 다양한 신경망 모델의 학습 속도를 향상시키는 간단한 정규화 방법인 layer normalization을 소개합니다.  
배치 정규화와 달리 제안된 방법은 합산된 입력에서 은닉층 내의 뉴런에 대한 정규화 통계를 직접 추정하므로  
정규화가 학슴샘플 간에 새로운 종속성을 도입하지 않습니다.  
우리는 레이어 정규화가 RNN에 잘 작동하고 여러 기존 RNN 모델의 훈련 시간과 일반화 성능을 모두 향상시킨다는 것을 보여줍니다.  

## 2. Background
feed-forward NN은 입력 x를 y로 매핑하는 비선형 매핑이다. 
