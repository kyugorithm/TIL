### 3 METHOD
표정, 조명, 시점에 관계없이 얼굴 비디오 이미지에 대한 ID 보존, 제어 가능한 re-aging 제시  
Skip-connection이 있는 fully convolution neural network architecture로 I2I 변환 문제를 공식화한다.  
네트워크는 소스/타겟 나이로 레이블된 동일한 합성 및 사실적인 사람을 보여주는 많은 수의 face image pair에 지도 방식으로 학습된다.  

#### 핵심요소
1) 여러 시점에서 (다양한 id/나이/민족)을 묘사하는 주석이 달린 긴 시간(나이)에 걸친 이미지 데이터 세트를 획득하는 불가능해 보이는 작업을 우회하기 위한 효과적인 전략을 도출 (s3.1)  
2) 비디오 프레임 전체에서 우수한 일관성을 유지하면서 변화하는 표정과 시점에 대한 ID 보존을 허용하기 위해 솔루션에 적합한 매개변수 공간을 설계 (s3.2)

#### 3.1 Synthesizing high-quality, longitudinal aging data

간단하고 완전히 감독되는 방식으로 re-aging 네트워크를 훈련시키기 위해 여기서 우리의 목표는 많은 수의 입력-출력 이미지 쌍을 생성하는 것이다.  
한 쌍의 이미지는 (얼굴 표정, 포즈, 조명 및 배경)이 다르고 나이가 서로 다르다.  
데이터 세트에 실제 사람이 포함되어야 하는 경우 분명히 이 작업을 수행할 수 없다. 따라서 수천 실제 얼굴에 사전 학습된 강력한 신경 얼굴 모델의 잠재 공간 내에서 의미론적 조작을 활용하는 최근 작업에서 영감을 얻어 포토리얼리스틱 합성 얼굴을 사용하여 이 목표를 달성한다.  
S4에서 설명하듯이 이러한 모델 내 조작은 실제 얼굴이 잠재 공간에 투영되어야 하기 때문에 비디오의 실제 얼굴에서 제대로 수행되지 않는 경우가 많다. 
새로 주어진 이미지를 과적합하도록 모델을 더욱 최적화(미세 조정)하더라도, 시간적 아티팩트는 특히 시점 전체에서 재노화가 거의 또는 전혀 없는 경우에도 여전히 명확하게 눈에 띈다.  

우리의 첫 번째 핵심 통찰은 위의 방법들이 그럼에도 불구하고 모델의 잠재 공간 내에서 이미 완벽하게 표현된 합성 얼굴에 대한 강력한 재노화 솔루션이라는 것이다.  
그리고 re-aged 얼굴은 노화 과정의 의미를 실제 훈련 이미지만큼 설득력 있게 포착하는것으로 보인다. 우리는 이 속성이 실제 얼굴을 재노화하기 위해 더 간단한 네트워크를 훈련할 수 있는 고품질 longitudinal 노화 데이터 세트를 합성하는 데 적합한 잠재 공간 순회 방법을 만든다는 점을 강조합니다.  
이러한 통찰력과 위의 traversal 접근 방식을 사용하여 특정 정체성과 나이를 나타내는 잠재 공간의 임의 지점을 고려한 다음 사전 훈련된 연령 회귀자가 조정하는 경로를 따라 잠재 공간 순회를 시작할 수 있다.  
이 순회를 따라 시간을 앞뒤로 이동하면 당면한 특정 ID에 대한 지속적인 연령 진행이 생성되어 교육을 위한 많은 수의 이미지 쌍이 생성된다.  
이 과정은 거의 무제한의 id에 대해 반복될 수 있으며 다양한 관점, 얼굴 표정, 조명 조건 및 배경에서도 샘플링할 수 있다.  

이러한 guided latent space traversal을 수행하기 위해 최근에 제안된 많은 방법 중 SAM을 사용하여 훈련 데이터 세트를 샘플링한다.  
이 선택의 주된 이유는 다른 얼굴 속성에 대한 부작용이 거의 없이 연령만 독점적으로 변경하는 잠재 공간에서 비선형 경로를 따라가는 방법의 우수한 능력으로 디지털 re-aging 품질을 최대화하려는 우리의 목표와 일치하기 때문이다.  
위의 전략에 따라 18세에서 85세 범위의 14가지 연령대가 있는 2000개의 ID로 구성된 얼굴 재노화를 위한 교육 데이터 세트를 생성하여 샘플링된 ID당 총 196개의 교육 쌍(동일한 연령 쌍 포함) ). 
이를 통해 실제 이미지에서 포착할 수 없는 고품질 훈련 데이터를 획득하는 첫 번째 주요 과제를 해결한다. 
딥 러닝에서 데이터는 매우 중요하기 때문에 이 성과는 다음에 설명된 간단한 솔루션을 가능하게 하여 재노화 문제를 해결하는 데 큰 도움이 된다.


#### 3.2 High-quality Face Re-Aging Network (FRAN)
감독되는 완전히 컨볼루션되고 제어 가능한 이미지 간 변환 솔루션을 제시한다.  

I2I 번역 특성을 감안할 때 입증된 U-Net 아키텍처를 채택하고 translation 품질 및 re-aging 제어를 위해 약간의 조정을 가한다.  
re-aging할 RGB 이미지로 구성된 5채널 텐서와 각 이미지 픽셀의 입력 및 출력 에이징을 지정하는 2개의 단일 채널 에이징 맵(예: 2개의 서로 다른 에이징이 있는 2개의 균일한 채널)을 입력으로 사용한다.  
그리고 U-Net은 최종 re-aging 생성을 위해 입력 이미지 위에 추가되는 픽셀당 RGB delta(오프셋)를 예측한다(그림 3).  
아래에 설명된 대로 L1, perceptual, adversarial loss와 paired 합성 데이터를 사용하여 학습된다.

FRAN에 입력으로 제공되는 두 개의  spatial age map은 입력 RGB 이미지와 동일한 공간 해상도의 단일 채널 이미지이다. 이러한 age map의 픽셀 값은 연속적인 연령 간격(년/100)을 나타내는 0과 1 사이에서 정규화된다. 대상 연령뿐만 아니라 입력 연령도 제공함으로써 우리는 FRAN이 입력의 현재 연령을 추정하기 위해 용량을 소비하는 대신 재노화 작업(에이징 델타 예측)에 집중할 수 있도록 한다. 기존의 사전 학습된 age regressors 분석기에 의해 잘 수행되었다. 입력 age 채널은 반드시 공간적으로 일정할 필요는 없으므로 비균질 값으로 채워져 얼굴의 서로 다른 영역에서 다양한 양의 재노화를 제어하여 재노화에 대해 공간적으로 다양한 제어를 제공할 수 있습니다(예: 그림 14 참조). 입력 age map도 비균질하게 편집하여 다양한 재노화 효과를 생성할 수 있으므로(주관적으로 인식된 입력 연령을 변경하여) 재노화 결과를 지시하고 미세 조정하려는 아티스트에게 더 창의적인 자유를 제공한다. 이러한 연령 맵을 사용하여 FRAN을 제어하면 BiSeNet을 쉽게 통합하여 생성된 효과에 대한 제어를 자동화하여 얼굴의 특정 영역으로 제한할 수 있다.  

I2I 변환 작업으로 re-aging을 공식화하면 몇 가지 이점이 있다.  
1) 입력의 공간 레이아웃을 보존하는 것으로 잘 알려진 U-Net 아키텍처의 결과인 ID 보존 : 이는 U-Net의 skip-connection이 고해상도에서 입력 이미지 feature에 직접 액세스할 수 있는 출력 레이어를 제공하기 때문이다.  
2) 여러 (표정, 관점 및 조명)에서 (서로 다른 정체성)의 얼굴을 완전히 생성할 필요가 없다. 입력 이미지 위에 RGB 오프셋으로 re-aging을 예측하면 되며, 이는 또한 입력 ID의 상당한 손실을 방지한다.  
3) 입력 비디오 프레임에 대한 시간적인 부드러움은 자연스럽게 FRAN의 출력에서 우수한 시간적 일관성에 기여한다.  
위 이러한 요소가 결합되어 FRAN은 비디오에서 실제 얼굴 re-aging을 위한 우수한 프로덕션 준비 솔루션이 된다.


#### Discriminator
PatchGAN discriminator, re-aged RGB 이미지와 target age map을 입력으로 사용한다. 생성된 re-aging 모양이 학습 데이터 세트에서 대상 연령과 일치하는지 여부를 판단하는 것이다. D는 주요 re-aging U-Net과 함께 훈련된다. 올바른 age lagel이 있는 합성 데이터 세트의 샘플은 'real' 예제로 제공되고 재노화 네트워크에서 생성된 샘플은 '가짜' 예제로 제공된다. 또한 추가 'fake'의 예로 age map이 잘못된 실제 이미지를 제공한다. 

#### Losses
<img width="250" alt="image" src="https://user-images.githubusercontent.com/40943064/209640501-0c7cf57f-8688-4dc1-aed9-2da120c19fc9.png">

## 4.6 Extent of Re-aging
다음을 변화시키지 않음 : 어린이 / 두피 / 머리 모양의 큰 변화  
주름같은 질감 변화가 쉽게 눈에 띄는 반면, 다른 얼굴 특징을 변경하고 현실적인 재노화에도 중요한 아래의 기하학적 변화를 도입한다.  
눈가(주름, 눈썹 처짐, 눈밑 처짐), 입가(입술이 얇아짐), 연골 조직 크기(귓볼, 코)  
<img width="500" alt="image" src="https://user-images.githubusercontent.com/40943064/212268400-f7514a1e-7e4e-423c-9e6d-a8186bb1d919.png">  

### 4.7 Segmentation at Inference
Segmentation mask를 사용하여 얼굴의 피부 픽셀에 대해서만 aging을 guide하는 효과를 평가한다.(그림 17)  
<img width="500" alt="image" src="https://user-images.githubusercontent.com/40943064/212259754-7b4dfbb6-c811-4602-889b-5076a4ead458.png">  
입력 이미지의 모든 픽셀(마스크 없는 결과)에 대해 35 > 65로 균일하게 설정하고 입력 및 출력, 에이징된 이미지 차이를 보여준다.  
배경 픽셀에 대해서 대상-입력 연령이 다르므로 배경에 원하지 않는 변경 사항이 포함된다.  
우리는 Bisenet V2로 추출한 segmentation mask에 의해 정의된 피부 픽셀에 대해서만 목표 연령을 65로 설정하는 실험을 반복하고 이 마스크 외부의 모든 픽셀에 대해 목표 연령을 35로 한다(입력 연령과 동일)(마스크와 함께 "표시된 결과"). 차이 영상에서 볼 수 있듯이 해당 출력 영상에는 원하지 않는 배경 변경이 없다.

### 4.8 Losses Ablation
단일 Loss만을 이용하여 학습을 진행(동일 조건 : epoch(20), 데이터(100명 x14 ages)  

1) L1 : 픽셀 단위로만 계산하여 선명도 부족  
2) Adv : 20 epoch는 짧아 에이징 효과를 생성하기에 충분하지 않다.  
3) LPIPS : 더 선명  
4) L1 + LPIPS : 선명한 결과에 더 강한 노화 효과  
5) L1 + LPIPS + ADV : 노화 효과와 선명도 추가 향상  
<img width="1024" src="https://user-images.githubusercontent.com/40943064/212270349-a3bc491c-4384-49c9-97db-bfbe881185ae.png">
