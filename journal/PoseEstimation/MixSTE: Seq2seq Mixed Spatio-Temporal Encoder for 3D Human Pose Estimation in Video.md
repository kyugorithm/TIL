MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video

## Abstract
시공간 상관 관계를 학습하기 위해 전체 프레임의 관절을 고려하여 2D 키포인트 시퀀스에서 3D 자세를 추정하는 tranformer 기반 방법론이 최근 도입되었다. 관절에 따라 움직임이 크게 달라지는 것을 알 수 있다. 그러나 이전 방법으로는 각 관절의 견고한 프레임 간 대응관계를 효율적으로 모델링할 수 없으므로 시공간적 상관관계에 대한 학습이 불충분하다. 각 관절의 시간적 움직임을 별도로 모델링하는 시간적 transformer 블록과 관절 간 공간적 상관관계를 학습하는 공간적 transformer 블록이 있는 MixSTE(Mix Spacio-Temporal Encoder)를 제안한다. 이들 2개의 블록을 번갈아 사용하여 시공간 feature encoding을 개선한다. 또, 네트워크 출력을 중앙 프레임으로부터 입력 비디오의 전체 프레임까지 연장해, 입력 시퀀스와 출력 시퀀스의 일관성을 향상시킨다.  
  
제안된 방법을 평가하기 위해 세 가지 벤치마크(Human3.6M, MPI-INF-3DHP, HumanEva)에 대해 광범위한 실험을 수행했다. 결과는 우리 모델이 Human3.6M 데이터 세트에서 10.9% P-MPJPE 및 7.6% MPJPE만큼 최첨단 접근 방식을 능가한다는 것을 보인다.

## Introduction

Monocular 관찰에서 3D 포즈 추정은 입력 이미지 또는 비디오에서 3D 관절 위치를 재구성하는 기본적인 비전 작업이다. 이 작업은 신체 기하학 및 동작에 대한 의미 있는 표현을 얻을 수 있으므로 컴퓨터 애니메이션, 동작 인식 및 인간 로봇 상호 작용과 같은 광범위한 응용 프로그램이 있다. 가장 최근의 작업은 2D 키포인트를 먼저 감지한 다음 3D로 들어 올리는 2D-to-3D 리프팅 파이프라인을 기반으로 한다. Monocular 데이터의 깊이 모호성으로 인해 동일한 2D 포즈에서 여러 개의 잠재적인 3D 포즈가 매핑될 수 있으므로 단일 프레임의 2D 키포인트 정보만으로는 정확한 3D 포즈를 복구하기 어렵다.  

단일 프레임에서 위의 문제를 해결하기 위해 입력 비디오에 포함된 시간 정보를 활용하여 주목할만한 진전이 있었다. 최근 시퀀스 데이터를 모델링하는 능력에 대한 트랜스포머의 성공에 힘입어 Zheng은 트랜스포머 기반 3D 포즈 추정 네트워크를 도입했다. 비디오에서 보다 정확한 중앙 프레임 포즈를 추정하기 위해 시공간 정보를 활용한다. 모든 관절 사이의 공간적 상관관계와 연속된 프레임 사이의 시간적 상관관계를 모델링함으로써, PoseFormer는 성능 향상을 달성한다. 그러나 신체 관절 간의 움직임 차이를 무시하여 시공간적 상관관계 학습이 부족하다. 또한, 시간 트랜스포머 모듈의 dimension을 증가시켜 더 긴 입력 시퀀스의 사용을 제한한다.  

Poseformer는 비디오를 입력으로 받아 중앙 프레임의 포즈만 추정한다. 이 파이프라인을 seq2frame 방식으로 요약한다. 최근의 많은 방법이 이 방식을 따른다. 특정 순간의 포즈 추정 정확도를 높이기 위해 인접 프레임을 활용하지만, 단일 프레임 출력으로 인해 시퀀스 일관성이 무시된다. 또한 추론하는 동안 이러한 seq2frame 솔루션은 모든 프레임의 3D 포즈를 얻기 위해 크게 겹치는 2D 키포인트 시퀀스를 반복적으로 입력해야 하므로 중복 계산이 발생한다. seq2frame 방식을 제외하고 입력 2D 키포인트에서 3D 포즈 시퀀스를 회귀하는 seq2seq 방식도 있다. 이러한 방법은 주로 LSTM또는 GCN에 의존하며 연속 추정 결과 중 시간 정보를 학습하는 데 좋은 성능을 보인다. 그러나 현재 seq2seq 네트워크에는 입력 시퀀스와 출력 시퀀스 간의 전역 모델링 기능이 부족하여 긴 시퀀스의 출력 포즈에서 과도하게 부드러운 경향이 있다. LSTM의 낮은 효율은 비디오에서 사람의 자세를 추정하는 데에도 심각한 문제이다.  

이전 작업은 공간 및 시간 영역의 모든 관절을 연결하는 데 중점을 두었지만 다른 신체 관절의 모션 궤적이 프레임마다 다르며 별도로 학습해야 함을 관찰했다. 또한 입력 2D 키포인트 시퀀스와 출력 3D 포즈 시퀀스는 견고한 전역 일관성을 가지며 정확하고 원활한 3D 포즈 추정을 위해 긴밀하게 결합되어야 한다.

이 작업에서 MixSTE를 제안하여 각 관절의 개별 시간적 움직임을 학습하고 seq2seq 방식으로 순차적인 일관된 인간 포즈 시퀀스를 부여한다. 새로운 seq2seq 아키텍처와 모션 인식 제약 세트를 통해 2D 키포인트 시퀀스를 3D 포즈 시퀀스로 들어 올린다. 구체적으로 그림 1의 상단과 같이 각 관절의 시간적 움직임 정보를 고려한 관절 분리를 제안한다. 각 2D 관절을 개별 특징(변환기에서는 토큰이라고 함)으로 취하여 시공간 상관 관계를 충분히 학습하고 시간 영역에서 관절 특징의 차원을 줄이는 데 도움이 된다. 또한 긴 시퀀스 내에서 더 나은 시퀀스 일관성을 유연하게 얻기 위해 seq2seq를 사용하는 교대 설계를 제안하여 중복 계산과 과도한 부드러움을 줄인다. 이러한 방식으로 서로 다른 신체 관절의 시간적 움직임 궤적을 적절하게 고려하여 정확한 3D 포즈 시퀀스를 예측할 수 있다. 우리가 아는 한, 제안된 방법은 seq2seq 파이프라인에서 변환기 인코더를 처음으로 활용하여 정확한 포즈 추정을 위한 학습 시공간 상관관계를 향상시키고 seq2frame 방법으로부터의 추론 속도를 크게 향상시킨다(그림 1의 하단 참조). 1) 게다가, 우리의 접근 방식은 입력 시퀀스의 모든 길이에 쉽게 적응할 수 있다.

<img width="540" alt="image" src="https://user-images.githubusercontent.com/40943064/159107284-1337ad1d-1a62-4981-9904-22446d84ef7e.png">

3D 포즈 추정에 대한 기여는 세 가지로 요약될 수 있다.

* 긴 시퀀스에 걸쳐 서로 다른 신체 관절의 시간적 움직임을 효과적으로 포착하기 위해 제안되었으며, 충분한 시공간 상관 관계를 모델링하는 데 도움이 된다.
* 재구성 포즈의 정확도를 향상시키기 위해 시퀀스 간의 전역 일관성을 학습하는 새로운 변환기 기반 seq2seq 방법을 제안한다.
* 우리의 접근 방식은 3가지 벤치마크에서 최첨단 성능을 달성하고 탁월한 일반화를 제공한다.

## 2. Related Work

#### 3D Human Pose Estimation
단안 데이터에서 3D 인간 포즈를 추정하는 것은 이전에 운동학적 feature 또는 골격 구조에 의존하여 시작되었다. 딥 러닝의 발달로 더 많은 데이터 기반 방법이 제안되었으며 이러한 방법은 end2end와 2D-3D 리프팅 방식으로 나눌 수 있다. End2end은 중간 2D 포즈 표현 없이 입력에서 3D 좌표를 직접 추정한다. 일부 방법은 이러한 방식을 따랐지만 이미지 공간에서 직접 회귀하기 때문에 높은 계산 비용이 필요했다. 반면 2D-3D 리프팅 파이프라인은 먼저 RGB 데이터의 2D 키포인트를 추정한 다음 2D 및 3D 인간 구조 간의 대응 관계를 활용하여 2D 키포인트를 3D 포즈로 들어 올린다. 2D 키포인트 감지 작업의 안정적인 노력 덕분에 최근의 2D에서 3D로의 리프팅 방법은 end2end을 능가했다. 따라서 강력한 2D 중간 감독을 얻기 위해 2D-to-3D 리프팅 방식을 따른다.

#### Seq2frame and Seq2seq under 2D-to-3D Lifting
최근에는 비디오의 시간 정보를 활용하여 다양한 방법으로 보다 강력한 예측을 수행하고 있다. 비디오 입력으로 많은 영향력 있는 작품(seq2frame)은 입력 비디오의 중앙 프레임을 예측하는 데 주의를 기울여 보다 강력한 예측과 노이즈에 대한 민감도를 낮춘다. Pavllo는 시간적 특징을 추출하기 위해 TCN을 기반으로 dilated temporal 컨볼루션을 제안했다. 다음의 몇몇 작품은 Attention 메커니즘을 활용하거나 포즈 추정 작업을 뼈 길이와 뼈 방향 예측으로 분해하여 TCN의 성능을 향상 시켰지만 입력 시퀀스의 receptive field를 수정해야했다. 이와는 대조적으로 우리의 접근 방식은 컨볼루션 커널이나 슬라이딩 윈도우 크기와 관련하여 각 입력의 길이를 미리 설정할 필요가 없다. 또한 GCN은 [1]의 작업에 적용되어 인간과 손 포즈의 다중 스케일 feature를 학습한다. 이 작품들은 좋은 성과를 거두었지만 계산 중복성은 이러한 방법의 일반적인 결함이다.  
반면 일부 작업(seq2seq)은 3D 포즈 추정의 일관성과 효율성을 향상시키고 입력 시퀀스의 모든 프레임을 한 번에 재구성한다. LSTM은 2D 키포인트 세트에서 비디오의 3D 포즈를 추정하기 위해 도입되었다[22]. Hossain은 시퀀스에 대한 시간적 일관성을 보장하기 위해 시간 도함수 손실 함수를 제시했지만 낮은 컴퓨팅 효율성 문제에 직면해 있다. Wang은 GCN 기반 접근 방식을 활용하고 짧은 시간 간격과 긴 시간 범위 모두에서 모션을 모델링하는 해당 손실을 설계했지만 입력 시퀀스의 전역 모델링 기능이 부족하다. [12,41]과 대조적으로 우리의 방법은 공간적, 시간적 영역에서 각 관절의 전역 모델링 능력이 있다는 장점이 있다. 또한 프레임 및 관절에 대한 병렬 프로세스를 통해 LSTM의 저효율 문제를 해결할 수 있다[11].

#### Self-attention and Transformer
Self-attention이 있는 트랜스포머 아키텍처는 [40]에 의해 처음 제안된 후 다양한 시각적 작업에 적용되었다. ViT로 분류하고 DETR로 감지한다. 인간 포즈 추정 작업을 위해 이미지에서 2D 포즈를 추정하기 위해 Transpose를 제안했다. [21]은 단일 이미지에서 인간 mesh 복구 및 포즈 추정을 위한 트랜스포머 프레임워크를 제시했지만 비디오의 시간 정보를 무시했다. 일부 연구자들은 또한 다중 뷰 3D 인간 포즈 추정 방식을 탐구했다. stride transformer encoder는 로컬 컨텍스트를 통합하기 위해 도입되었다. 또한, PoseFormer는 공간적, 시간적 종속성을 순차적으로 포착하기 위해 ViT 기반 모델을 구축했다. [19]와 [50] 모두 공간 및 시간 인코더의 순서를 고정해야 하며 비디오의 중앙 프레임만 복원된다. 우리의 접근 방식은 트랜스포머 아키텍처를 적용하는 것과 유사하다. 그러나 우리는 다른 신체 관절의 모션 궤적을 고려하고 더 나은 모델 시퀀스 일관성을 위해 seq2seq를 적용한다.

위의 관련 연구의 분석 및 비교를 통해 3D 인체 포즈 추정에서 Transformer 기반 방법에 대한 추가 탐색이 필요하고 실현 가능하지만 3D 인체 포즈 작업에서 Transformer와 seq2seq 프레임워크를 결합하는 방법은 없다.
