### Abstract
본 논문은 GAN에 대한 새로운 학습방법론을 설명한다. 낮은 해상도에서부터 학습이 진전됨에 따라 세밀한 디테일들을 점진적으로 모델링하는 새로운 레이어를 G/D 모두에 더해가는 것이다.  
전례없는 품질의 이미지를 생성하게 되면서 학습속도가 증가하고 매우 안정화된 학습을 수행한다.  
생성 이미지에서의 변화를 증가시키는 간단한 방법도 제안하며 CIFAR10 데이터셋에 대해서 8.8의 inception score를 기록한다.  
추가로 G와 D 사이의 학습에 부정적인 원치않는 경쟁에 있어 중요한 여러 구현 디테일들을 설명한다.  
마지막으로 GAN결과를 평가하기 위한 새로운 이미지 질과 다양성에 성능지표를 소개한다.  
추가 기여로, 고품질의 CELEBA 데이터셋 버전을 구성한다.  

### 1. INTRODUCTION 
이미지와 같은 고차원 데이터 분포에서 새로운 샘플을 생성하는 방법은 음성 합성, 이미지 대 이미지 번역 및 이미지 인 페인팅에서 널리 사용되고 있다.  
당시 대표적인 방법론은 autoregressive, VAE 및 GAN이 있다. 각자 장단점을 가지고 있다.   
1. **Autoregressive model**(e.x., PixelCNN) :  
 1) 선명한 이미지 생성
 2) 느린 평가속도
 3) 픽셀에 대한 조건부 분포를 직접 모델링하여 잠재적으로 적용 가능성을 제한  
2. **VAE**
 1) 학습하기 쉬움
 2) 최근 작업이 개선되고 있지만 모델의 제한 흐릿한 결과를 생성함
3. **GAN**
1) 매우 작은 해상도와 다소 제한적인 변형으로만 선명한 이미지를 생성
2) 경쟁적 학습방식으로 학습 안정성이 불안정함

(세가지 방법의 장점을 결합한 방법도 있으나 품질에서는 GAN에 뒤쳐짐)

**GAN에 대한 일반적 설명**  
G, D(aka critic)의 두 네트워크로 구성됨  
G는 latent code에서 이미지를 생성하며 이러한 이미지의 분포는 이상적으로는 학습 분포와 구분이 불가능하다.   
실제로 그러한지 구분하는 함수를 설계하는것은 불가능하기 때문에 D는 이를 평가하는데 학습한다. 그리고 이 네트워크는 미분가능하므로 gradient를 통한 올바른 방향으로의 학습이 가능하다.
D는 G가 학습되면 사용되지 않는다.
이 구성에는 여러 잠재적인 문제가있다. 
훈련 분포와 생성 된 분포 사이의 거리를 측정 할 때 분포가 실질적으로 겹치지 않는 경우 (즉, 너무 쉽게 알 수있는 경우) 기울기는 다소 임의의 방향을 가리킬 수 있다.  
원래 JS divergence는 거리 메트릭으로 사용되었으며 최근에는 least square, 마진을포함하는 절대편차, Wasserstein 거리를 포함하여 여러 안정적인 방법론이 제안되었다.  
본 논문은 진행중인 논의에 대체로 직교하며 **향상된 Wasserstein loss**를 주로 사용하지만 least-squre loss를 실험한다.  

**제시 방법론**
고해상도 이미지는 실제/생성이미지 구분이 쉽기 때문에 gradient 문제가 증폭되는 문제가 있으며 이로인해 고해상도 이미지 생성이 어렵다.  
해상도가 커지면 batch 크기를 상대적으로 낮춰야하는데 이때문에 학습안정성이 저하되기도 한다.  
본 논문의 아이디어는 저해상도 이미지부터 시작하여 G와 D를 점진적으로 학습시키고 훈련진행에 따라 고해상도 세부 정보를 도입하는 새로운 레이어를 추가 하는 것이다.  
이것은 Section2 에서 논의 할 것처럼 훈련속도를 크게 높이고 고해상도 안정성을 향상시킨다.  
GAN 구조의 결과물로 G는 전체 훈련 데이터 분포를 나타낼 것을 명시적으로 요구하지 않는다.  
이미지 품질과 변형제어 사이에 trade-off 관계가 있다는것이 일반적인 지식이나 최근 그러한 경향에 대한 해결책이 제시 되어지고 있다.  

보존된 변이의 수준은 주목을 받고 있으며, inception score, multi-scale structural similarity (MS-SSIM), birthday paradox,  
explicit tests for the number of discrete modes discovered 등이 측정하기 위한 방법들로 제안되었다.  
Section 3에서 변형을 장려하는 방법을 설명한다.  
Section 5에서 품질과 변형을 평가하기위한 새로운 메트릭을 제안한다.  
Section 4.1에서는 미세한 네트워크 초기화를 통한 여러 개층의 균형 잡힌 학습 속도를 제공한다.  
또한, mode collapses 현상은 전통적으로 작은 미니배치에서 매우 빠르게 발생하여 GAN의 학습을 저해하는것을 확인한다.  
일반적으로 D가 overshoot하여 과장된 기울기로 이어질 때 시작되며 두 네트워크에서 신호 크기가 증가하는 비정상적인 경쟁이 뒤 따른다.
G가  이러한 에스컬레이션에 참여하지 못하도록하여 문제를 극복하는 메커니즘을 제안한다. (Section4.2).  
CELEBA, LSUN, CIFAR10 데이터 세트를 사용하여 논문의 기여를 평가한다. 특히, CIFAR10에 대해 최고의 inception score를 제시한다.  
생성 방법을 벤치마킹하는 데 일반적으로 사용되는 데이터 세트는 상당히 낮은 해상도로 제한되어 있으므로 
최대 1024 × 1024 픽셀의 출력 해상도로 실험 할 수있는 CELEBA 데이터 세트의 고품질 버전도 생상한다.  

### 2 PROGRESSIVE GROWING OF GANS
본 논문의 핵심 아이디어는 저해상도에서 시작해서 그림1에서 보이는것처럼 레이어를 추가하여 해상도를 점진적으로 높이는 GAN 학습 방법론이다.  
![image](https://user-images.githubusercontent.com/40943064/120928033-044c7d00-c71e-11eb-9535-17c7501c236f.png)  
이러한 점진적 특성을 통해 학습은 최초 큰 틀의 구조를 학습한다. 모든 스케일을 동시에 학습 하지 않고 점점 더 미세한 스케일 세부 사항으로 확장한다.  
서로의 거울 이미지이며 항상 동기화되어 성장하는 생성기 및 판별 기 네트워크를 사용한다.  
각 네트워크는 학습과정에서 모든 계층이 학습가능상태로 유지된다.  
새로운 레이어가 네트워크에 추가되면 그림2와 같이 부드럽게 페이드인 된다.  
![image](https://user-images.githubusercontent.com/40943064/120928049-13332f80-c71e-11eb-85c2-34344b86a202.png)  
이를통해 잘 훈련 된 더 작은 해상도 레이어에 대한 갑작스러운 충격을 방지 할 수 있다.  
Appendix A는 다른 훈련 매개 변수와 함께 생성기 및 판별 기의 구조를 자세히 설명한다.  
우리는 점진적 훈련이 몇 가지 이점이 있음을 관찰한다. 
초기에 더 작은 이미지의 생성은 클래스 정보가 적고 모드가 적기 때문에 훨씬 더 안정적이다. 
해상도를 조금씩 늘림으로써 우리는 latent 벡터에서 예를 들어 매핑을 발견하는 최종 목표에 비해 훨씬 더 간단한 질문을 지속한다.  
이 접근법은 Chen의 최근 작업과 개념적으로 유사하다. 
실제로는 WGAN-GP 손실 및 LSGAN 손실을 사용하여 mega-pixel 스케일 이미지를 안정적으로 합성 할 수 있도록 훈련을 충분히 안정화한다.
