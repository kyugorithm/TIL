# Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion

## Abstract 
낮은 latency의 실시간 오디오 입력을 통해 3D 얼굴 애니메이션을 만드는 ML 기법을 제시한다.  
DL은 입력 **음성파형**을 얼굴 모델의 **3D vertex 좌표축**으로 mapping 한다.  
동시에 오디오만으로 설명할 수 없는 표정 변화를 모호하게 하는 compact하고 latent한 code를 찾아낸다.  
추론 과정에서 latent 코드는 얼굴 모델의 감정 상태를 직관적으로 제어하는데 사용된다.  
전통적인 vision 기반의 performance capture method를 사용하여 3-5분의 고품질 애니메이션 데이터를 얻어 네트워크를 학습한다.  
우선적인 목표가 단일 actor의 말하기 스타일을 모델링하는것이지만  
우리 모델은 심지어 다른 성별, 강쇠, 언어등의 변화를 가진 오디오를 이용할 때에도 합리적인 결과를 만들어 낸다.  
## Introduction
표현력 있는 얼굴 애니메이션은 **영화와 디지털 게임**에서 필수적이다.  
시각 기반 성능 캡처, 즉 인간 행위자의 관찰된 동작으로 애니메이션 얼굴을 움직이는 것은 대부분의 생산 파이프라인의 필수적인 구성 요소이다.  
캡처 시스템에서 얻을 수 있는 품질은 꾸준히 개선되고 있지만 고품질 안면 애니메이션 제작 비용은 여전히 높다.  
첫째, CV 시스템은 정교한 설정과 종종 노동 집약적인 정리 및 기타 처리 단계가 필요하다.  
두 번째, 덜 분명한 문제는 새로운 촬영이 녹음될 때마다 배우들이 장소에 있어야 하고 이상적으로 그들의 외모를 유지해야 한다는 것이다.  
예를 들어, 다른 역할이 수염을 기르도록 요구한다면 이것은 어려울 수 있다.  
오디오 기반 성능 캡처 알고리즘은 비전 시스템의 품질과 결코 일치하지 않을 것으로 보이지만, 보완적인 강점을 제공한다.  
가장 중요한 것은 많은 현대 게임에서 게임 내 캐릭터가 말하는 수십 시간의 대화는 비전 기반 시스템을 사용하여 제작하기에는 너무 비싸다.  
결국 일반적으로 영상학, 비전 시스템 등을 사용하여 주요 애니메이션만 제작하고 게임 내 자료의 대부분을 제작하기 위해 
오디오와 대본을 기반으로 하는 시스템에 의존하는 것이다.  
불행하게도, 그러한 시스템에 의해 제작된 애니메이션의 품질은 현재 아쉬운 점이 많다.  
또한 텔레프레전스 및 가상현실 아바타 분야에서 새롭게 부상하는 실시간 애플리케이션은 스크립트 부족, 
사용자 음성 및 물리적 설정의 광범위한 변동성, 엄격한 대기 시간 요구 사항으로 인해 추가적인 과제를 안고 있다. 
우리의 목표는 보컬 오디오 트랙만을 기반으로 그럴듯하고 표현력 있는 3D 얼굴 애니메이션을 만드는 것이다.
결과가 자연스럽게 보이려면, 애니메이션은 **음소 공동화, 어휘적 스트레스 및 얼굴 근육과 피부 조직 사이의 상호작용을 포함한 복잡하고**  
**공의존적인 현상을 설명해야 한다**. 따라서, 우리는 입과 입술뿐만 아니라 얼굴 전체에 초점을 맞춘다. 
우리는 학습 데이터에서 관찰된 관련 효과를 복제하기 위해 end-to-end 방식으로 심층 신경망을 훈련시키는 데이터 중심 접근법을 채택한다.  
처음에는 이 문제가 내재된 모호성 때문에 다루기 어려워 보일 수 있다. 동일한 소리는 매우 다른 얼굴 표정으로 말할 수 있고, 
오디오 트랙에는 단지 다른 변형을 구별할 수 있는 충분한 정보가 포함되어 있지 않다. 
현대의 CNN은 다양한 추론 및 분류 작업에서 매우 효과적이라는 것이 입증되었지만, 
학습 데이터에 모호성이 있는 경우 평균으로 회귀하는 경향이 있다.  
이러한 문제를 해결하기 위해 다음과 같은 세 가지 주요 기여도를 제시한다.  

• 인간의 음성을 효과적으로 처리하고 다른 speker에 대해 일반화하도록 조정된 CNN 구조   

• 오디오만으로는 설명할 수 없는 학습 데이터의 변화, 즉 명백한 감정 상태를 발견할 수 있는 새로운 방법   

• 모호한 학습 데이터에도 불구하고 애니메이션 하에서도 네트워크가 일시적으로 안정되고 응답성을 유지할 수 있는 3 way loss function  

방법은 짧은 대기 시간으로 오디오에서 표현력 있는 3D 얼굴 동작을 실시간으로 생성한다.  
다운스트림 애니메이션 시스템의 세부 정보로부터 최대의 독립성을 유지하기 위해,  
per-frame positions of the control vertices of a fixed topology facial mesh를 출력한다.  
압축, 렌더링 또는 편집 가능성에 필요한 경우 혼합 형상 또는 비선형 리그와 같은 대체 인코딩을 이후 파이프라인 단계에서 도입할 수 있다.  
기존의 비전 기반 성능 캡처 방법을 사용하여 얻은 3~5분 분량의 고품질 영상으로 모델을 학습한다.  
목표는 단일 배우의 말하기 스타일을 모델링하는 것이지만, 모델은 성별, 억양 또는 언어가 다른 다른 다른 스피커의 오디오로 구동될 때에도 합리적인 결과를 산출한다.  
게임 내 대화, 저비용 localization, VR 및 telepresence에서 이 기술을 사용하는 것을 볼 수 있다.  
그것은 또한 영화학에서조차 작은 스크립트 변화를 수용하는 데 유용할 수 있다.  

## 2 RELATED WORK
입력이 오디오나 텍스트이고 출력이 2D 비디오 혹은 3D 애니메이션인 시스템의 사전 연구를 검토한다.  
방법론을 linguistic and machine learning based model 기반으로 분할하고 명확한 감정상태를 보조하는 방법들을 review한다.  

Models based on linguistics
Models based on machine learning
Extracting and controlling the emotional state
Residual motion

## 3 END-TO-END NETWORK ARCHITECTURE
오디오 처리 및 음성 콘텐츠에서 감정 상태의 분리에 대한 세부 정보와 함께 네트워크 아키텍처를 설명한다.  
짧은 오디오에서 네트워크의 임무는 window 중앙에서 표정을 추론하는 것이다.  
fixed-topology face mesh의 neutral 포즈에서 vertex 별 차이 벡터로 직접 표정을 나타낸다.  
네트워크가 학습되면 보컬 오디오 트랙 위로 window를 밀어 mesh를 애니메이션하고 각 시간 단계에서 네트워크를 독립적으로 평가한다.  
네트워크 자체에는 과거 애니메이션 프레임에 대한 메모리가 없지만 실제로는 시간적으로 안정적인 결과를 생성한다.  

## Future work

1) 주요 목표  
: 게임 내 대화를 위한 고품질의 얼굴 애니메이션 생성 (제작 환경에서 실질적인 가치를 계속 평가할 계획)  
2) 다양한 조건에서 실험 수행  : 
소비자 수준의 장비와 다양한 수준의 배경 소음으로 캐주얼 환경에서 녹음된 오디오로  
범용적인 사용에 대한 방법의 적합성을 측정하기 위해 몇 가지 비공식 실험수행  
3) 적용가능 상황  
일반적으로, 우리의 방법은 입력 볼륨 레벨이 학습 데이터와 대략 일치하도록 정규화된 한 반응성을 유지하고,  
애니메이션이 오디오와 동기화되고 스피치의 속도가 너무 빠르지 않은 한 그럴듯해 보인다.  
4) 목표 
추후 여러명의 연사가 있는 실제 대화환경에서 이러한 효과와 관련 효과에 대한 보다 원칙적인 연구를 볼 수 있기를 바란다.  
방법의 주요 단점이 **performance capture data에 존재하는 미세한 디테일의 부족**이라고 느낀다.  
우리의 접근 방식을 **Generative neural network와 결합**하면 그러한 세부 사항의 더 나은 합성과  
눈과 같은 잔류 운동을 가능하게 할 수 있다.  
우리의 방법은 단지 5분의 학습 데이터를 기반으로 몇 가지 다른 감정 상태에 대해 그럴듯한 결과를 생성할 수 있지만,  
데이터 세트의 크기를 늘리면 결과가 훨씬 더 개선될 것이다. 문자 정체성의 잠재적이고 통일된 표현을 학습하기 위해  
여러 다른 문자에 대해 네트워크를 동시에 훈련시키는 것은 특히 흥미로울 것이다.  
또한 긴 오디오 컨텍스트를 기반으로 추론 중에 감정 상태를 자동으로 추론할 수 있을 것으로 보인다.

