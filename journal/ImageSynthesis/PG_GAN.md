### Abstract
GAN의 새로운 학습방법론 제시  
낮은 해상도에서부터 학습이 진전됨에 따라 세밀한 디테일들을 **점진적으로 모델링하는 새로운 레이어**를 생성기/판별기 모두에 더해간다.  
**높은 품질**의 이미지를 생성하게 되면서 **학습속도가 증가**하고 **안정적**으로 학습을 수행한다.  
생성 이미지에서의 변화를 증가시키는 간단한 방법도 제안하며 CIFAR10 데이터셋에 대해서 8.8의 inception score를 기록한다.  
추가로 G와 D 사이의 학습에 부정적인 원치않는 경쟁에 있어 중요한 여러 구현 디테일들을 설명한다.  
마지막으로 결과를 평가하기 위한 **새로운** 이미지 품질과 다양성 **지표**를 소개한다.  
추가로, 고품질의 CELEBA 데이터셋 버전을 구성한다.  
  
핵심 : (점진적 레이어 추가 학습방식) → 이미지 생성의 (**품질향상**, **학습속도 증가**, **안정성 확보**)  

### 1. INTRODUCTION 
대표적인 방법론은 autoregressive, VAE 및 GAN이 있으며 각자 장단점을 가지고 있다.   
Autoregressive models : sharp images, slow to evaluate, no latent space  
VAE : fast to train, blurry images  
<img src = 'https://user-images.githubusercontent.com/40943064/150683946-d75b96dc-454a-4d3b-b7b5-7bdf2612bd00.png' width = 200>  
GANs : sharp images, low resolutioin, limited variation, unstabble training  
(세가지 방법의 장점을 결합한 방법도 있으나 품질에서는 GAN에 뒤쳐짐)  
  
**GAN에 대한 일반적 설명**  
G, D 두 네트워크로 구성됨  
G는 latent code에서 이미지를 생성하며 생성한 이미지의 분포는 이상적으로는 실제 분포와 구분이 불가능하다.  
실제인지 구분하는 함수를 설계하는것은 불가능하기 때문에 D는 이를 평가하는 방법을 학습한다.  
D는 미분가능하므로 gradient를 통한 학습이 가능하다.(D는 학습후에 사용되지 않음)  

### GAN의 구성에는 내재하는 잠재 문제들

**Distance metric**   
실제 분포와 생성 분포 사이의 거리를 측정 할 때 분포가 겹쳐지지 않는 경우 기울기는 임의의 방향을 가리킬 수 있다.  
JS divergence는 거리 메트릭으로 사용되었으며 최근에는 least square, margin을 포함하는 절대편차, Wasserstein 거리를 포함하여  
여러 안정적인 방법론이 제안되었다.  
본 논문은 진행중인 논의에 대체로 독립적이며 **향상된 Wasserstein loss**를 주로 사용하지만 least-squre loss를 실험한다.  

**학습 안정성 문제**   
고해상도 이미지는 실제/생성이미지 구분이 쉽기 때문에 gradient가 증폭되는 문제가 있어 이미지 생성이 어렵다.  
해상도가 커지면 batch 크기를 상대적으로 낮춰야하는데 이 때문에 학습 안정성이 저하되기도 한다.  
본 논문의 아이디어는 저해상도 이미지부터 시작하여 G와 D를 점진적으로 학습시키고 학습 진행에 따라  
고해상도 세부 정보를 도입하는 새로운 레이어를 추가 하는 것이다.  
이는 학습 속도를 크게 높이고 고해상도 안정성을 향상시킨다.(S.2)  

**다양성**  
이미지 품질과 다양성 사이에 trade-off 관계가 있다는것이 일반적인 지식이나 최근 이에대한 해결책이 제시되고 있다.  
보존된 다양성수준은 주목을 받고 있으며, inception score, multi-scale structural similarity (MS-SSIM), birthday paradox,  
explicit tests for the number of discrete modes discovered 등이 측정하기 위한 방법들로 제안되었다.  
S3 : 다양성을 장려하는 방법 제안

**학습속도**  
S4.1 : 미세한 네트워크 초기화를 통한 여러 layer의 균형 잡힌 학습 속도를 제공  

**네트워크 비정상적 경쟁학습**  
Mode collapses 현상은 전통적으로 작은 mini-batch에서 매우 빠르게 발생하여 GAN의 학습을 저해하는것을 확인한다.  
일반적으로 D가 overshoot하여 과도한 기울기로 이어질 때 시작되며  
두 네트워크에서 신호 크기가 증가하는 비정상적인 경쟁이 뒤 따른다.  
S4.2 : G가 이러한 문제에 참여하지 못하도록하여 문제를 극복하는 메커니즘을 제안한다.  
![image](https://user-images.githubusercontent.com/40943064/150684637-b0db3c98-cd94-4fbc-afb0-8137654dba05.png)

**품질 평가 메트릭**  
S5 : 품질과 다양성을 평가하기 위한 새로운 메트릭  


**데이터 세트의 적용**  
CELEBA, LSUN, CIFAR10 데이터 세트를 사용하여 논문의 기여를 평가한다. 특히, CIFAR10에 대해 최고의 IS를 제시한다.  
생성 방법을 벤치마킹하는 데 일반적으로 사용되는 데이터 세트는 상당히 낮은 해상도로 제한되어 있으므로  
최대 1024 × 1024 픽셀의 출력 해상도로 실험 할 수있는 CELEBA 데이터 세트의 고품질 버전도 생성한다.  
  
### 2 Progressive growing of GANs  
핵심 아이디어는 저해상도부터 시작해서 레이어를 추가하여 해상도를 점진적으로 높이는 GAN 학습 방법론이다.  
![image](https://user-images.githubusercontent.com/40943064/120928033-044c7d00-c71e-11eb-9535-17c7501c236f.png)  
점진적 특성을 통해 최초로 큰 스케일의 구조를 학습한다.  
모든 스케일을 동시에 학습 하지 않고 점점 더 미세한 스케일 세부 사항으로 확장한다.  
서로의 거울 이미지이며 항상 동기화되어 성장하는 G 및 D를 사용한다.  
각 네트워크는 학습과정에서 모든 계층이 학습가능 상태로 유지된다.  
새로운 레이어가 네트워크에 추가되면서 그림2와 같이 부드럽게 페이드인 된다.  
![image](https://user-images.githubusercontent.com/40943064/120928049-13332f80-c71e-11eb-85c2-34344b86a202.png)  
이를통해 잘 훈련 된 더 작은 해상도 레이어에 대한 갑작스러운 충격을 방지 할 수 있다.  
Appendix A는 다른 훈련 매개 변수와 함께 G 및 D의 구조를 자세히 설명한다.  
우리는 점진적 학습이 몇 가지 이점이 있음을 관찰한다.  
초기에 더 작은 이미지의 생성은 클래스 정보가 적고 모드가 적기 때문에 훨씬 더 **안정적**이다.  
해상도를 조금씩 늘림으로써 우리는 latent 벡터에서 예를 들어 매핑을 발견하는 **최종 목표에 비해 훨씬 더 간단한 질문을 지속**한다.  
WGAN-GP 손실 및 LSGAN 손실을 사용하여 mega-pixel 스케일 이미지를 안정적으로 합성 할 수 있도록 학습을 충분히 안정화한다.  
  
또 다른 이점은 학습시간 단축이다. 점진적으로 증가하는 GAN으로 인해 대부분의 반복은 더 낮은 해상도에서 수행되며  
최종 출력 해상도에 따라 비슷한 품질을 최대 **2 ~ 6 배 더 빠르게** 얻을 수 있다.  
점진적으로 GAN을 성장시키는 아이디어는 pix2pixHD에서 여러 해상도에 서로다른 D를 사용하는 방식과 관련이 있다.  
<img src = "https://user-images.githubusercontent.com/40943064/150685549-c4df7455-c9a0-42b1-8507-3743f8c6be32.png" width = 400>  
위 아이디어는 하나의 G와 여러 D를 사용하는 Durugkar의 연구나 여러개의 G와 하나의 D를 사용하는 Ghosh의 연구에서 참고하였다.  
![image](https://user-images.githubusercontent.com/40943064/150685714-eb356a06-9700-4194-aaa1-e9e52aa41d17.png)
Hierarchical GAN은 이미지 피라미드의 각 수준에 대해 G와 D를 정의한다.  
<img src = https://user-images.githubusercontent.com/40943064/150685939-a5d5db36-a553-4cad-a838-56a5b4189666.png width = 600>  
이러한 방법은 latent에서 고해상도 이미지로의 복잡한 매핑이 단계적으로 학습하기 더 쉽다는 우리 작업과  
동일한 관찰을 기반으로하지만 중요한 차이점은 계층 구조 대신 단일 GAN 만 있다는 것이다.  
Adaptive하게 성장하는 네트워크에 대한 초기 작업,  
예를 들어 성장하는 neural gas 및 네트워크를 탐욕스럽게 성장시키는 증강 토폴로지의 neuro evolution과는 대조적으로,  
단순히 미리 구성된 레이어의 도입을 사용한다. 그런 의미에서 AE의 레이어 별 학습과 유사하다.  
![image](https://user-images.githubusercontent.com/40943064/150679673-06591815-a7cc-4f5c-b8cc-1405f5c17818.png)

https://github.com/happy-jihye/happy-jihye.github.io/blob/master/_posts/images/gan/pggan1.gif?raw=1

## 3. Increasing Variation using Minibatch Standard Deviation
GAN은 학습 데이터에서 발견되는 변이의 하위 집합만 캡처하는 경향이 있으며  
Salimans는 솔루션으로 "minibatch discrimination"을 제안한다.  
개별 이미지뿐만 아니라 미니 배치 전체에서 기능 통계를 계산하므로  
생성된 이미지와 학습 이미지의 미니 배치가 유사한 통계를 표시하도록 권장한다.  
이것은 D의 끝에 미니배치 레이어를 추가하여 구현된다. 여기서 레이어는 입력 활성화를 통계 배열에 투영하는 큰 tensor를 학습한다.  
미니배치의 각 예제에 대해 별도의 통계 세트가 생성되고 계층의 출력에 연결되므로 D가 내부적으로 통계를 사용할 수 있다.  
변형을 개선하는 동시에 이 접근 방식을 크게 단순화한다.  
  
우리의 단순화된 방법은, 학습 가능한 매개변수나 새로운 hyperparameter가 없다.  
먼저 미니 배치에 대한 각 공간 위치의 각 기능에 대한 표준 편차를 계산한다.  
그런 다음 모든 기능 및 공간 위치에 대해 이러한 추정치를 평균화하여 단일 값에 도달한다.  
값을 복제하고 모든 공간 위치와 미니 배치에 연결하여 하나의 추가(상수) 기능 맵을 생성한다.  
이 레이어는 판별기의 아무 곳에나 삽입할 수 있지만 끝으로 삽입하는 것이 가장 좋다(appendix A.1 참조).  
더 풍부한 통계 세트로 실험했지만 변형을 더 이상 개선할 수 없었다.  
병렬 작업에서 Lin은 D에게 여러 이미지를 표시할 때의 이점에 대한 이론적 통찰력을 제공한다.

다양성 문제에 대한 대안에는 업데이트를 정규화하기 위한 D unrolling과 G에 새로운 loss 항을 추가하여 mini-batch에서  
feature 벡터를 orthogonalize화하도록 권장하는 " repelling regularizer"가 포함된다.  
Ghosh의 여러 G도 비슷한 목표를 달성한다. 우리는 이러한 솔루션이 우리 솔루션보다 훨씬 더 변동을 증가시킬 수 있다는 것을 인정한다.  

## 4. Normalization in Generator and Discriminator
GAN은 두 네트워크 간의 비정상적 경쟁의 결과로 신호 크기가 확대되는 경향이 있다.  
과거에는 대부분 G 혹은 D에서 batch normalization의 변형을 사용하여 이 문제를 막으려 했다.  
이 방식은 원래 covariance shift를 제거하기 위해 도입되었다.  
그러나 우리는 GAN에서 문제가 되는 것을 관찰하지 않았으므로 GAN의 실제 필요성이 신호 크기와 경쟁을 제한하고 있다고 믿는다.  
학습 가능한 매개변수를 포함하지 않는 두 가지 요소로 구성된 다른 접근 방식을 사용한다.  

4.1 Equalized learning rate
우리는 신중한 가중치 초기화의 현재 경향에서 벗어나 대신 사소한 N(0, 1) 초기화를 사용한 다음 런타임에 가중치를 명시적으로 확장한다.  
정확히는 wˆi = wi/c로 설정한다. (wi : weight, c : He의 초기화 프로그램에서 가져온 레이어별 정규화 상수)  
초기화 대신 동적으로 수행하는 이점은 다소 미묘하며 RMSProp 및 Adam과 같이 일반적으로 사용되는 적응 확률적 경사 하강법의 규모 불변성과 관련이 있다.  
이러한 방법은 추정된 표준 편차로 gradient 업데이트를 정규화하므로 업데이트를 매개변수의 규모와 무관하게 만든다.  
결과적으로 일부 매개변수의 동적 범위가 다른 매개변수보다 크면 조정하는 데 시간이 더 오래 걸린다.  
이것은 최신 initializer가 유발하는 시나리오이므로 학습률이 동시에 너무 크고 너무 작을 수 있다.  
우리의 접근 방식은 동적 범위와 이에 따른 학습 속도가 모든 가중치에 대해 동일함을 보장한다.  
유사한 추론이 van Laarhoven(2017)에 의해 독립적으로 사용되었다.

4.2 Pixelwise feature vector normalization in generator
G의 픽셀 단위 feature vector normalization G 및 D의 크기가 경쟁의 결과로 통제 불능 상태가 되는 시나리오를 허용하지 않기 위해  
각 conv. 레이어 이후에 G의 단위 길이로 각 픽셀의 feature vector를 normalize한다.  
다음과 같이 구성된 "로컬 응답 정규화"의 변형을 사용하여 이 작업을 수행한다.  
![image](https://user-images.githubusercontent.com/40943064/150680873-d69d60d3-ef40-4ebd-a867-cf0294b0468c.png)  
(epsilon: 10^-8, N : feature map 개수, ax,y 및 bx,y : 픽셀(x, y)의 원본 및 정규화된 feature vector)  
우리는 이 무거운 제약이 어떤 식으로도 G에 해를 끼치지 않는 것처럼 보이며  
실제로 대부분의 데이터 세트에서 결과를 많이 변경하지 않는다는 것이 놀랍다는 것을 알게 되었지만  
필요할 때 신호 크기의 상승을 매우 효과적으로 방지한다.

## 5. Multi-Scale Statistical Similarity for Assessing GAN Results
한 GAN의 결과를 다른 GAN과 비교하려면 지루하고 어렵고 주관적일 수 있는 많은 수의 이미지를 조사해야 한다. 
따라서 대규모 이미지 컬렉션에서 일부 지표를 계산하는 자동화된 방법에 의존하는 것이 바람직하다. 
MS-SSIM과 같은 기존 방법은 대규모 모드 붕괴를 안정적으로 찾았지만 색상이나 질감의 변화 손실과 같은 작은 효과에는 반응하지 않으며 
학습세트와 유사성 측면에서 이미지 품질을 직접 평가하지 않는다. 
우리는 성공적인 G가 모든 스케일에 대한 학습 세트와 유사한 로컬 이미지 구조를 갖는 샘플을 생성할 것이라는 직관을 기반으로 한다.  
우리는 16 × 16 픽셀의 low-pass 해상도에서 시작하여 생성된 이미지와 대상 이미지의 Laplacian 피라미드(Burt & Adelson, 1987)  
표현에서 가져온 로컬 이미지 패치의 분포 사이의 다중 스케일 통계적 유사성을 고려하여 이를 연구할 것을 제안한다.  
표준 관행에 따라 피라미드는 전체 해상도에 도달할 때까지 점진적으로 두 배가 되며,  
각 연속 레벨은 차이를 이전 레벨의 업샘플링된 버전으로 인코딩한다.  
단일 라플라시안 피라미드 레벨은 특정 공간 주파수 대역에 해당한다.  
우리는 16384개의 이미지를 무작위로 샘플링하고 라플라시안 피라미드의 각 레벨에서 128개의 설명자를 추출하여  
level당 2^21(2.1M)개의 descriptor를 제공한다.

각 디스크립터는 x ∈ R 7×7×3 = R 147로 표시되는 3개의 색상 채널이 있는 7 × 7 픽셀 이웃이다.  
학습 세트와 생성 세트의 레벨 l에서 패치를 각각 {x l i } 2 21 i=1 및 {y l i } 2 21 i=1로 표시한다.  
먼저 {x l i } 및 {y l i } w.r.t를 정규화한다.  
각 색상 채널의 평균과 표준 편차를 확인한 다음 512개의 투영을 사용하여 슬라이싱된 Wasserstein 거리  
SWD({xli }, {yli })를 계산하여 통계적 유사성을 추정한다.  
직관적으로 작은 Wasserstein 거리는 패치의 분포가 유사함을 나타낸다.  
즉, 학습 이미지와 생성기 샘플이 이 공간 해상도에서 모양과 변형 모두에서 유사하게 나타난다.  
특히, 가장 낮은 해상도의 16 × 16 이미지에서 추출한 패치 세트 간의 거리는 대규모 이미지 구조에서 유사성을 나타내는 반면,  
가장 정밀한 패치는 가장자리의 선명도 및 노이즈와 같은 픽셀 수준 속성에 대한 정보를 인코딩한다.  

