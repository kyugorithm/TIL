### Abstract
GAN의 새로운 학습방법론 제시  
낮은 해상도에서부터 학습이 진전됨에 따라 세밀한 디테일들을 **점진적으로 모델링하는 새로운 레이어**를 생성기/판별기 모두에 더해가는 것이다.  
**높은 품질**의 이미지를 생성하게 되면서 **학습속도가 증가**하고 **안정적**으로 학습을 수행한다.  
생성 이미지에서의 변화를 증가시키는 간단한 방법도 제안하며 CIFAR10 데이터셋에 대해서 8.8의 inception score를 기록한다.  
추가로 G와 D 사이의 학습에 부정적인 원치않는 경쟁에 있어 중요한 여러 구현 디테일들을 설명한다.  
마지막으로 N결과를 평가하기 위한 **새로운** 이미지 품질과 다양성 **지표**를 소개한다.  
추가로, 고품질의 CELEBA 데이터셋 버전을 구성한다.  
  
핵심 : (점진적 레이어 추가 학습방식) → 이미지 생성의 (**품질향상**, **학습속도 증가**, **안정성 확보**)  

### 1. INTRODUCTION 
이미지와 같은 고차원 데이터 분포에서 새로운 샘플을 생성하는 방법은 음성 합성, I2I 번역 및 이미지 in-painting에서 널리 사용된다.  
대표적인 방법론은 autoregressive, VAE 및 GAN이 있으며 각자 장단점을 가지고 있다.   
  
![image](https://user-images.githubusercontent.com/40943064/150671554-913d48a9-9af2-4e98-a103-734617a7ea88.png)  
(세가지 방법의 장점을 결합한 방법도 있으나 품질에서는 GAN에 뒤쳐짐)

**GAN에 대한 일반적 설명**  
G, D 두 네트워크로 구성됨  
G는 latent code에서 이미지를 생성하며 이러한 이미지의 분포는 이상적으로는 학습 분포와 구분이 불가능하다.  
실제로 그러한지 구분하는 함수를 설계하는것은 불가능하기 때문에 D는 이를 평가하는데 학습한다.  
D는 미분가능하므로 gradient를 통한 학습이 가능하다.  
D는 G가 학습되면 사용되지 않는다.  

이러한 GAN의 구성에는 여러 잠재적인 문제가 몇가지 있다.  
학습 분포와 생성 분포 사이의 거리를 측정 할 때 분포가 실질적으로 겹치지 않는 경우 기울기는 임의의 방향을 가리킬 수 있다.  
JS divergence는 거리 메트릭으로 사용되었으며 최근에는 least square, margin을 포함하는 절대편차, Wasserstein 거리를 포함하여  
여러 안정적인 방법론이 제안되었다.  
본 논문은 진행중인 논의에 대체로 독립적이며 **향상된 Wasserstein loss**를 주로 사용하지만 least-squre loss를 실험한다.  

**제시 방법론**
고해상도 이미지는 실제/생성이미지 구분이 쉽기 때문에 gradient가 증폭되는 문제가 있어 이미지 생성이 어렵다.  
해상도가 커지면 batch 크기를 상대적으로 낮춰야하는데 이 때문에 학습 안정성이 저하되기도 한다.  
본 논문의 아이디어는 저해상도 이미지부터 시작하여 G와 D를 점진적으로 학습시키고 학습 진행에 따라  
고해상도 세부 정보를 도입하는 새로운 레이어를 추가 하는 것이다.  
이것은 S.2 에서 논의 할 것처럼 학습 속도를 크게 높이고 고해상도 안정성을 향상시킨다.   

GAN 구조의 결과물로 G는 전체 학습 데이터 분포를 명시적으로 요구하지 않는다.  
이미지 품질과 변형제어 사이에 trade-off 관계가 있다는것이 일반적인 지식이나  
최근 그러한 경향에 대한 해결책이 제시 되어지고 있다.  
보존된 변이의 수준은 주목을 받고 있으며, inception score, multi-scale structural similarity (MS-SSIM), birthday paradox,  
explicit tests for the number of discrete modes discovered 등이 측정하기 위한 방법들로 제안되었다.  
Section 3에서 변형을 장려하는 방법을 설명한다.  
Section 5에서 품질과 변형을 평가하기위한 새로운 메트릭을 제안한다.  
Section 4.1에서는 미세한 네트워크 초기화를 통한 여러 개층의 균형 잡힌 학습 속도를 제공한다.  
또한, mode collapses 현상은 전통적으로 작은 미니배치에서 매우 빠르게 발생하여 GAN의 학습을 저해하는것을 확인한다.  
일반적으로 D가 overshoot하여 과장된 기울기로 이어질 때 시작되며 두 네트워크에서 신호 크기가 증가하는 비정상적인 경쟁이 뒤 따른다.
G가  이러한 에스컬레이션에 참여하지 못하도록하여 문제를 극복하는 메커니즘을 제안한다. (Section4.2).  
CELEBA, LSUN, CIFAR10 데이터 세트를 사용하여 논문의 기여를 평가한다. 특히, CIFAR10에 대해 최고의 inception score를 제시한다.  
생성 방법을 벤치마킹하는 데 일반적으로 사용되는 데이터 세트는 상당히 낮은 해상도로 제한되어 있으므로 
최대 1024 × 1024 픽셀의 출력 해상도로 실험 할 수있는 CELEBA 데이터 세트의 고품질 버전도 생상한다.  

### 2 PROGRESSIVE GROWING OF GANS
본 논문의 핵심 아이디어는 저해상도에서 시작해서 그림1에서 보이는것처럼 레이어를 추가하여 해상도를 점진적으로 높이는 GAN 학습 방법론이다.  
![image](https://user-images.githubusercontent.com/40943064/120928033-044c7d00-c71e-11eb-9535-17c7501c236f.png)  
이러한 점진적 특성을 통해 최초 큰 스케일의 구조를 학습한다. 모든 스케일을 동시에 학습 하지 않고 점점 더 미세한 스케일 세부 사항으로 확장한다.  
서로의 거울 이미지이며 항상 동기화되어 성장하는 생성기 및 판별 기 네트워크를 사용한다.  
각 네트워크는 학습과정에서 모든 계층이 학습가능상태로 유지된다.  
새로운 레이어가 네트워크에 추가되면 그림2와 같이 부드럽게 페이드인 된다.  
![image](https://user-images.githubusercontent.com/40943064/120928049-13332f80-c71e-11eb-85c2-34344b86a202.png)  
이를통해 잘 훈련 된 더 작은 해상도 레이어에 대한 갑작스러운 충격을 방지 할 수 있다.  
Appendix A는 다른 훈련 매개 변수와 함께 생성기 및 판별 기의 구조를 자세히 설명한다.  
우리는 점진적 훈련이 몇 가지 이점이 있음을 관찰한다. 
초기에 더 작은 이미지의 생성은 클래스 정보가 적고 모드가 적기 때문에 훨씬 더 안정적이다. 
해상도를 조금씩 늘림으로써 우리는 latent 벡터에서 예를 들어 매핑을 발견하는 최종 목표에 비해 훨씬 더 간단한 질문을 지속한다.  
이 접근법은 Chen의 최근 작업과 개념적으로 유사하다. 
실제로는 WGAN-GP 손실 및 LSGAN 손실을 사용하여 mega-pixel 스케일 이미지를 안정적으로 합성 할 수 있도록 훈련을 충분히 안정화한다.

또 다른 이점은 학습시간 단축이다. 점진적으로 증가하는 GAN으로 인해 대부분의 반복은 더 낮은 해상도에서 수행되며 최종 출력 해상도에 따라  
비슷한 품질을 최대 2 ~ 6 배 더 빠르게 얻을 수 있다. 점진적으로 GAN을 성장시키는 아이디어는 Wang의 여러 해상도에 서로다른 D를 사용하는 방식과 관련이 있다.  
이 아이디어는 하나의 G와 여러 D를 사용하는 Durugkar의 연구나 여러개의 G와 하나의 D를 사용하는 Ghosh의 연구에서 참고하였다.  
Hierarchical GAN은 이미지 피라미드의 각 수준에 대해 G와 D를 정의한다.  
이러한 방법은 latent에서 고해상도 이미지로의 복잡한 매핑이 단계적으로 학습하기 더 쉽다는 우리 작업과 동일한 관찰을 기반으로하지만  
중요한 차이점은 계층 구조 대신 단일 GAN 만 있다는 것이다.  

Adaptive하게 성장하는 네트워크에 대한 초기 작업, 예를 들어 성장하는 신경 가스 및 네트워크를 탐욕스럽게 성장시키는 증강 토폴로지의 신경 진화와는 대조적으로, 우리는 단순히 미리 구성된 레이어의 도입을 사용한다. 그런 의미에서 AE의 계층 별 학습과 유사하다.
