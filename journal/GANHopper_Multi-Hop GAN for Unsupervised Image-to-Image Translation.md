## Abstract. 
multiple hops를 통해 두 도메인간 이미지를 점진적으로 변화하는 방법을 제안한다.  
변환을 직접 실행하는 대신 입력 도메인의 이미지 간 weigthed-hybrid와 유사한 중간 이미지를 생성하도록 요구하여 번역을 조정한다.  
네트워크는 중간 이미지 없이 두 도메인의 페어링되지 않은 이미지에 대해서만 훈련된다.  
모든 hop은 각 방향을 따라 단일의 G를 사용하여 생성된다.  
Cyc. loss와 Adv. Loss 외에도 새로운 hybrid D를 도입하여 G에 의해 생성된 중간 이미지를  
사전정의된 hop 수를 기반으로 하는 가중치를 사용하여 가중 하이브리드로 분류하도록 학습되었다.
또한 각 hop의 magnitude를 제한하는 smootheness 항을 추가하여 변환을 더욱 정규화한다.  
이전 방법과 비교하여 GANHopper는 도메인별 이미지 feature 및 geometry 변환과 관련된  
이미지 변환에 탁월하며 일반 색 구성표와 같은 비도메인별 기능도 보존한다.

## 1. Introduction

기존 방법론(CycleGAN, DualGAN)은 개/고양이 변환 같은 geometry and shape에 취약해서 pixel색상/texture 변환만 가능하다. (그림1)  
도메인 차이가 클수록 변환학습이 복잡하고 어려울 것으로 예상되며 pair 이미지를 지도 학습하지 않고서는 변환에 대한 탐색공간은 너무나도 거대할 수 있다.   
큰 이미지 변환으로 설명할 자유도는 더 높기 때문에 더 큰 제약을 가하고 더 조절 가능한 탐색이 필요하다.  
본 논문에서는 개고양이와 같은 도메인 사이에서 점진적으로 이미지를 변환하도록 제약하는 이미지 변환 방법을 제안한다.  
직접변환을 수행하는 것 대신에, hop이라 표현하는 단계별 task를 수행한다.  
본 multi-hop network 구조는 CycleGAN으로 구성된다.  
대신 두 입력 도메인의 이미지 간의 weighted hybrid와 유사한 이미지를 생성하도록 강제하여 번역 경로를 조정한다.  
예를 들어 4-hop 네트워크를 이용하여 25%/75%, 50%/50%, 75%/25%의 개/고양이 이미지를 생성한다. 마지막 단계는 100% 변환된 고양이 이미지가 된다.  
GANHopper는 학습 세트에 중간 하이브리드 이미지가 없이도 unpaired 세팅으로 학습이 가능하다.  
또 중요한 것은 모든 hop이 각 방향을 따라 단일 G를 사용하여 생성되므로 네트워크 용량이 CycleGAN의 용량을 초과하지 않는다는 것이다.  

학습을 가능하게 하기 위해 이미지에 대해서만 훈련되어 정의한 hop 수에 따라 가중치 하이브리드로 분류하여 중간 이미지를 평가하는 새로운 hybrid D를 도입한다.  
CycleGAN의 기존 loss에 아래 loss 두개를 추가한다.  
1.	Hybrid loss: 이미지가 입력 도메인 중 하나에 속하는 정도를 평가  
2.	Smoothness loss : hop 시퀀스에서 생성된 이미지가 이전 이미지에서 크게 벗어나지 않도록 이미지 전환을 조절  
GANHopper는 단순히 입력 고양이를 개로 변환하지 않는다. 많은 개가 D를 속일 수 있지만 특히   
주어진 고양이와 비슷한 개를 생성하는 것을 목표로 한다. 그림 1 참조  

본 방법론은 기존 방법론과 비교하여 도메인별 이미지 특징 및 geometry 변형과 관련된 이미지 번역에서 탁월함과 동시에 털 색상과 같은 같은 도메인에 무관한 이미지 특징을 보존할 수 있다 (그림 1).
감독되지 않은 도메인 변환을 통해 기하 변형과 같은 큰 변화를 생성하는 능력은 가장 많이 추구되는 문제였다. 원래 CycleGAN/DualGAN 구조는 geometry 변형을 학습할 수 없다는 일반적인 생각이 있어왔으며 변환을 위해서는 feature 표현, 학습 방법을 바꿔야만 했다. 그 결과 많은 접근방법이 latent 공간변환(style-content, scale 분리, feature disenglement)에 집중해왔다.  
GANHopper는 이미지 공간에서 직접 작동하는 CycleGAN과 근본적으로 동일한 아키텍처를 따르기 때문에 앞선 믿음에 도전하는 연구이다. 
이미지 변환을 조정하고 조절하기 위해 점진적인 다중 hop 변환을 시행할 뿐이다.  
또한, 다중 hop GAN은 예를 들어 중간 번역기의 수와 아키텍처를 변경하는 측면에서 상당히 확장 가능한 일반적인 "meta idea"를 나타낸다. 
그림 1 및 이후의 더 많은 결과에서 알 수 있듯이 하나의 네트워크를 사용하는 가장 간단한 옵션이라도 이미 다양한 도메인 번역 작업에서 상당한 차이를 만들 수 있다.

## 2 Related Work
**2.1 Supervised Setting**
현대 이미지-이미지 변환의 기초는 semantic image segmentation을 위해 처음 개발된 UNet 이다.  
이는 나중에 조건부 적대 훈련을 통해 다양한 I2I 변환 작업으로 확장되었다.  
추가 연구로 "one to many" 변환 작업에서 동일이미지에 대해 고해상도 출력과 여러 가능한 출력(e.x., grey-scale image colorization)이 생성되었다.  
이 방법은 학습 데이터로 쌍을 이루는 입력 및 출력 이미지 {(xi , yi)}가 필요하다.  
**2.2 Unsupervised Setting**
최근의 I2I 변환 네트워크는 입력/출력 이미지의 두 세트 {xi} 및 {yi}의 형태로 쌍을 이루지 않은 데이터에서 학습할 수 있다.  
이러한 방법은 G를 함께 학습하여 x에서 y로 매핑하고 네트워크 F를 y에서 x로 매핑하여 훈련 시간에 F(G(x)) = x 및 G(F(y)) = y를 적용한다.  
이러한 cycle-consistency는 학습된 매핑을 임의의 번역이 아니라 semantically meaning한 것으로 정규화하는 것으로 생각된다.  
이 방식은 약한 변환(예: 여름에서 겨울, 낮에서 밤)과 관련된 도메인 번역에서는 성공하지만 큰 형태 변형이 필요한 경우(예: 고양이에서 강아지로) 종종 실패한다.  
cycle-consistency를 포함하는 네트워크는 더 전역적인 이미지 컨텍스트를 고려하는 D와 perceptual loss로 학습될 때 더 큰 모양 변화를 수행하는 것으로 나타났다[5].  
다른 접근 방식은 두 도메인의 이미지가 생성되는 공유 잠재 코드 z를 삽입하는 것이다(x = F(z) & y = G(z)) [15].  
이 방법을 확장하여 여러 출력 이미지로 번역할 수도 있다[7].  
또 다른 방법은 geometry와 appearance를 명시적으로 별도로 모델링하는 것이다.  
사람의 얼굴을 캐리커처 스케치로 변환하는 영역별 방법은 얼굴 랜드마크를 감지하고 변형한 다음 입력 얼굴을 왜곡하는 데 사용하여 이를 수행한다[2].  
보다 최근의 연구는 얼굴에만 국한되지 않는 관련 기술을 제안했다[24].  
마지막으로 사전 훈련된 이미지 분류 네트워크의 특징 계층을 통해 도메인 변환을 수행하는 것도 가능하다[10].  
이 방법은 또한 큰 모양 변화를 일으킬 수 있다.  

**2.3 본 방법론**
위와 대조적으로, 우리는 변환이 일련의 smooth hopping으로 수행되는 경우 직접 I2I 변환이 모양 세부 사항을 보존하면서 큰 모양 변경을 생성할 수 있음을 보인다.  
이 과정은 도메인간 interp. 시퀀스를 생성하는 것으로 볼 수 있다. 많은 GAN은 latent space에서 linear interp.를 통해 이미지 사이에 보간을 생성할 수 있다.  
이러한 보간은 데이터 세트[12]에 지정되거나 자동으로 추론되는[4] 해석 가능한 방향을 따를 수도 있다. 그러나 GAN latent interp.는 교차 영역 보간을 수행하지 않는다.  
Aberman et al. [1] 두 도메인의 이미지에서 해당 지점을 식별하고 이 지점을 입력으로 사용하여 이미지 모핑을 구동하여 교차 도메인 interp.을 수행한다[13].  
그러나 이 접근 방식은 source 및 target 도메인 모두에 있는 이미지를 interp.해야 하는 반면, 우리의 방법은 소스 이미지만 가져와 가장 일치하는 대상 이미지에 대한 interp.을 생성한다.  
마지막으로 InstaGAN[18]은 한 번에 하나의 인스턴스를 변환하기 위해 분할 마스크에 의존하는 다중 인스턴스 변형 네트워크를 사용하여 바지에서 치마로의 큰 모양 변경을 처리한다.  
구현에는 다른 목적을 위한 순차적 미니배치 추론/훈련 옵션이 포함된다.  

## 3. Method
Source 도메인 X, target 도메인 : Y  
목표는 x ∈ X가 주어졌을 때 y0이 데이터 세트 Y의 이미지 x에 대응하는 것으로 인식되도록 다른 이미지 y0 ∈ Y를 출력하는 변환을 배우는 것이다.  
y ∈ Y에서 x0 ∈ X로의 아날로그 변환에서도 동일한 작업이 수행되어야 한다. 이 작업은 CycleGAN에서 수행한 작업과 동일하다[28].  
그러나 우리는 네트워크를 통해 한 번에 입력 이미지를 번역하지 않는다. 오히려 일련의 중간 이미지를 통해 번역 프로세스를 용이하게 한다.  
우리는 G를 사용하여 제한된 양만큼 하나의 이미지를 대상 도메인으로 warping하는 프로세스로 정의하는 hop의 개념을 소개한다.  
반복된 hop은 번역 프로세스의 부산물로 하이브리드 이미지를 생성한다. 네트워크를 통한 단일 패스로 이미지를 번역하지 않기 때문에  
학습 프로세스는 전통적인 cycle-consistency 학습 프레임워크에서 수정되어야 한다. 특히, 학습 데이터에 이러한 이미지가 포함되어 있지 않기 때문에  
번역 중에 하이브리드 이미지를 생성하는 것은 어려운 일이다. 따라서 이러한 생성된 이미지의 혼성성은 훈련 중에 즉석에서 추정되어야 한다.  
이를 위해 우리는 hybrid-D라고 하는 새로운 D를 도입한다. 이 D는 이미지가 두 입력 도메인과 얼마나 유사한지를 평가하여 멤버십 점수를 생성하는 것이다.  
또한 smoothness loss를 추가한다. 목적은 G가 번역을 초과하지 않도록 hop을 통해 이미지의 점진적인 warping을 장려하는 것이다.  
다음 하위 섹션에서는 다중 hop 프레임워크를 제공한다.

### 3.1 Multi-hop framework
G와 F로 표시되는 CycleGAN의 원래 두 개의 G와 세 개의 D로 구성되며, 그 중 두 개는 CycleGAN의 원래 DY, DX이다.  
세번째는 새로운 하이브리드 판별자 DH이다. 그림 2는 학습 시간 동안 이러한 다양한 G/D가 함께 작동하여 다중 hop을 통해 이미지를 번역하는 방법을 보여준다.  
**Hop nomenclature.**  
G 혹은 F를 사용하여 이미지를 각각 Y 또는 X 영역으로 warping하는 것으로 정의된다.  
완전한 변환은 동일한 G를 사용하여 사전 정의한 h회의 hop을 수행함으로써 달성된다.  
E.x.,  
h = 3, G(G(G(x))) = y' , 여기서 x ∈ X 및 y' ∈ Y 입니다. 유사하게, F(F(F(y))) = x' , 여기서 y ∈ Y 및 x' ∈ X.  
이미지 i가 주어지면 변환 hop은 다음 반복 관계를 통해 정의된다.
![image](https://user-images.githubusercontent.com/40943064/135717509-3f1c5c4d-4f65-4b15-b038-07e72122f3f0.png)

**Generator Architecture.**  
CycleGAN에서 사용되며 Johnson[9]이 제안한 architecture 및 layer nomenclature를 채택한다.  
**c7s1-k** : [conv : 3x3 Conv-IN-ReLU / filter:k / stride:1],   
**dk** : [conv. : 3x3 Conv-IN-ReLU / filter : k / stride : 2] (Reflection padding을 통해 artifact를 줄임)  
**Rk** : [conv. : ResidualBlock with 3x3 Conv / filter : k] 
**uk** : [conv. : 3x3 TransposeConv.-IN-ReLU / filter : k / stride : 1/2]  
128×128 이미지를 입력으로 받고 c7s1-64, d128, d256, R256(×12), u128, u64, c7s1-3 레이어로 구성된다.  

**Discriminator architecture.**   
DY, DX, DH에 대해 CycleGAN에서 사용된 것과 동일한 70x70 PatchGAN[8]을 사용한다.  
**Ck** : [conv. : 4x4 Conv-IN-LearkyReLU / filter : k / stride : 2]  
128×128 입력 이미지가 주어지면 16×16 특징 행렬을 생성한다.  
각 요소는 입력 이미지의 70x70 패치 중 하나와 연결된다.  
D는 C64, C128, C256, C512 레이어로 구성된다.  
