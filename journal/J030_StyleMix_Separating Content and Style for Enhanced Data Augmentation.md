## Abstract
심층신경망의 성능은 성공적으로 향상되어 왔으나 학습된 네트워크는 과적합이나 adversarial task에 취약하다.  
최근 이러한 문제 대비하여 mixup 방법이 활발히 연구되고 있다.  
이러한 방식은 이미지의 content/style을 구분하지 않고 이미지를 혼합하거나 잘라내어 붙여넣는다.  
이미지 쌍의 content/style을 별도로 조작하는 첫 번째 혼합 방법으로 **StyleMix**와 **StyleCutMix**를 제안한다.  
이로써 더 풍부하고 강력한 샘플을 만들어 모델 훈련의 일반화를 개선할 수 있다.  
이미지 쌍의 클래스 거리에 따라 스타일 혼합 정도를 결정하는 자동 체계를 개발하여  
너무 다른 스타일의 쌍으로부터 지저분한 혼합 이미지를 방지한다.  
CIFAR-10, CIFAR-100 및 ImageNet 데이터 세트에 대한 실험은 StyleMix가 SOTA보다 낫거나 유사한 성능을 달성하고  
adversarial task에 대한 보다 강력한 D를 학습한다는 것을 보여준다.

## 1. Introduction
심층 신경망은 많은 분류 작업에서 성과를 달성했지만, 학습된 네트워크는 과적합 및 적대적 공격에 취약하다.  
이러한 문제를 완화하기 위한 가장 유망한 접근법 중 하나로, data augmentation 및 normalization 방법이 활발하게 연구되었다.  
Mixup은 이미지와 라벨의 보간을 통해 두 샘플을 혼합하여 증강한다.  
CutMix는 패치 영역에 비례하여 실측 레이블과 함께 한 이미지에서 다른 이미지로 패치를 잘라내어 붙여 넣는다.  
Manifold Mixup은 네트워크가 은닉 상태를 보간하는 중간 표현에 과적합되는 것을 방지하기 위해 정규화한다.  
이러한 접근 방식은 입력 노이즈 및 공격에 대한 분류 성능과 견고성을 확실히 개선하지만 새로운 혼합 샘플을 생성하기 위한  
이미지의 content/style을 구분하지 않는다. 내용은 일반적으로 이미지의 모양과 형태를 말하며 스타일은 주로 질감과 색상을 포함한다.  
CNN이 content/style의 구분 없이 학습되면 label를 학습하기 위해 정보의 일부분에 초점을 맞추는 경향이 있다.  
(예: 코끼리는 전경이 어떻게 보이든 회색 피부 질감만 사용하여 식별된다.)  

이 작업에서 우리는 두 입력 이미지의 내용과 스타일을 신중하게 혼합하면 더 풍부하고 강력한 샘플을 생성하는 데 도움이 될 수 있으며  
결국 모델 훈련의 일반화를 향상시킬 수 있다고 주장한다.  
스타일 전달 방법은 최근 큰 발전을 보여 딥 네트워크가 content뿐만 아니라 이미지의 style 정보도 인코딩하고  
semantic contents를 보존하면서 이미지의 low-level visual feature 분포를 변경할 수 있는 가능성을 제공함을 입증했다.  

따라서 각 입력 이미지의 content/style을 별도로 고려하여 다양한 style의 contents 보존 이미지 생성,  
동일한 스타일 이미지의 다양한 foreground object 생성 또는 둘 다와 같은 훈련 데이터를 보강하기 위한 두 가지 수준의 옵션이 있다.  
따라서 분류기는 content/style 기능을 모두 인식하여 분류 성능과 노이즈 및 공격에 대한 견고성을 향상시키는 방법을 완전히 학습한다.  
content/style 특성의 convex 조합을 통해 다양한 학습 샘플을 생성할 수 있는 데이터 증대를 위한 새로운 혼합 방법으로 StyleMix를 제안한다.  
![image](https://user-images.githubusercontent.com/40943064/136573448-cca61a0f-81ad-4b31-bd99-12a836077fa4.png)

그런 다음 CutMix의 아이디어를 기반으로 하위 이미지 수준 조작을 허용하는 StyleCutMix로 확장한다.  
마지막으로 주어진 한 쌍의 이미지 사이의 클래스 거리에 따라 스타일 믹싱 정도를 자동으로 결정하는 기법을 개발한다.  
CIFAR-10], CIFAR100 및 ImageNet 데이터 세트에 대한 분류 실험을 통해 우리의 각 제안은 분류 성능을 크게 향상시키고  
궁극적으로 SOTA 혼합 방법과 비교 성능을 달성한다.  
이 작업의 기여는 다음과 같이 요약할 수 있다.  
1. StyleMix를 이미지 pair의 content/style을 별도로 조작하는 mixing 방법을 도입한다.   
2. content/style 입력이 명확하게 식별되는 style transfer 작업과 달리 mixup context에서  
두 개의 입력 이미지는 훈련 세트의 임의의 쌍이다.  
너무 다른 스타일의 쌍으로 인해 지저분한 혼합 이미지가 성능을 크게 손상시키는 것을 방지하기 위해 
쌍의 클래스 거리에 따라 style 혼합 정도를 결정하는 자동 방식을 제안한다.  
3. CIFAR-10/100에서 SOTA 혼합 방법을 능가하며 Cutout, GridMix, Manifold Mixup], CutMix], AugMix 및 ImageNet에서 비교한다. 
4. 퍼즐 믹스 또한 우리의 방법이 최근의 다른 혼합 방법보다 adversarial 공격에 대한 분류기의 견고성을 향상시킨다는 것을 보여준다.  
