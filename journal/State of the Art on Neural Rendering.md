# State of the Art on Neural Rendering

### Nomenclature
1. NR : Neural Rendring
2. CG : Cumputer Graphics
3. CV : Computer Visions
4. ML : Machine Learning
5. DL : Deep Learning

## Abstract 
현실같은 가상 세계를 구현하기 위한 **효율적인 rendering**은 컴퓨터 그래픽에서 오랜 노력의 대상이었다.  
현대 그래픽 기술은 수작업 장면 표현을 통한 사실적 이미지 합성에 성공적인 결과들을 얻어왔다.  
그러나 모양/재질/빛 그리고 장면에서의 다른 면들에 대한 자동 생성은 여전히 도전적인 문제로 남겨져 있으며  
해결된다면 사실적인 컴퓨터 그래픽을 통해 더 광범위하게 사용될 것이다.  

한편, CV와 ML의 발전은 Deep generative model이라 하는 새로운 이미지 합성 및 편집 방식을 낳았다.  
NR은 **Generative ML**과 **CG의 물리적 지식**을 결합한 급격하게 발전되는 분야이다.  
(미분가능 rendering을 NN 학습에 통합해서)  
CG와 CV에서 수많은 application을 가지면서 NR은 그래픽 커뮤니티에서 새로운 분야가 되고있다.  
본 SOTA 리포트는 NR의 최신 트렌드와 사례를 요약한다.  
  
제어 가능하고 사실적인 출력을 얻기 위해 **전통 CG**와 **Deep generative model**을 결합하는 접근법에 집중한다.  
CG와 ML 컨셉에 대한 개요로 시작해서 NR 접근법의 중요한 점을 논의한다.  
  
특히, 아래 접근방식에 중점을 둔다.   
1) 제어방식    
2) 파이프라인의 어떤 부분을 학습하는지  
3) explicit control vs implicit control  
4) generalization  
5) stochastic vs deterministic 합성  
  
이 SOTA 리포트의 뒤에는 다음의 분야에 집중한다.  
1. Novel view synthesis  
2. Semantic photo manipulation  
3. Facial and body reenactment  
4. Relighting  
5. Free-viewpoint video  
6. The creation of photo-realistic avatars for virtual and augmented reality telepresence  
  
마지막에서 기술의 사회적 의미에 대한 토론으로 결론을 내리고 공개 연구 문제를 조사한다.  

## 1. Introduction
사실적 이미지 생성은 정교한 CG 개발의 최우선 추진 과제 중 하나다.  
CG 방식은  
1) (최신 컴퓨터 게임 생성을 가능하게 하는) real-time rendering  
2) (장편 영화에서의 사진 촬영 디지털 인간 생성을 위한) 정교한 글로벌 lighting 시뮬레이션이 있다.  
여기서 주요 병목은 content 생성이다.  

Surface geometry, appearance/material, illumination 및 animation 측면에서  
기본 장면 표현 생성을 위해 숙련된 예술가의 방대한 양의 지루하고 값 비싼 수작업이 필요하다.  
동시에, 강력한 생성모델이 CV와 ML에 나타나기 시작했다.  
GAN의 중요한 연구는 최근 몇년간 고해상도 이미지/비디오 생성을 위한 deep generative 모델로 발전했다.  
다른 도메인의 제어 매개 변수 또는 이미지에서 NN를 조절하여 합성 콘텐츠에 대한 control를 달성할 수 있다.  
최근에는 두 영역이 합쳐져 NR으로 연구되고 있다. 이 용어를 사용한 최초의 출판물 중 하나는 GQN이다.  
이는 기계가 표현/생성 NN를 기반으로 주변 환경을 인식하는 것을 배울 수 있게 한다.  
NN이 장면의 여러 이미지를 입력으로 받아들이고 정확한 occlusion으로 임의 뷰를 출력할 수 있기 때문에  
3D에 대한 implicit 개념을 가지고 있다고 주장한다.  
Implicit 3D 개념 대신 그래픽 파이프라인의 구성 요소를 이용하여 3D의 개념을 더 명확하게 포함하는 다양한 다른 방법들이 뒤따랐다.  
고전 CG는 geometry, surface properties 및 카메라와 같은 물리학의 관점에서 출발하지만, 기계 학습은 통계적 관점에서 이루어진다.  
  
이를 위해 CG 생성 이미지의 품질은 **모델의 물리적 정확성에 의존**하지만  
ML 방식의 품질은 대부분 신중하게 **ML 모델과 사용된 학습 데이터의 품질**에 의존한다.  
장면 속성의 명시적 재구성은 어렵고 오류가 발생하기 쉬우며 rendering된 콘텐츠에 왜곡이 생긴다.  
이미지 기반 rendering 방법은 간단한 경험론을 사용하여 캡처된 이미지를 결합하여 이러한 문제를 해결한다.  
그러나 복잡한 풍경에서, 이 방법은 seams나 ghost과 같은 artifact를 보여준다.  
NR은 deep NN를 사용하여 캡처된 이미지에서 새로운 이미지로의 복잡한 매핑을 학습함으로써  
reconstruction 및 rendering을 모두 처리할 수 있는 가능성을 제공한다.  
NR은 수학적 투영 모델과 같은 물리적 지식을 학습된 구성 요소와 결합하여  
제어 가능한 이미지 생성을 위한 새롭고 강력한 알고리즘을 산출한다.  
NR은 문헌에서 아직 명확한 정의가 없다. 여기서는 NR을 다음과 같이 정의한다.  
  
_(illumination/camera parameters/pose/geometry/appearance/semantic structure)등의 장면특성 제어를  
(implicit or explicit)한 방식으로 가능하게 하는 (Deep 이미지/영상 생성 방식)_  

이 SOTA 보고서는 여러 NR 방법을 정의하고 분류한다.  
이미지 생성 프로세스의 제어 가능성이 많은 CG 애플리케이션에 필수적이기 때문에 CG와 학습 기반 방식을 결합하여  
제어 가능한 이미지 생성을 위한 새롭고 강력한 알고리즘을 생성하는 방법에 초점을 맞춘다.  
이 보고서를 구성하는 한 가지 중심 계획은 각 접근법에 의해 제공되는 제어 가능성이다.  
우리는 NR의 전제 조건인 CG, CV 및 ML의 기본 개념에 대해 논의하는 것으로 시작한다.  

이후, 제어유형, 제어 제공 방법, 파이프라인의 학습 부분, explicit control vs implicit control,  
generalization, stochastic vs deterministic 합성과 같은  
NR 접근법의 중요한 측면에 대해 논의한다.  
다음으로, NR에 의해 활성화되는 애플리케이션의 환경에 대해 논의한다.  
NR의 응용은 아래 목록과 같이 다양하다.  
  
1) novel view synthesis  
2) semantic photo manipulation  
3) facial/body reenactment  
4) relighting  
5) free-viewpoint video  
6) creation of photo-realistic avatars for virtual and augmented reality telepresence  
  
실제 구별할 수 없는 이미지의 생성/조작은 특히 인간이 사진에 찍힐 때 많은 사회적 의미를 갖기 때문에  
이러한 의미와 합성 콘텐츠의 감지 가능성에 대해서도 논의한다.  
NR 분야는 여전히 빠르게 발전하고 있으므로 현재 열린 연구 문제로 결론을 내린다.  

## 2. Related Surveys and Course Notes 

심층 생성 모델은 문헌에서 널리 연구되어 왔으며, 이를 설명하는 여러 survey와 강의 노트가 있다.  
여러 보고서는 GAN 및 VAE와 같은 특정 생성 모델에 초점을 맞추고 있다.  
고전 CG와 CV 기술을 사용한 control 가능한 이미지 합성도 광범위하게 연구되었다.  
이미지 기반 rendering은 여러 조사 보고서에서 논의되었다.  
Szeliski의 책은 3D 재구성 및 이미지 기반 rendering 기술에 대한 훌륭한 소개를 제공한다.  
최근 조사 보고서는 3D 재구성에 대한 접근법과 다양한 애플리케이션의 control 가능한 얼굴 rendering을 논의한다.  
NR의 일부 측면은 최근 CV 컨퍼런스의 튜토리얼과 워크샵에서 다루어졌다.  
여기에는 자유 시점 rendering 및 전신 성능의 reconstruction을 위한 접근법, 얼굴 합성을 위한 NR 튜토리얼  
및 신경 NN를 이용한 3D 장면 생성 등이 포함된다.  
그러나 위의 조사와 과정 중 NR과 NR의 모든 다양한 응용을 체계적이고 포괄적으로 살펴볼 수 있는 것은 없다.  

## 3. Scope of this STAR
이 최신 보고서에서는 (**CG 파이프라인**)과 (**학습 가능한 요소**)를 결합한 새로운 접근 방식에 중점을 둔다.  
ML을 통해 고전적인 rendering 파이프라인을 개선할 수 있는 위치와 방법과 학습에 필요한 데이터에 대해 논의하고 있다.  
포괄적인 개요를 제공하기 위해 CG와 ML의 관련 기초에 대해서도 간략하게 소개한다.  
하이브리드 방법의 한계도 있다. 이 보고서는 또한 기법에 의해 강화되는 새로운 응용 프로그램에 대해서도 논의한다.  
우리는 ML을 통해 제어 가능한 사실적 사진 이미지를 생성하는 것을 주요 목표로 하는 기술에 초점을 맞춘다.  
우리는 3D 재구성 및 장면 이해에 더 초점을 맞춘 기하학적 및 3D DL에 대한 작업은 다루지 않는다.  
이 작업은 많은 NR 접근 방식, 특히 3D 구조 장면 표현을 기반으로 하지만 이 조사의 범위를 벗어난다.  
우리는 또한 ray 추적 이미지를 제거하기 위해 ML을 사용하는 기술에 초점을 맞추지 않는다.  

## 4. Theoretical Fundamentals
NR 공간 작업의 이론적 기초에 대해 논의한다.  
CGI 형성 모델에 대해 논의한 다음 고전 이미지 합성 방법에 대해 논의한다.  
다음으로, 우리는 DL에서 생성 모델에 대한 접근법에 대해 논의한다.

### 4.1. Physical Image Formation
전통적인 CG 방법은 실제 세계에서 이미지 형성의 물리적 과정에 가깝다.  
광원은 카메라에 기록되기 전에 형상과 재료 특성의 함수로 장면의 물체와 상호 작용하는 광자를 방출한다.  
이 과정은 light transport로 알려져 있다.  
Camera optics는 조리개로부터 들어오는 빛을 획득하여 카메라 본체 내부의 센서나 필름 평면에 초점을 맞춘다.  
센서 또는 필름은 해당 평면의 입사광의 양을 비선형 방식으로 기록한다.  
광원, 재료 특성 및 카메라 센서 등 이미지 형성의 모든 구성 요소는 파장에 따라 달라진다. 
실제 필름과 센서는 종종 인간 시각 시스템의 민감도에 맞춰 1~3개의 다른 파장 분포만을 기록한다.  
이 물리적 이미지 형성의 모든 단계(광원, 장면 형상, 재료 특성, 광전송, 광학 및 센서 동작)는 CG로 모델링된다.

### 4.1.1. Scene Representations
Scene object를 모델링하기 위해 scene geometry에 대한 다양한 표현이 제안되었다.  
그것들은 explicit/implicit 표현으로 분류될 수 있다.  
Explicit : 장면을 삼각형, 점 같은 원시 요소 또는 고차 파라메트릭 표면과 같은 기하학적 원시 요소의 모음으로 설명한다.  
Implicit : 표면이 함수(또는 다른 수준 집합)의 영점수로 정의되는 R3 → R로부터의 부호 거리 함수 매핑을 포함한다.  
실제로 대부분의 하드웨어와 소프트웨어 renderer는 삼각형 메시에서 가장 잘 작동하도록 조정되어 있으며  
rendering을 위해 다른 표현을 삼각형으로 변환한다.
  
빛과 장면 표면과의 상호 작용은 표면의 재료 특성에 따라 다르다.  

재료는 아래 두가지로 표현될 수 있다.  
1) BRDF(Bidirectional Reflectance Distribution Functions;양방향 반사율 분포 함수)  
- 들어오는 각 광선 방향의 표면 지점에서 발생하는 주어진 파장의 빛이 나가는 각 광선 방향을 향해 반사되는 정도를 설명하는 5차원 함수  
- 분석 모델 또는 측정된 데이터를 사용하여 나타낼 수 있다.  
- BRDF가 표면에 걸쳐 변화할 때 이를 spatially varing BRDF(svBRDF)라고 한다.  
2) BSSRDF(Bidirectional Subsurface Scattering Reflectance Distribution Functions;양방향 지하 산란 반사율 분포 함수)  
- BRDF는 단일 표면 지점에서 발생하는 빛 상호 작용만 모델링하는 반면,  
- BSSDRF는 한 표면 지점에서 발생하는 빛이 다른 표면 지점에서 반사되는 방법을 모델링하여 7-D 함수를 만든다.  

![image](https://user-images.githubusercontent.com/40943064/144844016-498e9cbe-4d52-438d-8025-3ca1234b6511.png)  

기하학 전반에 걸쳐 공간적으로 변화하는 동작은 이산 재료를 다른 기하학적 원시 요소에 결합하거나 텍스처 매핑을 사용하여 나타낼 수 있다.  
텍스처 맵은 2D 또는 3D 도메인에서 표면으로 확산 알베도와 같은 재료 매개 변수의 연속적 값 세트를 정의한다.  
3D 텍스처는 공간의 경계 영역을 통해 값을 나타내며 명시적이거나 암시적인 geomtry에 적용될 수 있다.  
2D 도메인에서 2차원 텍스처 맵 파라메트릭 지표면. 따라서 일반적으로 명시적 geometry에만 적용된다.  

장면의 광원은 파라미터 모델로 나타낼 수 있으며, 빛을 방출하는 장면 표면으로 표현되는 점, 방향 조명, 영역 광원이 포함된다.  
어떤 방법은 텍스처 맵이나 함수에 의해 정의된 표면에 걸쳐 지속적으로 변화하는 방출을 설명한다.  
종종 환경 지도는 조밀하고 먼 장면 조명을 나타내기 위해 사용된다.  
이러한 환경 맵은 구나 큐브에 비모수 텍스처로 저장하거나 구면 조화 기반 계수로 근사할 수 있다.  
장면의 매개 변수는 시간이 지남에 따라 변화하는 것으로 모델링될 수 있으며,  
이를 통해 연속 프레임에 걸친 애니메이션과 단일 프레임 내에서 모션 블러 시뮬레이션을 모두 수행할 수 있다.  
  
### 4.1.2. Camera Models
CG에서 가장 흔한 카메라 모델은 핀홀 카메라 모델로, 광선이 핀홀을 통과해 필름 평면(영상 평면)에 부딪힌다.  
핀홀의 3D 위치, 이미지 평면 및 센서나 필름의 공간 범위를 나타내는 평면 내의 직사각형 영역에 의해 파라미터화될 수 있다.  
작동은 균일한 좌표를 사용하여 3D 기하학적 표현을 이미지 평면의 2차원 영역으로 변환하는 투영 기하학을 사용하여 압축적으로 표현될 수 있다.  
이것은 완전 투시 투영 모델이라고도 알려져 있다.  
약한 투시 투영과 같은 이 모델의 근사치는 전체 투시 투영법의 비선형성으로 인해 복잡성을 줄이기 위해 컴퓨터 비전에 종종 사용된다.  
CG의 보다 정확한 투영 모델은 왜곡, 이상, 비그네팅, 디포커스 블러, 심지어 렌즈 요소들 사이의 상호반사를 포함한 비이상적 렌즈의 효과를 고려한다.  
  
### 4.1.3. Classical Rendering
Camera, light, surface geometry, material을 포함한 장면 정의를 시뮬레이션된 카메라 이미지로 변환하는 과정을 rendering이라고 한다.  
rendering에 대한 가장 일반적인 두 가지 접근법은 **rasterization**과 **raytracing**이다.  

![image](https://user-images.githubusercontent.com/40943064/145209104-d37cef08-d5f7-4d2b-9f53-1e746f6d6d2d.png)  

Rasterization은 geomtry가 이미지 영역으로 변환되는 피드포워드 과정으로, 때로는 페인터의 알고리즘으로 알려진 앞뒤 순서로 변환된다.  
Raytracing은 이미지 픽셀에서 가상 장면으로 광선을 거꾸로 투사하고 geometry의 교차점에서 새로운 광선을 재귀적으로 투사해 반사 및 굴절을 시뮬레이션하는 공정이다.  
하드웨어 가속 rendering은 메모리 일관성이 좋기 때문에 일반적으로 rasterization에 의존한다.  
그러나 global illumination 및 기타 형태의 복잡한 light transport, depth of field, motion blur 등과 같은 많은 실제 이미지 효과는  
raytracing을 사용하여 더 쉽게 시뮬레이션되며, 최근 GPU는 이제 실시간 그래픽 파이프라인(예: NVIDIA RTX 또는 DirectX Raytracking)에서  
raytracing을 사용할 수 있도록 가속 구조를 갖추고 있다.  
Rasterization은 explicit geometric representation을 필요로 하지만, raytracing/raycasting은 implicit representation에도 적용될 수 있다.  
실제로 implicit representation은 marching cubes algorithm 및 기타 유사한 방법을 사용하여 rasterization를 위해 명시적 형태로 변환할 수도 있다.  
렌더러는 rasterization과 raycasting의 조합을 사용하여 고효율과 물리적 사실감을 동시에 얻을 수 있다(예: 스크린 공간 raytracing).  
주어진 rendering 파이프라인에서 생성되는 이미지의 품질은 파이프라인에 있는 다양한 모델의 정확도에 크게 좌우된다.  
구성요소는 샘플링 및 신호 재구성 이론의 신중한 적용을 사용하여 픽셀 중심 사이의 간격과 같은 컴퓨터 시뮬레이션의 이산적 특성을 설명해야 한다.  
새로운 뷰를 생성하거나 재료나 조명을 편집하거나 새로운 애니메이션을 만들기 위해 실제 데이터로부터  
다른 모델 매개변수(카메라, 기하학, 재료, 광 파라미터)를 추정하는 과정을 _inverse rendering_ 이라고 한다.  
컴퓨터 비전과 컴퓨터 그래픽의 맥락에서 탐구된 inverse rendering은 NR과 밀접한 관련이 있다.  
Inverse rendering의 단점은 수학적 복잡성과 계산 비용 때문에 고전적인 rendering에 사용되는 미리 정의된 물리적 모델이나  
데이터 구조가 실제 물리적 프로세스의 모든 특징을 정확하게 재현하지 못한다는 것이다.  
대조적으로, NR은 그러한 모델 대신 학습된 구성 요소를 rendering 파이프라인에 도입한다.  
심층 신경망은 통계적으로 그러한 물리적 프로세스를 근사화하여, inverse rendering보다 실제 효과를 더 정확하게 재생산하여  
학습 데이터와 더 밀접하게 일치하는 출력을 생성할 수 있다.  

역 rendering과 신경 rendering이 교차하는 지점에는 접근법이 있다. Li는 글로벌 조명 효과를 근사화한 NR을 사용하여  
깊이, 정상, 알베도, 거칠기 맵을 예측하는 역 rendering 방법을 효율적으로 학습한다.  
또한 신경망을 사용하여 셰이더와 같은 고전적 rendering 파이프라인의 특정 구성 요소를 향상시키는 접근 방식도 있다.  
레이너는 Bidirectional Texture Function을, 맥시모프는 Appearance Maps을 학습한다.  

### 4.1.4. Light Transport
Light transport는 빛을 방출하는 광원으로부터, scene을 통해, 그리고 camera로 보내는 가능한 모든 경로를 고려한다.  
이 문제의 잘 알려진 공식은 고전적 rendering 방정식이다.  
![image](https://user-images.githubusercontent.com/40943064/144739258-d3599714-c0b6-44af-bca6-7c19d9c09ea9.png)  
Lo : 위치, 광선 방향, 파장, 시간의 함수로써 표면으로부터의 나가는 광도  
Le : 용어는 직접 표면 방출  
Lr : 입사광과 표면 반사율의 상호작용  
![image](https://user-images.githubusercontent.com/40943064/144739268-07f6206a-7a7a-4e76-bacf-6e198ae48603.png)  

이 공식은 투명한 물체와 지표면 아래 또는 부피 산란의 영향을 고려하지 않는다. rendering 방정식은 적분 방정식이며,  
오른쪽에 나타나는 입사 광도 Li가 같은 광선의 다른 표면에서 나오는 송신 광도 Lo와 같기 때문에 비사소적인 장면에서는 닫힌 형태로 풀 수 없다.  
따라서, 방대한 수의 근사치가 개발되었다. 가장 정확한 근사치는 몬테카를로 시뮬레이션으로 장면을 통과하는 광선 경로를 샘플링한다.  
더 빠른 근사는 우측을 한두 번 확장한 다음 재발을 차단하여 소수의 "발광"만 시뮬레이션할 수 있다.  
컴퓨터 그래픽 아티스트들은 또한 비물리적 기반 광원을 장면에 추가함으로써 추가적인 바운스를 시뮬레이션 할 수 있다.

### 4.1.5. Image-based Rendering
3D 컨텐츠를 2D 평면에 투영하는 기존의 rendering과 달리, 이미지 기반 rendering 기법은 이미지 세트를 일반적으로 뒤틀고 함께 합성하여  
새로운 이미지를 생성한다. 이미지 기반 rendering은 Thies에서 볼 수 있듯이 애니메이션을 처리할 수 있지만,  
가장 일반적인 사용 사례는 캡처된 뷰의 이미지 콘텐츠를 프록시 지오메트리와 추정된 카메라 포즈를 기반으로 하여  
새로운 뷰로 뒤틀리는 정적 객체의 새로운 뷰 합성이다.  
완전한 새 이미지를 생성하려면 여러 캡처된 뷰를 대상 뷰로 뒤틀어야 하므로 혼합 단계가 필요하다.  
일부 재료는 뷰 포인트에서 모양을 크게 바꾸기 때문에 결과 이미지 품질은 지오메트리의 품질, 입력 뷰의 수 및 배열,  
장면의 재료 특성에 따라 달라진다.  
혼합 및 뷰 의존적 효과의 보정을 위한 휴리스틱 방법이 좋은 결과를 보여주지만, 최근 연구는 이러한 이미지 기반 rendering 파이프라인의  
일부를 학습된 구성 요소로 대체했다.  
심층 신경망은 혼합 아티팩트와 뷰 의존적 효과에서 비롯된 아티팩트를 모두 줄이기 위해 성공적으로 사용되었다.

### 4.2. Deep Generative Models
기존의 CG는 이미지를 생성하기 위해 장면을 물리적으로 모델링하고 light transport를 시뮬레이션하는 데 중점을 두는 반면,  
ML을 사용하여 실제 이미지의 분포를 학습하여 통계적 관점에서 이 문제를 해결할 수 있다.  
역사적으로 작은 이미지 세트(예: 수백 개)를 사용한 기존의 이미지 기반 rendering과 비교하여  
deep generative model은 대규모 이미지 컬렉션에서 이미지 prior를 학습할 수 있다.  
  
초기 심층 생성 모델 연구는 숫자나 얼굴 정면의 무작위 샘플을 생성하는 것을 학습 할 수 있었으나  
품질과 해상도는 물리적 기반 rendering 기술에 못미쳤다.  
최근에는 GAN과 그 확장을 사용하여 사실적 이미지 합성 성능이 입증되었다.  
최근에는 실제와 구별할 수 없는 무작위 고해상도 초상화를 합성할 수 있다.  
  
심층 생성 모델은 학습 세트와 유사한 통계로 랜덤 사실적 이미지를 생성하는 데 탁월하다.  
그러나 **사용자 제어**와 **상호 작용**은 이미지 합성 및 조작에서 중요하다.  
예를 들어, 콘셉트 아티스트는 무작위 장면보다는 자신의 디자인 아이디어를 반영한 장면을 만들고 싶어한다.  
따라서 영상 합성 프로세스를 명시적으로 제어할 수 있도록 생성 모델을 조건부 설정으로 확장해야 한다.  
초기 작업은 픽셀별 'lp 거리'로 FNN을 학습시켜 주어진 조건부 입력을 생성했다.  
그러나 픽셀별 lp 거리는 각 픽셀을 독립적으로 고려하고 시각적 구조의 복잡성을 무시하기 때문에  
결과가 흐릿한 경우가 많다. 게다가, 가능한 여러 결과물의 평균을 내는 경향이 있다.  
이를 해결하기 위해, 최근 연구는 사전 학습된 NN에 의해 구성된 높은 수준의 심층 feature embedding 공간에서  
합성 결과와 실측 결과 사이의 불일치를 측정하기 위한 perceptual similarity distance를 제안한다.  
응용에는 artistic stylization, image generation/synthesis, super-resolution 등이 포함된다.  
출력을 실제와 일치시킨다고 해서 출력이 자연스러워 보이지는 않기 때문에,  
cGAN은 출력과 대상 사이의 거리를 최소화하는 대신 주어진 입력의 출력 조건부 분포를 일치시키는 것을 목표로 한다.  
그 결과 실제 이미지와 같지 않을 수도 있지만, 결과물은 자연스러워 보인다.  
cGAN은 거친 컴퓨터 그래픽 rendering과 해당 실제 이미지 사이의 간격을 메우거나  
사용자가 지정한 의미 레이아웃이 주어진 사실적인 이미지를 생성하기 위해 사용되었다.  
아래에서는 NN 아키텍처와 학습 목표 모두에 대한 더 많은 기술적 세부 사항을 제공한다.  
  
### 4.2.1. Learning a Generator
조건부 입력 x ∈ X를 출력 y∈Y에 매핑할 수 있는 G를 학습하는 것을 목표로 한다. (X:입력도메인, Y:출력도메인)  
(User-provided sketch image, camera parameters, lighting conditions, scene attributes, textual descriptions)  
y는 영상, 비디오에서 voxel 또는 mesh와 같은 3D 데이터에 이르기까지 다양할 수 있다.  
(각 애플리케이션에 대해 가능한 NN 입력 및 출력의 전체 목록은 표 1을 참조)  
여기서는 일반적으로 사용되는 세 가지 G의 구조를 설명합니다.(S6의 app.별 세부 사항을 확인하도록 권장된다.)  
1) **FCN**  
FCN은 임의 크기의 입력 이미지를 촬영하고 동일한 크기로 출력을 예측한다.  
이미지를 벡터로 매핑하는 AlexNet 및 VGG와 같은 대중적인 이미지 분류 NN에 비해 FCN은 공간 이미지 해상도를 보존하기 위해  
fractionally-strided convolutions을 사용한다.  
FCN은 원래 semantic segmentation 및 object detection과 같은 recognition을 위해 설계되었지만 이미지 합성 작업에 널리 사용되었다.  
(2) **UNet**  
Unet은 localization이 향상된 FCN 기반 구조이다.  
초기 레이어의 고해상도 feature map에서 뒤 레이어의 업샘플링 feature로 이른바 "skip-connection"을 추가한다.  
입력에서 나오는 고주파 정보는 출력으로 직접 전달될 수 있기 때문에 이러한 skip-connection은 상세한 출력을 생성하는 데 도움이 된다.  
(3) ResNet  
ResNet기반 G는 residual 블록을 사용하여 고주파 정보를 출력으로 전달하며 style transfer 및 이미지 super-resolution에 사용되었다.  

### 4.2.2. Learning using Perceptual Distance
많은 입출력 pair를 수집하고 G를 선택하면, 입력이 주어졌을 때 원하는 출력을 생산하는 G를 어떻게 학습시킬 수 있을까?  
이 학습 문제에 효과적인 객관적 기능은 무엇인가? 한 가지 간단한 방법은 다음과 같이 G(x)와 y(GT) 사이의 거리를 최소화하는 것이다.  
![image](https://user-images.githubusercontent.com/40943064/145590791-676fa40c-771f-4858-9443-1244b6f68928.png)
안타깝게도 학습된 G는 흐릿한 이미지 또는 여러 그럴듯한 출력에 대한 평균 결과를 생성하는 경향이 있다.  
예를 들어, image colorization에서 학습된 G는 평균화 효과로 인해 때때로 desaturated 결과를 생성한다.  
이미지  super-resolution에서는 p-norm이 각 픽셀을 독립적으로 보기 때문에 생성기가 구조와 세부 정보를 합성하지 못한다.  

이미지 유사성에 대한 인간의 인식과 더 잘 일치하는 학습 목표를 설계하기 위해, 최근 연구는 사전 학습된  
이미지 classifier F(예: VGG)에 의해 추출된 deep feature representation 사이의 거리 측정을 제안한다.  
이러한 loss는 'p-norm'이 각 픽셀의 품질을 독립적으로 평가하는 반면, deep feature representation은  
전체 이미지를 전체적으로 요약하기 때문에 'pnorm'에 비해 유리하다.  
G는 다음과 같은 feature matching 목표를 최소화하도록 학습된다.  
![image](https://user-images.githubusercontent.com/40943064/145592173-2aabccb5-6d79-430c-9d15-ece1f645ef52.png)
F(t) :T개 레이어를 가진 사전 학습된 F의 t번째 레이어에서 feature extracter  
Nt : layer t의 총 feature 수  
Δt : 각 layer의 weight  
위의 거리는 종종 "perceptual disctance"라는 신조어로 만들어지지만, NN가 원래 이미지 합성 작업보다는  
이미지 분류 작업을 위해 훈련되었기 때문에 다단계 deep feature space에서 일치하는 통계가 인간의 인식과 일치하고  
고품질 결과 합성에 도움이 될 수 있는 이유는 흥미롭다.  
최근의 한 연구는 강력한 분류자가 학습한 풍부한 특징이 기존의 수작업 지각 메트릭스를 능가하는  
인간의 지각 작업에 유용한 표현을 제공한다는 것을 시사한다.  

### 4.2.3. Learning with Conditional GANs
그러나 출력과 실측값 사이의 거리를 최소화한다고 해서 실제처럼 보이는 출력이 보장되지는 않는다.  
그들은 또한 small distance and photorealism이 서로 상충한다는 것을 증명한다.  
따라서 거리 최소화 대신 deep generative 모델은 distribution matching, 즉 생성된 결과의 분포를 학습 데이터의 분포에 일치시키는 데 초점을 맞춘다.  
많은 유형의 생성 모델 중에서 GAN은 많은 CV 작업에 대해 유망한 결과를 보여주었다.  
Goodfellow의 원본 작업에서 GAN 생성기 G : z → y는 저차원 무작위 벡터 z에서 출력 이미지 y로의 매핑을 학습한다.  
일반적으로 입력 벡터는 다변량 가우스 또는 균등 분포에서 샘플링한다.  
G는 적대적으로 훈련된 D에 의해 "실제"와 구별할 수 없는 출력을 생성하도록 학습된다.  
D는 G에 의해 생성된 합성 이미지를 감지하도록 훈련된다.  
얼굴이나 차량과 같은 객체 범주에 대해 훈련된 GAN은 객체의 고품질 인스턴스 합성을 배우지만, 일반적으로 합성된 배경은 품질이 낮다.  
최근 논문은 전체 scene의 생성 모델을 학습하여 이 문제를 완화하려고 한다.  
조건부 정보를 입력으로 추가하기 위해, cGAN은 관찰된 입력 x와 출력 이미지 y에 무작위로 샘플링된 벡터 z로부터 매핑 G : {x,z} → y를 학습한다.  
관찰된 입력 x는 또한 D로 전달되어 이미지 pair {x, y}의 real/fake 여부를 모델링한다.  
cGAN에서 입력 x는 모델이 생성해야 하는 객체 범주를 control하는 categorical label이다.  
pix2pix와 같은 이미지 조건부 GAN의 경우, G는 semantic label map과 같은 입력 x를 사실적으로 보이는  
출력 이미지로 변환하는 것을 목표로 하는 반면 D는 real/fake를 구별하는 것을 목표로 한다.  
모델은 해당 입력 xi와 출력 이미지 yi로 구성된 쌍 데이터 세트 {xi , yi} N i=1로 학습된다.  
cGAN은 다음 미니맥스 게임을 통해 입력이 주어진 출력의 조건부 분포와 일치한다.
![image](https://user-images.githubusercontent.com/40943064/145593823-5b721158-b603-439a-aa52-ff02338116f8.png)  
일반적으로 목적함수는 아래와 같이 정의된다.  
![image](https://user-images.githubusercontent.com/40943064/145593852-d0ed3476-4d15-4bad-b6df-2c43c7cf32de.png)  

초기 cGAN 구현에서는 노이즈 벡터가 주입되지 않으며, 학습 중에 NN에 의해 무시되는 경향이 있기 때문에 매핑이 결정적이다.  
보다 최근의 연구는 multi-modal image synthesis를 가능하게 하기 위해 latent vector z를 사용한다.  
학습을 안정화시키기 위해, cGANs 기반 방법은 픽셀당 'l1 loss Lrecon(G) (Equation (1)) 및 pereceptual distance loss Lperc(G) (Equation (2))도 채택한다.  
학습 중에 D는 real/fake를 구분하는 능력을 향상시키려고 하는 반면 G는 D를 속이는 능력을 향상시키려고 한다.  
pix2pix 방법은 G의 아키텍처로 U-Net을 채택하고 D로 patch 기반 FCN을 사용한다.  

개념적으로 perceptual distance와 cGAN은 둘 다 더 나은 G를 학습하기 위한 효과적인 학습 목표를 정의하기 위해  
보조 NN(F 또는 D)를 사용하기 때문에 관련이 있다.  
High-level 추상화에서 G(x)의 품질을 평가하기 위한 정확한 CV 모델(F 또는 D)은 neural rendering 문제를 해결하는 데 크게 도움이 될 수 있다.  
그러나 두 가지 중요한 차이가 있다.  
첫째, perceptual distance는 fake와 real 사이의 불일치를 측정하는 것을 목표로 하는 반면,  
cGAN은 real/fake의 조건부 분포의 근접성을 측정한다.  
둘째, perceptual distance의 경우 feature extractor F는 사전 학습되고 고정되는 반면,  
cGAN은 G에 따라 D를 즉시 학습한다.  
실제로, 두 가지 방법은 상호 보완적이며, 많은 neural rendering 애플리케이션은 두 손실을 동시에 사용한다.  
GAN 외에도 최근 VAE, auto regressive NN(예: PixelCNN, PixelRNN), invertible density 모델 등 많은 유망한 연구 방향이 등장했다.  
StarGAN은 서로 다른 도메인을 가진 여러 데이터 세트를 기반으로 i2i 변환을 위한 단일 모델을 학습할 수 있다.  
논의를 간결하게 유지하기 위해 여기서는 GAN에 초점을 맞춘다.  
우리는 독자들이 deep generative model의 완전한 그림을 위해 튜토리얼과 코스 노트를 검토할 것을 촉구한다.  
  
  
### 4.2.4. Learning without Paired Dat
위의 목표를 가진 G를 학습하려면 수백에서 수백만 개의 pair 학습 데이터가 필요하다.  
많은 실제 애플리케이션에서 pair 학습 데이터는 수집이 어렵고 비용이 많이 든다.  
분류 작업을 위해 이미지에 레이블을 지정하는 것과 달리  annotator는 이미지 합성 작업을 위해  
모든 픽셀에 레이블을 지정해야 한다.  
예를 들어, semantic segmentation과 같은 작업을 위한 몇 개의 작은 데이터 세트만 존재한다.  
Artistic stylization과 같은 그래픽 작업을 위한 입력-출력 쌍을 얻는 것은 종종 예술적 저작이 필요하고  
때로는 잘 정의되지도 않기 때문에 훨씬 더 어려울 수 있다.  
이 설정에서 모델에는 source domain {xi} N i=1(xi x X)과 target domain {y j}j=1(y j y Y)이 지정된다.  
우리가 아는 것은 출력 G(x)가 어느 대상 도메인에서 와야 하는지이다. 즉, 도메인 Y의 이미지와 같다.  
그러나 특정 입력이 주어지면 출력이 어떤 대상 이미지여야 하는지 알 수 없다.  
X에서 Y까지의 이미지를 projection하기 위해 무한히 많은 mapping이 있을 수 있다.  
따라서 우리는 추가적인 제약이 필요하다.  
Bijective mapping 적용을 위한 cyclic consistency loss, pixel space 또는 feature embeeding 공간에서  
출력이 입력 이미지에 근접하도록 장려하기 위한 distance preserving loss,  
도메인 간 공유 표현을 학습하기 위한 weight 공유 전략 등을 포함한 여러 제약 조건이 제안되었다.  
위의 방법은 조건부 GAN의 적용 범위를 넓히고 객체 변환, 도메인 전송 및 CG2real과 같은 많은 그래픽 애플리케이션을 가능하게 한다.  


## 5. Neural Rendering

고품질의 장면 사양을 고려할 때, 고전 rendering 방법은 다양한 복잡한 실제 현상에 대해 사실적인 이미지를 rendering할 수 있다.  
또한 rendering을 통해 장면요소(카메라 시점, 조명, 기하학 및 재료)를 명시적으로 편집할 수 있다.  
그러나 특히 이미지에서 직접 고품질 장면 모델을 구축하는 데는 상당한 수작업이 필요하며  
이미지에서 자동화된 장면 모델링은 해결과제이다.  
반면 심층 생성 NN는 이제 무작위 노이즈 또는 장면 분할 및 레이아웃과 같은 특정 사용자 사양에 따라  
조정된 시각적으로 매력적인 이미지와 비디오를 생성하기 시작했다.  
장면 외관에 대한 fine control은 어려우며 장면 속성 간의 복잡한 non-local 3D interation을 항상 처리할 수는 없다.  
NR은 입력 이미지/비디오에서 새로운 이미지를 control 가능한 고품질 합성을 가능하게 하기 위해 이러한 접근 방식을 결합할 가능성을 가진다.  
NR 기술은 다양하며 장면 외관, 필요한 입력, 생성하는 출력 및 사용하는 NN 구조에 따라 다르다.  
일반적인 NR 접근법은 특정 장면 조건(예: 시점, 조명, 레이아웃 등)에 해당하는 입력 영상으로 사용하고,  
그것으로부터 "neural" 장면 표현을 구축하고, 새로운 장면 속성으로 이 표현을 "rendering"하여 새로운 영상을 합성한다.  
학습된 장면 표현은 단순한 장면 모델링 근사치에 의해 제한되지 않으며 고품질의 새로운 이미지에 최적화될 수 있다.  
동시에, NR 접근법은 입력 기능, 장면 표현 및 NN 아키텍처의 형태로 고전적인 그래픽의 아이디어를 통합하여  
학습 작업을 더 쉽게 하고 출력을 더 쉽게 control할 수 있도록 한다.  

중요하게 생각하는 축을 따라 신경 rendering 접근법의 분류법을 제안한다.  
  
• Control : Control하고자 하는 것은 무엇이며 어떻게 조절하는가?  
• CG Modules : 어떤 CG 모듈이 사용되며 NR 파이프라인에 어떻게 통합되는가?  
• Explicit or Implicit Control : 매개변수에 대한 explicit 통제를 제공하는가, 아니면 출력으로 얻을 것으로 예상되는 것의 예를 보여줌으로써 implicit으로 수행되는가?  
• Multi-modal Synthesis : 특정 입력이 주어졌을 때 여러 선택적 출력을 출력하도록 학습되었는가?  
• Generality: rendering 접근 방식이 여러 장면/객체에 걸쳐 일반화되었는가?  
  
다음에서는 현재의 SOTA 방법을 분류하는 데 사용하는 이러한 축에 대해 논의한다(표 1 참조).  
  
![image](https://user-images.githubusercontent.com/40943064/145702883-d1103a34-f839-4e9d-8b5d-f1be35e57a31.png)  
  
![image](https://user-images.githubusercontent.com/40943064/145702900-f1cc7e7e-2aea-42d5-b493-e7177f3290b5.png)  

### 5.1. Control
NR은 사용자가 지정한 장면 조건에서 고품질 이미지를 rendering하는 것을 목표로 한다.  
사례 : novel view synthesis, relighting under novel lighting, animating faces/bodies under novel expressions/poses
이러한 접근 방식이 서로 다른 주요 축은 control이 NN에 입력되는 방식에 있다.  
한 가지는 장면 매개 변수를 첫 번째 또는 중간 NN layer에 입력으로 직접 전달하는 것이다.  
관련 전략은 입력 이미지의 모든 픽셀에 걸쳐 장면 매개 변수를 타일링하거나 내부 NN layer의 활성화에 연결하는 것이다.  
또 다른 접근법은 영상의 공간 구조에 의존하고 i2i 변환 NN를 사용하여  
"guide image" 또는 "conditioning image"에서 출력으로 매핑하는 것이다.  
예를 들어, 그러한 접근법은 semantic mask에서 출력 이미지에 매핑하는 방법을 배울 수 있다.  
다음에서 설명하는 또 다른 옵션은 제어 매개 변수를 그래픽 레이어 입력으로 사용하는 것이다.

### 5.2. Computer Graphics Modules
NR의 새로운 경향 중 하나는 CG 지식을 NN 설계에 통합하는 것이다.  
따라서 접근 방식은 시스템에 내장된 "고전적인" CG 지식 수준에 따라 다를 수 있다.  
예를 들어 장면 파라미터에서 출력 영상으로 직접 매핑하면 그래픽 지식이 사용되지 않는다.  
그래픽 지식을 통합하는 간단한 방법 중 하나는 미분 불가능한 CG 모듈이다.  
이러한 모듈은 예를 들어 장면의 이미지를 렌더링하고 이를 NN에 밀도 조절 입력으로 전달하는 데 사용할 수 있다.  
깊이 맵, 일반 맵, 카메라/세계 공간 위치 맵, albedo 맵, 장면의 분산 렌더링 등과 같은 다양한 채널이 NN 입력으로 제공될 수 있다.  
이는 문제를 잘 연구된 설정인 이미지 대 이미지 변환 작업으로 변환하며, 예를 들어 스킵 연결이 있는 심층 조건부 생성 모델로 해결할 수 있다.  
차별화된 그래픽 모듈을 기반으로 그래픽 지식을 NN에 더 깊이 통합할 수 있습니다.  
이러한 차별화 가능한 모듈은 예를 들어 완전한 컴퓨터 그래픽 렌더러, 3D 회전 또는 조명 모델을 구현할 수 있다.  
이러한 구성 요소는 NN에 물리적으로 영감을 받은 유도 편향을 추가하는 동시에 역전파를 통한 종단 간 학습을 가능하게 한다.  
이는 특히 제한된 학습 데이터만 사용할 수 있는 경우 NN 구조에서 세상에 대한 정보를 분석적으로 적용하고  
NN 용량을 확보하며 더 나은 일반화로 이어지는 데 사용될 수 있다.

### 5.3. Explicit vs. Implicit Control
NR 접근법을 분류하는 또 다른 방법은 제어 방식이다.  
일부 접근방식은 explicit 제어를 허용한다. 즉, 사용자가 semantically meaningful한 방식으로 장면 파라미터를 수동으로 편집할 수 있다.  
예를 들어, 현재의 NR 접근 방식을 사용하면 카메라 시점, 장면 조명, 얼굴 자세 및 표정을 explicit으로 제어할 수 있다.  
다른 접근법은 representative sample을 통한 implicit control 만 허용한다.  
기준 영상/비디오에서 장면 파라미터를 복사할 수 있지만 이러한 파라미터를 explicit으로 조작할 수는 없다.  
여기에는 인간의 머리 움직임을 참조 비디오에서 대상 사람에게 전송하는 방법 또는 전신 모션의 대상을 변경하는 방법이 포함된다.  
explicit 제어를 허용하는 방법에는 이미지/비디오 및 해당 장면 매개 변수가 포함된 학습 데이터 세트가 필요하다.  
반면에, implicit control은 보통 더 적은 감독을 요구한다. 이러한 방법은 explicit 3D 장면 매개 변수 없이 약한 주석을 통해서만 학습할 수 있다.  
예를 들어, 얼굴 재현을 위한 explicit 제어로 NN를 학습시키려면 조밀한 얼굴 성능 캡처가 필요하지만,  
해당하는 희박한 2D 요점이 있는 비디오에서만 학습함으로써 implicit control을 달성할 수 있다.  

### 5.4. Multi-modal Synthesis
보통 선택할 수 있는 출력 옵션이 여러 개 있는 것이 유용하다. 
장면 파라미터의 하위 집합만 제어되는 경우 다른 장면 파라미터와 관련하여 큰 multi-modal 출력 공간이 존재할 수 있다.  
하나의 출력으로 표시되지 않고 사용자는 여러 가지 선택가능한 결과물을 볼 수 있다.  
이 경우 사용자가 출력 환경을 더 잘 이해하고 원하는 결과를 선택하는 데 도움이 된다.  
NN 또는 control 신호는 서로 현저히 다른 다양한 출력을 달성하려면 확률성 또는 구조화된 분산을 가져야 한다.  
예를 들어 VAE는 가변성이 내장된 프로세스를 모델링하며 multi-modal synthesis을 달성하는 데 사용할 수 있다.  
가장 최근의 예는 Park으로, 가변성을 통합하는 한 가지 방법을 시연하고 UI를 통해 표면화한다.  
동일한 의미 맵이 주어지면 버튼을 누르면 현저하게 다른 이미지가 생성된다.  

### 5.5 Generality
NR은 object specificity가 다르다.  
1) 범용 모델 학습 + 해당 작업의 모든 인스턴스에 적용  
만약 이 방법이 인간의 머리에 적용된다면, 그것은 모든 사람에게 적용되는 것을 목표로 한다.  
2) 인스턴스별 학습  
(특정 의상/위치에서) 개인에 대해 운영되며 새로운 NN는 각 주제에 대해 재학습되어야 한다.  
  
많은 작업에서 객체별 접근 방식은 현재 각 객체 인스턴스에 대한 긴 학습 시간을 희생하면서 더 높은 품질의 결과를 산출하고 있다.  
실제 애플리케이션의 경우 이러한 학습 시간은 매우 중요하다.  
일반 모델을 개선하는 것은 미해결 문제이며 흥미로운 연구 방향이다.  

## 6. Applications of Neural Rendering
NR은 다음과 같은 여러 중요한 활용사례를 가진다.  
1) semantic photo manipulation
2) novel view synthesis
3) relighting
4) free viewpoint video
5) facial and body reenactment
  
표 1은 본 설문조사에서 논의된 다양한 적용에 대한 개요를 제공한다.  
각각에 대해 다음 특성을 정리한다.  
  
![image](https://user-images.githubusercontent.com/40943064/145716482-7b1e4094-53cb-42be-9be2-6b4aaa1652a3.png)  
![image](https://user-images.githubusercontent.com/40943064/145716491-eff0aed3-9f8c-481b-a2bc-111145fb2b3e.png)  

### 6.1. Semantic Photo Synthesis and Manipulation
사진모양을 semantically meaningful 방식으로 제어하고 수정하는 대화형 이미지 편집 도구를 가능하게 한다.  
중요한 작업인 Image Analogies는 patch 기반 텍스처 합성을 사용하여 semantic 레이아웃과  
reference 이미지가 주어지면 새로운 텍스처를 만든다.  
이러한 단일 이미지 patch 기반 방법은 이미지 reshuffling, retargeting, inpainting은 가능하지만  
개체를 추가하거나 이미지를 처음부터 합성하는 것과 같은 높은 수준의 작업은 허용할 수 없다.  
데이터 기반 그래픽 시스템은 대규모 사진 수집에서 검색된 이미지에서 여러 이미지 영역을 합성하여 새로운 이미지를 만든다.  
이러한 방법을 사용하면 스케치 또는 의미 레이블 맵과 같은 입력을 사용하여 원하는 장면 레이아웃을 지정할 수 있다.  
가장 최근의 개발은 장면 컨텍스트, 모양 및 부품을 일치시켜 영역을 구성하는 OpenShapes이다.  
이러한 시스템은 매력적인 결과를 얻으면서도 대형 이미지 DB에서 검색할 때 속도가 느린 경우가 많다.  
또한 이미지 간의 시각적 불일치로 인해 원하지 않는 아티팩트가 발견되기도 한다.  

### 6.1.1. Semantic Photo Synthesis
Non-parametric 방식과 달리 최근에는 cGAN objective를 사용하여 FCN을 학습하여 semantic layout/색상/스케치/질감과 같은  
사용자 입력을 사실적인 이미지에 직접 매핑했다.  
이 중 pix2pix와 Karacan은 학습기반 semantic 이미지 합성(street view, natural scene 생성 포함)을 최초로 제시했다.  
이미지 해상도를 높이기 위해 Cascaded Refinement NN은 perceptual loss로 학습된 coarse-fine G를 학습한다.  
고해상도 결과물을 생성하지만 고주파 텍스처와 디테일이 부족하다.  
pix2pixHD는 풍부한 세부 정보를 합성하기 위해 사실적인 질감으로 2048 × 1024를 생성하는 cGAN을 제안한다.  
pix2pix와 비교하여 주요 확장 기능에는 CRN과 유사한  coarse-to-fine G, 여러 scale local 이미지 통계를 캡처하는 multi-scale D,  
perceptual distance와 유사하지만 대신 작업별 특징을 추출하기 위해 적응형 D를 사용하는 multi-scale D 기반 feature-matching objective가 있다.  
특히 비전 및 그래픽 분야에서 수십 년 된 방식인 multi-scale 파이프라인은 deep image synthesis에 여전히 매우 효과적이다.  
pix2pixHD와 BicycleGAN은 동일한 사용자 입력이 주어지면 가능한 여러 출력을 합성할 수 있으므로 사용자가 다른 스타일을 선택할 수 있다.  
후속 시스템은 비디오 도메인으로 확장되어 사용자가 비디오의 의미를 제어할 수 있다.  
Semi-parametric systems은 기존의 데이터 기반 이미지 합성과 FNN을 결합한다.  
가장 최근에 GauGAN은 **SP**atial **A**daptive **DE**normalization layer를 사용하여 G의 의미 정보를 더 잘 보존한다.  
이전 조건부 모델이 여러 정규화 layer(예: InstanceNorm)를 통해 의미론적 레이아웃을 처리하는 동안 채널별 정규화 계층은  
특히 균일하고 평평한 입력 레이아웃 영역에 대해 의미론적 정보를 "씻어 버리는" 경향이 있다.  
대신 GauGAN G는 이미지 스타일 코드로 임의의 잠재 벡터를 사용하고 공간 적응 정규화 계층(SPADE)이  
있는 여러 ResNet 블록을 사용하여 최종 출력을 생성한다.  
그림 2에서 볼 수 있듯이 이 디자인은 시각적으로 매력적인 결과를 제공할 뿐만 아니라 스타일과 의미 체계에 대한 사용자 제어를 향상시킨다.  
Adaptive normalization layer는 또한 stylization/super-resolution에 효과적이다.  
![image](https://user-images.githubusercontent.com/40943064/145835630-7cbbcd44-2274-48dd-965c-c0a821d065ce.png)

### 6.1.2. Semantic Image Manipulation
위의 이미지 합성 시스템은 입력으로 사용자 컨트롤이 주어지면 새로운 시각적 contents를 만드는 데 탁월하다.  
그러나 deep generative model을 사용하여 사용자가 제공한 이미지의 semantic 이미지 조작은 두 가지 이유로 여전히 어려운 과제이다.  
첫째, 입력 영상을 편집하려면 G로 정확하게 재구성해야 하는데, 이는 최근 GAN으로도 어려운 작업이다.  
둘째, 제어가 적용되면 새로 합성된 contents가 입력 사진과 호환되지 않을 수 있다.  
이러한 문제를 해결하기 위해 iGAN은 이미지 편집 작업에 앞서 무조건 GAN을 자연 이미지로 사용할 것을 제안한다.  
이 방법은 먼저 GAN이 입력 사진을 충실히 재현할 수 있도록 저차원 잠재 벡터를 최적화한다.  
재구성 방법은 quasi-Newton 최적화와 인코더 기반 초기화를 결합한다.  
그런 다음 시스템은 색상, 스케치 및 뒤틀기 도구를 사용하여 생성된 이미지의 모양을 수정한다.  
결과를 rendering하기 위해 가이드 이미지 필터링을 사용하여 생성된 이미지에서 원본 사진으로 편집 내용을 전송한다.  
Neural Photo Editing에 대한 후속 작업은 VAE-GAN을 사용하여 이미지를 latent 벡터로 인코딩하고 수정된 contents와 원본 픽셀을 혼합하여 출력을 생성한다.  
시스템은 수염 추가와 같은 얼굴의 semantic 편집을 허용한다. 여러 작품에서 G와 함께 인코더를 학습한다.  
그들은 추가 이미지 속성(예: semantics, 3D 정보, facial attribute)을 예측하고 사용자가 이러한 속성을 수정할 수 있도록 두 번째 인코더를 배포한다.  
GAN을 사전 이미지로 사용하는 이 아이디어는 나중에 이미지 인페인팅 및 deblurring에 사용되었다.  
위의 시스템은 단일 객체 또는 특정 클래스의 저해상도 이미지에서 잘 작동하며  
직접 GAN의 결과가 충분히 현실적이지 않기 때문에 종종 후처리(예: 필터링 및 혼합)가 필요한다.  
이러한 문제를 극복하기 위해 GANPaint는 사전 학습된 GAN 모델을 특정 이미지에 적용한다.  
학습된 이미지별 GAN은 전체 이미지 컬렉션과 해당 특정 이미지의 이미지 통계에서 이전에 학습된 것을 결합한다.  
이전 작업과 유사하게 이 방법은 먼저 입력 이미지를 latent 벡터에 projection한다.  
벡터로부터의 reconstruction은 입력에 가깝지만 많은 시각적 세부 사항이 누락된다.  
그런 다음 이 방법은 네트워크의 내부 매개변수를 약간 변경하여 입력 이미지를 보다 정확하게 재구성한다.  
테스트 시간 동안 GANPaint는 사용자 입력에 따라 GAN의 중간 표현을 수정한다.  
Deep Image Prior에서 수행된 것처럼 단일 이미지에서 무작위로 초기화된 CNN을 학습하는 대신  
GANPaint는 사전 학습된 생성 모델에서 학습한 prior을 활용하고 각 입력 이미지에 대해 미세 조정한다.  
그림 3과 같이 이를 통해 특정 객체를 사실적으로 추가 및 제거할 수 있다.  
사전 훈련을 통한 분포 사전 학습과 제한된 데이터에 대한 미세 조정은 많은 One-shot 및 Few-shot 합성 시나리오에 유용하다.  
![image](https://user-images.githubusercontent.com/40943064/145996872-0350212c-0ab7-4f59-bb5d-59ccb1a87442.png)

### 6.2.2. Neural Rerendering
Neural Rerendering은 고전적인 3D 표현과 렌더러를 심층 신경망과 결합하여 고전적인 렌더링을 보다 완전하고 사실적인 뷰로 다시 렌더링한다.  
신경 이미지 기반 렌더링(N-IBR)과 달리 신경 렌더링은 런타임에 입력 보기를 사용하지 않고  
대신 심층 신경망에 의존하여 누락된 세부 정보를 복구한다.  
Neural Rerendering in the Wild는 다양한 조명 조건에서 관광지 랜드마크의 사실적인 뷰를 합성하기 위해  
NR을 사용한다 (그림 4 참조).  
저자는 이 문제를 appearance code와 함께 깊이 및 색상 채널을 포함하는 렌더링된 깊은 버퍼를 입력으로 취하고  
장면의 사실적인 보기를 출력하는 다중 모드 이미지 합성 문제로 문제를 던졌다.  
시스템은 Structure-from-Motion 및 Multi-View Stereo를 사용하여 인터넷 사진에서 짙은 색상의 포인트 클라우드를 재구성하고  
각 입력 사진에 대해 복구된 포인트 클라우드를 추정된 카메라에 렌더링한다.  
실제 사진 쌍과 해당 렌더링된 딥 버퍼를 사용하여 다중 모드 이미지 합성 파이프라인은 3D 모델에 없는 시간,  
기상 조건 및 기타 속성을 나타내는 암시적 모양 모델을 학습한다.  
모델이 보행자나 자동차와 같은 일시적인 개체를 합성하는 것을 방지하기 위해  
저자는 예상 이미지의 의미 체계 레이블로 렌더러를 조정할 것을 제안한다.  
추론할 때 이러한 의미 체계 레이블은 그러한 일시적인 개체를 생략하도록 구성할 수 있다.  
Pittaluga는 신경 렌더링 기술을 사용하여 동작에서 구조 재구성을 반전하고 일반적으로 색상 및 SIFT 기능을 포함하는 동작에서  
구조 3D 재구성의 개인 정보 위험을 강조한다.  
저자는 성긴 포인트 클라우드가 어떻게 반전될 수 있는지 보여주고 그로부터 사실적인 새로운 뷰를 생성한다.  
매우 희박한 입력을 처리하기 위해 포인트를 가시적이거나 표시되지 않는 것으로 분류하고  
3D 재구성의 실제 대응으로 훈련되는 가시성 네트워크를 제안한다.  
 
### 6.2.3. Novel View Synthesis with Multiplane Images
개체에 대한 입력 보기의 희소 집합이 주어지면 Xu 또한 새로운 관점에서 객체를 렌더링하는 문제를 해결한다 (그림 5 참조).  
자연 조명과 작은 기준선에서 캡처한 이미지와 함께 작동하는 이전 뷰 보간 방법과 달리  
이 방법은 반사와 같은 뷰 종속 효과를 포함하여 장면의 빛 전달을 캡처하는 것을 목표로 한다.  
또한 캡처 프로세스를 보다 가볍게 만들기 위해 큰 기준선에서 캡처된 희소 이미지 세트에서 이를 수행하려고 한다.  
그들은 약 60º의 원뿔에서 점 조명 아래 장면의 6개 이미지를 캡처하고 이 원뿔 내에서 새로운 관점을 렌더링한다.  
입력 이미지는 새로운 관점과 정렬된 평면 스위핑 볼륨을 구성하는 데 사용된다.  
이 볼륨은 3D CNN에 의해 처리되어 장면 깊이와 모양을 모두 재구성한다.  
큰 기준선으로 인한 occlousion을 처리하기 위해 다른 픽셀에서 입력 시점의 가시성을 캡처하는 예측 주의 맵을 제안한다.  
이러한 어텐션 맵은 모양 평면 스윕 볼륨을 조정하고 일관성 없는 콘텐츠를 제거하는 데 사용된다.  
네트워크는 지오메트리와 모양 모두에 대한 감독과 함께 종합적으로 렌더링된 데이터에 대해 훈련된다.   
테스트 시간에 그림자 및 반사와 같은 고주파 광 전달 효과를 특징으로 하는 실제 장면의 사실적인 결과를 합성할 수 있다.  
DeepView는 새로운 뷰에서 라이트 필드를 시각화하는 기술이다.  
보기 합성은 입력 보기의 희소 집합이 주어지면 학습된 경사 하강법으로 추정되는 다중 평면 이미지를 기반으로 한다.  
이미지 기반 렌더링과 유사하게 이미지 평면은 새 보기로 뒤틀릴 수 있으며 대상 이미지에 대해 앞뒤로 렌더링된다.  

### 6.2.4. Neural Scene Representation and Rendering 
다중 평면 이미지 및 이미지 기반 렌더링을 기반으로 하는 신경 렌더링 방법은 몇 가지 인상적인 결과를 가능하게 했지만  
장면에 대한 모델의 내부 표현을 포인트 클라우드, 다중 평면 이미지 또는 메쉬로 규정하고 허용하지 않는다.   
모델을 사용하여 장면의 형상과 모양에 대한 최적의 표현을 학습한다.  
새로운 뷰 합성의 최근 라인은 신경 장면 표현으로 모델을 구축하는 것이다.  
학습된 특징 기반 장면 속성 표현이다.  
Generative Query Network는 불완전한 관찰로 인한 신경 장면 표현의 확률적 특성을 명시적으로 
모델링하는 장면의 저차원 기능 임베딩을 학습하기 위한 프레임워크이다.  
장면은 관찰 컬렉션으로 표현되며, 각 관찰은 이미지와 해당 카메라 포즈의 튜플이다.  
컨텍스트 관찰 세트와 대상 카메라 포즈를 조건으로 하는 GQN은 컨텍스트 관찰과 일치하여 대상 카메라 포즈에서 
관찰된 프레임에 대한 분포를 매개변수화한다.  
GQN은 컨텍스트와 동일한 장면의 다른 관찰이 주어지면 각 관찰의 로그 가능성을 최대화하여 훈련된다.  
단일 장면에 대한 여러 컨텍스트 관찰이 주어지면 컨볼루션 인코더는 각각을 저차원 잠재 벡터로 인코딩한다.  
이러한 잠재 벡터는 합계를 통해 단일 표현 r로 집계된다.  
ConvLSTM(Convolutional Long-Short Term Memory network)은 잠재 변수 z에 대한 자동 회귀 사전 분포를 매개변수화한다.  
모든 시간 단계에서 ConvLSTM의 숨겨진 상태는 샘플링된 관찰을 나타내는 캔버스 u에 대한 잔여 업데이트로 디코딩된다.  
최적화 문제를 다루기 쉽게 만들기 위해 GQN은 훈련 시간에 근사 사후값을 사용한다.  
저자는 새로운 뷰 합성, 시뮬레이션된 로봇 팔 제어 및 미로 환경 탐색에서 장면의 풍부한 기능 표현을 학습하는 GQN의 기능을 보여준다.  
DQN의 확률적 공식을 통해 모델은 컨텍스트 관찰과 일치하는 서로 다른 프레임을 샘플링할 수 있으며, 
예를 들어 컨텍스트 관찰에서 가려진 장면 부분에 대한 불확실성을 캡처할 수 있다.  

### 6.2.5. Voxel-based Novel View Synthesis Methods
학습된 비정형 신경 장면 표현은 손으로 만든 장면 표현에 대한 매력적인 대안이지만 여러 가지 단점이 있다.  
무엇보다도 그들은 장면의 자연스러운 3D 구조를 무시한다.  
결과적으로 제한된 훈련 데이터 영역에서 다중 뷰 및 원근 기하학을 발견하지 못한다.  
기하학적 DL의 최근 발전에 영감을 받아 장면을 복셀 그리드로 표현하여 3D 구조를 적용하도록 제안하는 일련의 신경 렌더링 접근 방식이 등장했다.  
  
**RenderNet**은 3D 복셀 그리드로 명시적으로 표현된 장면에서 미분 가능한 렌더링을 구현하는 컨볼루션 신경망 아키텍처를 제안한다.  
모델은 각 객체 클래스에 대해 재학습되며 레이블이 지정된 카메라 포즈가 있는 이미지 튜플이 필요하다.  
RenderNet은 새로운 뷰 합성, 텍스처 편집, 재조명 및 음영 처리를 가능하게 하다.  
카메라 포즈를 사용하여 복셀 그리드가 먼저 카메라 좌표로 변환된다.  
3D 컨볼루션 세트는 3D 특징을 추출한다.  
기능의 3D 복셀 그리드는 "투영 장치"라는 하위 네트워크를 통해 2D 기능 맵으로 변환된다.  
로젝션 유닛은 먼저 3D 기능 복셀 그리드의 마지막 두 채널을 축소한 다음 1x1 2D 컨볼루션을 통해 채널 수를 줄인다.  
1x1 컨볼루션은 단일 카메라 광선을 따라 모든 기능에 액세스할 수 있으므로 일반적인 클래식 렌더러의 투영 및 가시성 계산을 수행할 수 있다.  
마지막으로 2D 상향 컨볼루션 신경망은 2D 기능 맵을 업샘플링하고 최종 출력을 계산한다.  
저자는 RenderNet이 저해상도 복셀 그리드에서 고해상도 이미지를 렌더링하는 방법을 배운다는 것을 보여준다.  
RenderNet은 다양한 텍스처와 셰이더를 적용하는 방법을 추가로 학습하여 장면 재조명 및 조작된 장면의 새로운 뷰 합성을 가능하게 한다.  
그들은 또한 RenderNet이 반복적인 재구성 알고리즘을 통해 단일 이미지에서 장면의 3D 복셀 그리드 표현을 복구하는 데 
사용될 수 있음을 보여줌으로써 표현의 후속 조작을 가능하게 한다.  
  
**DeepVoxels**는 장면의 형상과 모양의 공동 재구성 및 후속 새로운 뷰 합성을 가능하게 한다.  
DeepVoxels는 이미지와 외부 및 내부 카메라 매개변수만 제공되는 특정 장면에서 훈련되며 명시적인 장면 형상이 필요하지 않다.  
이는 다중 뷰 및 투영 기하학 연산자를 사용하여 이미지 형성을 명시적으로 구현하는 
네트워크 아키텍처와 결합된 임베디드 기능의 데카르트 3D 그리드로 장면을 표현함으로써 달성된다.  
특징은 먼저 2D 관찰에서 추출된다.  
그런 다음 2D 기능은 각각의 카메라 광선을 따라 복제하여 투영되지 않고 작은 3D U-net에 의해 복셀 그리드에 통합된다.  
주어진 카메라 외부 및 내부 매개변수를 사용하여 장면을 렌더링하기 위해 가상 카메라가 세계 좌표에 배치된다.  
고유한 카메라 매개변수를 사용하여 복셀 그리드가 표준 보기 볼륨으로 다시 샘플링된다.  
Occlusion을 추론하기 위해 저자는 occlusion 추론 모듈을 제안한다.  
오클루전 모듈은 3D U-Net으로 구현되어 카메라 광선을 따라 모든 기능과 깊이를 입력으로 받고 각 광선에 따른 
점수의 합이 1인 광선을 따라 각 기능에 대한 가시성 점수를 출력으로 생성한다.  
그런 다음 최종 투영된 특징은 각 광선을 따라 특징의 가중치 합으로 계산된다.  
마지막으로 결과 2D 기능 맵은 작은 UNet을 사용하여 이미지로 변환된다.  
Occlusion 추론 모듈의 부작용으로 DeepVoxels는 감독되지 않은 방식으로 깊이 맵을 생성한다.  
모델은 끝에서 끝까지 완전히 구별할 수 있으며 훈련 세트에 적용된 2D 재렌더링 손실에 의해서만 감독된다.  
이 논문은 합성 및 실제의 여러 도전적인 장면에서 넓은 기준선의 새로운 뷰 합성을 보여주며 3D 구조를 사용하지 않는 기준선보다 큰 차이를 보인다.  
  
**VON(Visual Object Networks)** 은 얽힌 3D 표현으로 개체의 모양을 합성하기 위한 3D 인식 생성 모델이다.  
VON은 고전적인 렌더링 파이프라인에서 영감을 받아 신경 이미지 형성 모델을 관점, 모양 및 질감의 세 가지 요소로 분해한다.  
모델은 미분 가능한 프로젝션 모듈을 통해 2D 이미지와 3D 모양의 분포를 공동으로 학습하는 종단 간 적대적 학습 프레임워크로 학습된다.  
테스트 시간 동안 VON은 중간 2인 3D 모양을 합성할 수 있다.  
3D 깊이 표현과 최종 2D 이미지를 한번에 이 3D 풀림을 통해 사용자는 개체의 모양, 관점 및 질감을 독립적으로 조작할 수 있다.  

**HoloGAN**은 RenderNet의 학습된 투영 단위를 기반으로 하여 명시적 관점 변경을 허용하는 무조건 생성 모델을 빌드한다.  
학습된 3D 기능에 뷰 조작을 직접 적용하는 명시적 아핀 변환 레이어를 구현한다.  
DeepVoxels에서와 같이 네트워크는 3D 기능 공간을 학습하지만 이러한 딥 복셀을  
임의의 잠재 벡터 z로 변환함으로써 3D 객체/장면에 대한 더 많은 편향이 도입된다.  
이러한 방식으로 관점 변경을 기본적으로 지원하는 무조건적인 GAN은 감독되지 않은 방식으로 훈련될 수 있다.  
특히 HoloGAN에는 포즈 레이블 및 고유 카메라 정보나 개체에 대한 다중 뷰가 필요하지 않다.  

### 6.2.6. Implicit-function based Approaches
3D 복셀 그리드는 3D 구조화된 장면 표현이 다중 뷰 일관성 있는 장면 모델링에 도움이 된다는 것을 입증했지만,  
메모리 요구 사항은 공간 해상도에 따라 입체적으로 확장되며 표면을 부드럽게 매개 변수화하지 않으므로  
신경망이 인접 복셀의 결합 확률로 모양에 대한 사전 학습이 필요하다.  
결과적으로, 그들은 충분한 공간 해상도로 큰 장면을 파라미터화할 수 없으며,  
지금까지 장면 전체에 걸쳐 모양과 외관을 일반화하지 못하여 소수의 관측치에서만 장면 기하학 재구성 같은 응용이 가능하다.  
기하학적 DL에서 최근 연구는 형상을 신경망의 레벨 세트로 모델링하여 이러한 문제를 완화했다.  
최근의 NR 작업은 이러한 접근 방식을 일반화하여 풀 컬러 이미지의 렌더링을 허용한다.  
Pixel-Aligned Implicit Functions는 암묵적 함수를 통해 지표면 형상을 파라미터화할 뿐만 아니라   
암묵적 함수를 통해 객체 색상을 나타냅니다.  
이미지는 먼저 컨볼루션 신경망을 통해 픽셀 단위 피처 맵으로 인코딩된다.  
완전히 연결된 신경망은 특정 픽셀 위치의 특징과 깊이 값 z를 입력으로 받아 깊이를 물체의 내부/외부로 분류한다.  
동일한 아키텍처가 색상을 인코딩하는 데 사용됩니다.  
모델은 엔드 투 엔드 방식으로 훈련되며, 이미지와 3D 지오메트리로 감독된다.  
저자들은 옷을 입은 인간의 싱글샷 및 멀티샷 3D 재구성 및 새로운 시각 합성을 시연한다.  
장면 표현 네트워크(SRN)는 세계 좌표를 로컬 장면 속성의 형상 표현에 매핑하는   
단일 완전 연결 신경망인 SRN에서 장면 지오메트리와 외관 모두를 인코딩한다.  
구별 가능하고 학습된 신경 광선 분석기는 이미지와 그 외적 및 내재적 카메라 매개 변수만 주어진 상태에서 종단 간 훈련을 받는다.   
즉, 실측 자료 형상 정보가 필요하지 않다.  
SRN은 입력 (x, y,z) 월드 좌표로 사용하고 형상 임베딩을 계산한다.  
이미지를 렌더링하기 위해, 카메라 광선은 구별 가능한 학습된 레이마커를 통해 장면 지오메트리(있는 경우)와의 교차점까지 추적되며,   
이 레이마처는 현재 교차점 추정치에서 SRN에 의해 반환된 특징을 기반으로 다음 단계의 길이를 계산한다.  
그런 다음 SRN은 광선 교차점에서 샘플링되어 모든 픽셀에 대한 특징을 제공한다.  
이 2D 피쳐 맵은 픽셀 단위로 완전히 연결된 네트워크에 의해 이미지로 변환됩니다.  
DeepSDF와 유사하게 SRN은 코드 벡터 z로 각 장면을 표현함으로써 동일한 클래스의 장면에서 일반화된다.  
코드 벡터 z는 소위 하이퍼 네트워크라고 불리는 완전히 연결된 신경망을 통해 SRN의 매개 변수에 매핑된다.  
하이퍼 네트워크의 매개 변수는 코드 벡터 및 픽셀 생성기의 매개 변수와 함께 최적화된다.  
저자들은 ShapeNet 데이터 세트(그림 6)에서 형상의 단일 이미지 재구성과 객체의 모양(multiple-view consistent view synthesis)을 시연하고 있다.  
SRN은 픽셀 단위 제형으로 인해 줌이나 카메라 롤처럼 완전히 보이지 않는 카메라 포즈로 일반화된다.  


### 6.3. Free Viewpoint Videos
### 6.3.1. LookinGood with Neural Rerendering
###6.3.2. Neural Volumes
### 6.3.3. Free Viewpoint Videos from a Single Sensor

### 6.4. Learning to Relight
### 6.4.1. Deep Image-based Relighting from Sparse Samples
### 6.4.2. Multi-view Scene Relighting
### 6.4.3. Deep Reflectance Fields
### 6.4.4. Single Image Portrait Relighting

### 6.5. Facial Reenactment
### 6.5.1. Deep Video Portraits
### 6.5.2. Editing Video by Editing Text
### 6.5.3. Image Synthesis using Neural Textures
### 6.5.4. Neural Talking Head Models
### 6.5.5. Deep Appearance Models
### 6.6. Body Reenactment

## 7. Open Challenges
**Generalization**
**Scalability**
**Social Implications**
**Multimodal Neural Scene Representations**

## 8. Social Implications
### 8.1. Forgery Detection
## 9. Conclusion
