# State of the Art on Neural Rendering

### Nomenclature
1. NR : Neural Rendring
2. CG : Cumputer Graphics
3. CV : Computer Visions
4. ML : Machine Learning

## Abstract 
현실같은 가상 세계를 구현하기 위한 **효율적인 렌더링**은 컴퓨터 그래픽에서 오랜 노력의 대상이었다.  
현대 그래픽 기법은 수작업 장면 표현을 통한 진짜같은 이미지 합성에 성공적인 결과들을 얻어왔다.  
그러나 모양/재질/빛 그리고 장면에서의 다른 면들에 대한 자동 생성은 여전히 도전적인 문제로 남겨져 있으며  
이러한 문제가 풀린다면 진짜같은 컴퓨터 그래픽을 통해 더 광범위하게 사용될 것이다.  

동시에, CV와 ML의 진보는 Deep generative model이라 하는 새로운 이미지 합성 및 편집 방식을 낳았다.  
NR은 **Generative ML**과 **CG의 물리적 지식**을 결합한 급격하게 발전되는 분야이다.  
(미분가능 렌더링을 네트워크 학습에 통합함으로써)  
CG와 CV에서 수많은 application을 가지면서 neural rendering은 그래픽스 커뮤니티에서 새로운 분야가 되고있다.  
본 SOTA 리포트는 neural rendering의 최근 트렌드와 application을 요약한다.  
  
control 가능하고 진짜 같은 출력을 얻기 위해 **전통 CG**와 **Deep generative model**을 결합하는 접근법에 집중한다.  
CG와 ML 컨셉에 대한 개요로 시작해서 우리는 neural rendering 접근법의 중요한 점을 논의한다.  
특히, control 방식, 파이프라인의 어떤 부분을 학습하는지, 명시적 control 대 암묵적 control, 일반화 및 확률적 대 결정론적 합성에 중점을 둔다.  
  
이 SOTA 리포트의 뒤에는 다음의 분야에 집중한다. 
1. Novel view synthesis
2. Semantic photo manipulation
3. Facial and body reenactment
4. Relighting
5. Free-viewpoint video
6. The creation of photo-realistic avatars for virtual and augmented reality telepresence
  
마지막으로, 기술의 사회적 의미에 대한 토론으로 결론을 내리고 공개 연구 문제를 조사한다.

## 1. Introduction
가상세계의 진짜같은 이미지 생성은 정교한 CG 개발의 최우선 추진 과제 중 하나다.  

CG 접근은 
1) (최신 컴퓨터 게임 생성을 가능하게 하는)실시간 렌더링부터  
2) (장편 영화에서의 사진 촬영 디지털 인간 생성을 위한) 정교한 글로벌 조명 시뮬레이션가 있다.  
두 경우에서의 주요 병목은 content 생성이다.  
Surface geometry, appearance/material, illumination 및 animation 측면에서  
기본 scene 표현을 만들기 위해 숙련된 예술가의 방대한 양의 지루하고 값 비싼 수작업이 필요하다.  
동시에, 강력한 Generative 모델이 CV와 ML에 나타나기 시작했다.  
GAN에 대한 중요한 작업은 최근 몇 년 동안 고해상도 이미지와 비디오 생성을 위한 deep generative 모델로 발전했다.  
여기서, 다른 도메인의 control 매개 변수 또는 이미지에서 네트워크를 조절하여 합성 콘텐츠에 대한 control를 달성할 수 있다.  
최근에는 두 영역이 합쳐져 "NR"으로 연구되고 있다. 이 용어를 사용한 최초의 출판물 중 하나는 GQN이다.  
이는 기계가 표현과 생성 네트워크를 기반으로 주변 환경을 인식하는 것을 배울 수 있게 한다.  
네트워크가 장면의 여러 이미지를 입력으로 받아들이고 정확한 occlusion으로 임의 뷰를 출력할 수 있기 때문에  
3D에 대한 암묵적 개념을 가지고 있다고 주장한다.  
암묵적인 3D 개념 대신 그래픽 파이프라인의 구성 요소를 이용하여 3D의 개념을 더 명확하게 포함하는 다양한 다른 방법들이 뒤따랐다. 
고전적인 CG는 geometry, surface properties 및 카메라와 같은 물리학의 관점에서 출발하지만, 기계 학습은 통계적인 관점에서 이루어진다.  

이를 위해 CG 생성 이미지의 품질은 채택된 **모델의 물리적 정확성에 의존**하는 반면,  
ML 접근법의 품질은 대부분 신중하게 **ML 모델과 사용된 훈련 데이터의 품질**에 의존한다.  
장면 속성의 명시적 재구성은 어렵고 오류가 발생하기 쉬우며 렌더링된 콘텐츠에 왜곡이 발생한다.  
이를 위해 이미지 기반 렌더링 방법은 간단한 경험론을 사용하여 캡처된 이미지를 결합하여 이러한 문제를 극복한다.  
그러나 복잡한 풍경에서, 이 방법들은 seams나 ghost과 같은 artifact를 보여준다.  
NR은 딥 네트워크를 사용하여 캡처된 이미지에서 새로운 이미지로의 복잡한 매핑을 학습함으로써 재구성 및 렌더링을 모두 처리할 수 있는 가능성을 제공한다.  
NR은 수학적 투영 모델과 같은 물리적 지식을 학습된 구성 요소와 결합하여 control 가능한 이미지 생성을 위한 새롭고 강력한 알고리즘을 산출한다.  
NR은 문헌에서 아직 명확한 정의가 없다. 여기서는 NR을 다음과 같이 정의한다.  
  
_(조명/카메라/자세/geometry/외모/semantic 구조)등의 장면특성control를  
(명시적이거나 암시적)으로 가능하게 하는 (Deep 이미지/영상 생성 접근)_  

이 SOTA 보고서는 여러 유형의 NR 방법을 정의하고 분류한다.  
이미지 생성 프로세스의 control 가능성이 많은 CG 애플리케이션에 필수적이기 때문에 CG와 학습 기반 방식을 결합하여  
control 가능한 이미지 생성을 위한 새롭고 강력한 알고리즘을 생성하는 방법에 초점을 맞춘다.  
이 보고서를 구성하는 한 가지 중심 계획은 각 접근법에 의해 제공되는 controllability이다. 
우리는 NR의 전제 조건인 CG, CV 및 ML의 기본 개념에 대해 논의하는 것으로 시작한다. 

이후, control 유형, control 제공 방법, 파이프라인의 학습 부분, 명시적 control 대 암묵적 control, 일반화 및 확률적 대 결정론적 합성과 같은 
NR 접근법의 중요한 측면에 대해 논의한다.  
다음으로, NR에 의해 활성화되는 애플리케이션의 환경에 대해 논의한다.  
NR의 응용은 새로운 시각 합성, 의미론적 사진 조작, 얼굴과 몸의 재연, 자유 시각 비디오, 가상 및 증강 현실 텔레프레전스를 위한 
사진 사실 아바타의 생성에 이르기까지 다양하다.  
실제 사진과 구별할 수 없는 이미지의 생성과 조작이 사람을 가지고 있기 때문이다.  
특히 인간이 사진에 찍힐 때, 우리는 또한 이러한 의미와 합성 콘텐츠의 탐지 가능성에 대해 논의한다.  
NR 분야는 여전히 빠르게 발전하고 있기 때문에, 우리는 현재 열린 연구 문제로 결론을 내린다.  

## 2. Related Surveys and Course Notes 

심층 생성 모델은 문헌에서 널리 연구되어 왔으며, 이를 설명하는 여러 survey와 강의 노트가 있다.  
여러 보고서는 GAN 및 VAE와 같은 특정 생성 모델에 초점을 맞추고 있다.  
고전 CG와 CV 기술을 사용한 control 가능한 이미지 합성도 광범위하게 연구되었다.  
이미지 기반 렌더링은 여러 조사 보고서에서 논의되었다.  
Szeliski의 책은 3D 재구성 및 이미지 기반 렌더링 기술에 대한 훌륭한 소개를 제공한다.  
최근 조사 보고서는 3D 재구성에 대한 접근법과 다양한 애플리케이션의 control 가능한 얼굴 렌더링을 논의한다.  
NR의 일부 측면은 최근 CV 컨퍼런스의 튜토리얼과 워크샵에서 다루어졌다.  
여기에는 자유 시점 렌더링 및 전신 성능의 reconstruction을 위한 접근법, 얼굴 합성을 위한 NR 튜토리얼  
및 신경 네트워크를 이용한 3D 장면 생성 등이 포함된다.  
그러나 위의 조사와 과정 중 NR과 NR의 모든 다양한 응용을 체계적이고 포괄적으로 살펴볼 수 있는 것은 없다.  

## 3. Scope of this STAR
이 최신 보고서에서는 기존의 CG 파이프라인과 학습 가능한 구성 요소를 결합한 새로운 접근 방식에 중점을 둔다. 
구체적으로, ML을 통해 고전적인 렌더링 파이프라인을 개선할 수 있는 위치와 방법과 교육에 필요한 데이터에 대해 논의하고 있다. 
포괄적인 개요를 제공하기 위해 두 분야, 즉 CG와 ML의 관련 기초에 대해서도 간략하게 소개한다. 
현재 하이브리드의 이점뿐만 아니라 한계도 보여진다. 이 보고서는 또한 이러한 기법에 의해 강화되는 새로운 응용 프로그램에 대해서도 논의한다. 
우리는 ML을 통해 control 가능한 사실적 사진 이미지를 생성하는 것을 주요 목표로 하는 기술에 초점을 맞춘다. 
우리는 3D 재구성 및 장면 이해에 더 초점을 맞춘 기하학적 및 3D 딥 러닝에 대한 작업은 다루지 않는다. 
이 작업은 많은 NR 접근 방식, 특히 3D 구조 장면 표현을 기반으로 하지만 이 조사의 범위를 벗어난다. 
우리는 또한 ray 추적 이미지를 제거하기 위해 ML을 사용하는 기술에 초점을 맞추지 않는다.

## 4. Theoretical Fundamentals
다음에서는 NR 공간에서 작업의 이론적 기초에 대해 논의한다.  
먼저 CGI 형성 모델에 대해 논의한 다음 고전적인 이미지 합성 방법에 대해 논의한다.  
다음으로, 우리는 딥 러닝에서 생성 모델에 대한 접근법에 대해 논의한다.

### 4.1. Physical Image Formation
전통적인 CG 방법은 실제 세계에서 이미지 형성의 물리적 과정에 가깝다.  
광원은 카메라에 기록되기 전에 형상과 재료 특성의 함수로 장면의 물체와 상호 작용하는 광자를 방출한다.  
이 과정은 light transport로 알려져 있다.  
Camera optics는 조리개로부터 들어오는 빛을 획득하여 카메라 본체 내부의 센서나 필름 평면에 초점을 맞춘다.  
센서 또는 필름은 해당 평면의 입사광의 양을 비선형 방식으로 기록한다.  
광원, 재료 특성 및 카메라 센서 등 이미지 형성의 모든 구성 요소는 파장에 따라 달라진다. 
실제 필름과 센서는 종종 인간 시각 시스템의 민감도에 맞춰 1~3개의 다른 파장 분포만을 기록한다.  
이 물리적 이미지 형성의 모든 단계(광원, 장면 형상, 재료 특성, 광전송, 광학 및 센서 동작)는 CG로 모델링된다.

### 4.1.1. Scene Representations
Scene object를 모델링하기 위해 scene geometry에 대한 다양한 표현이 제안되었다.  
그것들은 explicit/implicit 표현으로 분류될 수 있다.  
Explicit : 장면을 삼각형, 점 같은 원시 요소 또는 고차 파라메트릭 표면과 같은 기하학적 원시 요소의 모음으로 설명한다.  
Implicit : 표면이 함수(또는 다른 수준 집합)의 영점수로 정의되는 R3 → R로부터의 부호 거리 함수 매핑을 포함한다.  
실제로 대부분의 하드웨어와 소프트웨어 renderer는 삼각형 메시에서 가장 잘 작동하도록 조정되어 있으며  
렌더링을 위해 다른 표현을 삼각형으로 변환한다.
  
빛과 장면 표면과의 상호 작용은 표면의 재료 특성에 따라 다르다.  

재료는 아래 두가지로 표현될 수 있다.  
1) BRDF(Bidirectional Reflectance Distribution Functions;양방향 반사율 분포 함수)  
- 들어오는 각 광선 방향의 표면 지점에서 발생하는 주어진 파장의 빛이 나가는 각 광선 방향을 향해 반사되는 정도를 설명하는 5차원 함수  
- 분석 모델 또는 측정된 데이터를 사용하여 나타낼 수 있다.  
- BRDF가 표면에 걸쳐 변화할 때 이를 spatially varing BRDF(svBRDF)라고 한다.  
2) BSSRDF(Bidirectional Subsurface Scattering Reflectance Distribution Functions;양방향 지하 산란 반사율 분포 함수)  
- BRDF는 단일 표면 지점에서 발생하는 빛 상호 작용만 모델링하는 반면,  
- BSSDRF는 한 표면 지점에서 발생하는 빛이 다른 표면 지점에서 반사되는 방법을 모델링하여 7-D 함수를 만든다.  

![image](https://user-images.githubusercontent.com/40943064/144844016-498e9cbe-4d52-438d-8025-3ca1234b6511.png)  

기하학 전반에 걸쳐 공간적으로 변화하는 동작은 이산 재료를 다른 기하학적 원시 요소에 결합하거나 텍스처 매핑을 사용하여 나타낼 수 있다.  
텍스처 맵은 2차원 또는 3차원 도메인에서 표면으로 확산 알베도와 같은 재료 매개 변수의 연속적 값 세트를 정의한다.  
3차원 텍스처는 공간의 경계 영역을 통해 값을 나타내며 명시적이거나 암시적인 지오메트리에 적용될 수 있다. 
2차원 도메인에서 2차원 텍스처 맵 파라메트릭 지표면. 따라서 일반적으로 명시적 지오메트리에만 적용된다.

장면의 광원은 파라메트릭 모델로 나타낼 수 있으며, 빛을 방출하는 장면 표면으로 표현되는 점, 방향 조명, 영역 광원이 포함된다.  
어떤 방법은 텍스처 맵이나 함수에 의해 정의된 표면에 걸쳐 지속적으로 변화하는 방출을 설명한다.  
종종 환경 지도는 조밀하고 먼 장면 조명을 나타내기 위해 사용된다.  
이러한 환경 맵은 구나 큐브에 비모수 텍스처로 저장하거나 구면 조화 기반 계수로 근사할 수 있다.  
장면의 매개 변수는 시간이 지남에 따라 변화하는 것으로 모델링될 수 있으며,  
이를 통해 연속 프레임에 걸친 애니메이션과 단일 프레임 내에서 모션 블러 시뮬레이션을 모두 수행할 수 있다.  

### 4.1.2. Camera Models
컴퓨터 그래픽에서 가장 흔한 카메라 모델은 핀홀 카메라 모델로, 광선이 핀홀을 통과해 필름 평면(영상 평면)에 부딪힌다. 이러한 카메라는 핀홀의 3D 위치, 이미지 평면 및 센서나 필름의 공간 범위를 나타내는 평면 내의 직사각형 영역에 의해 파라미터화될 수 있다. 이러한 카메라의 작동은 균일한 좌표를 사용하여 3D 기하학적 표현을 이미지 평면의 2차원 영역으로 변환하는 투영 기하학을 사용하여 압축적으로 표현될 수 있다. 이것은 완전 투시 투영 모델이라고도 알려져 있다. 약한 투시 투영과 같은 이 모델의 근사치는 전체 투시 투영법의 비선형성으로 인해 복잡성을 줄이기 위해 컴퓨터 비전에 종종 사용된다. 컴퓨터 그래픽의 보다 정확한 투영 모델은 왜곡, 이상, 비그네팅, 디포커스 블러, 심지어 렌즈 요소들 사이의 상호반사를 포함한 비이상적 렌즈의 효과를 고려한다.

### 4.1.3. Classical Rendering
Cameras, lights, surface geometry, material을 포함한 장면 정의를 시뮬레이션된 카메라 이미지로 변환하는 과정을 렌더링이라고 한다.  
렌더링에 대한 가장 일반적인 두 가지 접근법은 **rasterization**과 **raytracing**이다.  

![image](https://user-images.githubusercontent.com/40943064/145209104-d37cef08-d5f7-4d2b-9f53-1e746f6d6d2d.png)  

Rasterization은 geomtry가 이미지 영역으로 변환되는 피드포워드 과정으로, 때로는 페인터의 알고리즘으로 알려진 앞뒤 순서로 변환된다.  
Raytracing은 이미지 픽셀에서 가상 장면으로 광선을 거꾸로 투사하고 geometry의 교차점에서 새로운 광선을 재귀적으로 투사해 반사 및 굴절을 시뮬레이션하는 공정이다.  
하드웨어 가속 렌더링은 메모리 일관성이 좋기 때문에 일반적으로 rasterization에 의존한다.  
그러나 global illumination 및 기타 형태의 복잡한 light transport, depth of field, motion blur 등과 같은 많은 실제 이미지 효과는  
raytracing을 사용하여 더 쉽게 시뮬레이션되며, 최근 GPU는 이제 실시간 그래픽 파이프라인(예: NVIDIA RTX 또는 DirectX Raytracking)에서  
raytracing을 사용할 수 있도록 가속 구조를 갖추고 있다.  
Rasterization은 explicit geometric representation을 필요로 하지만, raytracing/raycasting은 implicit representation에도 적용될 수 있다.  
실제로 implicit representation은 marching cubes algorithm 및 기타 유사한 방법을 사용하여 rasterization를 위해 명시적 형태로 변환할 수도 있다.  
렌더러는 rasterization과 raycasting의 조합을 사용하여 고효율과 물리적 사실감을 동시에 얻을 수 있다(예: 스크린 공간 raytracing).  
주어진 렌더링 파이프라인에서 생성되는 이미지의 품질은 파이프라인에 있는 다양한 모델의 정확도에 크게 좌우된다.  
구성요소는 샘플링 및 신호 재구성 이론의 신중한 적용을 사용하여 픽셀 중심 사이의 간격과 같은 컴퓨터 시뮬레이션의 이산적 특성을 설명해야 한다.  
새로운 뷰를 생성하거나 재료나 조명을 편집하거나 새로운 애니메이션을 만들기 위해 실제 데이터로부터  
다른 모델 매개변수(카메라, 기하학, 재료, 광 파라미터)를 추정하는 과정을 _inverse rendering_ 이라고 한다.  
컴퓨터 비전과 컴퓨터 그래픽의 맥락에서 탐구된 inverse rendering은 NR과 밀접한 관련이 있다.  
Inverse rendering의 단점은 수학적 복잡성과 계산 비용 때문에 고전적인 렌더링에 사용되는 미리 정의된 물리적 모델이나  
데이터 구조가 실제 물리적 프로세스의 모든 특징을 정확하게 재현하지 못한다는 것이다.  
대조적으로, NR은 그러한 모델 대신 학습된 구성 요소를 렌더링 파이프라인에 도입한다.  
심층 신경망은 통계적으로 그러한 물리적 프로세스를 근사화하여, inverse rendering보다 실제 효과를 더 정확하게 재생산하여  
학습 데이터와 더 밀접하게 일치하는 출력을 생성할 수 있다.  

역 렌더링과 신경 렌더링이 교차하는 지점에는 접근법이 있다. Li는 글로벌 조명 효과를 근사화한 NR을 사용하여  
깊이, 정상, 알베도, 거칠기 맵을 예측하는 역 렌더링 방법을 효율적으로 학습한다.  
또한 신경망을 사용하여 셰이더와 같은 고전적 렌더링 파이프라인의 특정 구성 요소를 향상시키는 접근 방식도 있다.  
레이너는 Bidirectional Texture Function을, 맥시모프는 Appearance Maps을 학습한다.  

### 4.1.4. Light Transport
Light transport는 빛을 방출하는 광원으로부터, scene을 통해, 그리고 camera로 보내는 가능한 모든 경로를 고려한다.  
이 문제의 잘 알려진 공식은 고전적 렌더링 방정식이다.  
![image](https://user-images.githubusercontent.com/40943064/144739258-d3599714-c0b6-44af-bca6-7c19d9c09ea9.png)  
Lo : 위치, 광선 방향, 파장, 시간의 함수로써 표면으로부터의 나가는 광도  
Le : 용어는 직접 표면 방출  
Lr : 입사광과 표면 반사율의 상호작용  
![image](https://user-images.githubusercontent.com/40943064/144739268-07f6206a-7a7a-4e76-bacf-6e198ae48603.png)  

이 공식은 투명한 물체와 지표면 아래 또는 부피 산란의 영향을 고려하지 않는다. 렌더링 방정식은 적분 방정식이며,  
오른쪽에 나타나는 입사 광도 Li가 같은 광선의 다른 표면에서 나오는 송신 광도 Lo와 같기 때문에 비사소적인 장면에서는 닫힌 형태로 풀 수 없다.  
따라서, 방대한 수의 근사치가 개발되었다. 가장 정확한 근사치는 몬테카를로 시뮬레이션으로 장면을 통과하는 광선 경로를 샘플링한다.  
더 빠른 근사는 우측을 한두 번 확장한 다음 재발을 차단하여 소수의 "발광"만 시뮬레이션할 수 있다.  
컴퓨터 그래픽 아티스트들은 또한 비물리적 기반 광원을 장면에 추가함으로써 추가적인 바운스를 시뮬레이션 할 수 있다.

### 4.1.5. Image-based Rendering
3D 컨텐츠를 2D 평면에 투영하는 기존의 렌더링과 달리, 이미지 기반 렌더링 기법은 이미지 세트를 일반적으로 뒤틀고 함께 합성하여  
새로운 이미지를 생성한다. 이미지 기반 렌더링은 Thies에서 볼 수 있듯이 애니메이션을 처리할 수 있지만,  
가장 일반적인 사용 사례는 캡처된 뷰의 이미지 콘텐츠를 프록시 지오메트리와 추정된 카메라 포즈를 기반으로 하여  
새로운 뷰로 뒤틀리는 정적 객체의 새로운 뷰 합성이다.  
완전한 새 이미지를 생성하려면 여러 캡처된 뷰를 대상 뷰로 뒤틀어야 하므로 혼합 단계가 필요하다.  
일부 재료는 뷰 포인트에서 모양을 크게 바꾸기 때문에 결과 이미지 품질은 지오메트리의 품질, 입력 뷰의 수 및 배열,  
장면의 재료 특성에 따라 달라진다.  
혼합 및 뷰 의존적 효과의 보정을 위한 휴리스틱 방법이 좋은 결과를 보여주지만, 최근 연구는 이러한 이미지 기반 렌더링 파이프라인의  
일부를 학습된 구성 요소로 대체했다.  
심층 신경망은 혼합 아티팩트와 뷰 의존적 효과에서 비롯된 아티팩트를 모두 줄이기 위해 성공적으로 사용되었다.

### 4.2. Deep Generative Models
기존의 CG는 이미지를 생성하기 위해 장면을 물리적으로 모델링하고 light transport를 시뮬레이션하는 데 중점을 두는 반면,  
ML을 사용하여 실제 이미지의 분포를 학습하여 통계적 관점에서 이 문제를 해결할 수 있다.  
역사적으로 작은 이미지 세트(예: 수백 개)를 사용한 기존의 이미지 기반 렌더링과 비교하여  
deep generative model은 대규모 이미지 컬렉션에서 이미지 prior를 학습할 수 있다.  
  
초기 심층 생성 모델 연구는 간단한 숫자와 정면면의 무작위 샘플을 생성하는 것을 학습 할 수 있었으나  
품질과 해상도는 물리적 기반 렌더링 기술을 사용하여 달성할 수 있는 것과는 거리가 멀었다.  
그러나 최근에는 GAN과 그 확장을 사용하여 photo-realistic image 합성이 입증되었다.  
최근 연구는 종종 실제 얼굴과 구별할 수 없는 무작위 고해상도 초상화를 합성할 수 있다.  
  
심층 생성 모델은 학습 세트와 유사한 통계로 랜덤 사실적 이미지를 생성하는 데 탁월하다.  
그러나 **사용자 control**와 **상호 작용**은 이미지 합성 및 조작에서 중요한 역할을 한다.  
예를 들어, 콘셉트 아티스트는 무작위 장면보다는 자신의 디자인 아이디어를 반영하는 특정 장면을 만들고 싶어한다.  
따라서 CG 애플리케이션의 경우 영상 합성 프로세스를 explicit 하게 control할 수 있도록 생성 모델을 조건부 설정으로 확장해야 한다.  
초기 작업은 픽셀당 'lp 거리'로 피드 포워드 신경망을 훈련시켜 주어진 조건부 입력을 생성했다.  
그러나 픽셀 공간의 lp 거리는 각 픽셀을 독립적으로 고려하고 시각적 구조의 복잡성을 무시하기 때문에 생성된 결과가 흐릿한 경우가 많다.  
게다가, 가능한 여러 결과물의 평균을 내는 경향이 있다.  
문제를 해결하기 위해, 최근 연구는 사전 학습된 네트워크에 의해 구성된 높은 수준의 심층 feature embedding 공간에서  
합성 결과와 실측 결과 사이의 불일치를 측정하기 위한 perceptual similarity distance를 제안한다.  
응용 프로그램에는 artistic stylization, image generation/synthesis, super-resolution 등이 포함된다.  
출력을 실제와 일치시킨다고 해서 출력이 자연스러워 보이지는 않는다.  
cGAN은 출력과 대상 사이의 거리를 최소화하는 대신 주어진 입력의 출력 조건부 분포를 일치시키는 것을 목표로 한다.  
그 결과는 실제 이미지와 같지 않을 수도 있지만, 결과물은 자연스러워 보인다.  
cGAN은 거친 컴퓨터 그래픽 렌더링과 해당 실제 이미지 사이의 간격을 메우거나  
사용자가 지정한 의미 레이아웃이 주어진 사실적인 이미지를 생성하기 위해 사용되었다.  
아래에서는 네트워크 아키텍처와 학습 목표 모두에 대한 더 많은 기술적 세부 사항을 제공한다.

### 4.2.1. Learning a Generator

조건부 입력 x ∈ X를 출력 y∈Y에 매핑할 수 있는 G를 학습하는 것을 목표로 한다. (X:입력도메인, Y:출력도메인)  
(User-provided sketch image, camera parameters, lighting conditions, scene attributes, textual descriptions)  
y는 영상, 비디오에서 voxel 또는 mesh와 같은 3D 데이터에 이르기까지 다양할 수 있다.  
(각 애플리케이션에 대해 가능한 네트워크 입력 및 출력의 전체 목록은 표 1을 참조)  
여기서는 일반적으로 사용되는 세 가지 G의 구조를 설명합니다.(S6의 app.별 세부 사항을 확인하도록 권장된다.)  
1) **FCN**  
FCN은 임의 크기의 입력 이미지를 촬영하고 동일한 크기로 출력을 예측한다.  
이미지를 벡터로 매핑하는 AlexNet 및 VGG와 같은 대중적인 이미지 분류 네트워크에 비해 FCN은 공간 이미지 해상도를 보존하기 위해  
fractionally-strided convolutions을 사용한다.  
FCN은 원래 semantic segmentation 및 object detection과 같은 recognition을 위해 설계되었지만 이미지 합성 작업에 널리 사용되었다.  
(2) **UNet**  
Unet은 localization이 향상된 FCN 기반 구조이다.  
초기 레이어의 고해상도 feature map에서 뒤 레이어의 업샘플링 feature로 이른바 "skip-connection"을 추가한다.  
입력에서 나오는 고주파 정보는 출력으로 직접 전달될 수 있기 때문에 이러한 skip-connection은 상세한 출력을 생성하는 데 도움이 된다.  
(3) ResNet  
ResNet기반 G는 residual 블록을 사용하여 고주파 정보를 출력으로 전달하며 style transfer 및 이미지 super-resolution에 사용되었다.  

### 4.2.2. Learning using Perceptual Distance
많은 입출력 pair를 수집하고 G를 선택하면, 입력이 주어졌을 때 원하는 출력을 생산하는 G를 어떻게 학습시킬 수 있을까?  
이 학습 문제에 효과적인 객관적 기능은 무엇인가? 한 가지 간단한 방법은 다음과 같이 G(x)와 y(GT) 사이의 거리를 최소화하는 것이다.  
![image](https://user-images.githubusercontent.com/40943064/145590791-676fa40c-771f-4858-9443-1244b6f68928.png)
안타깝게도 학습된 G는 흐릿한 이미지 또는 여러 그럴듯한 출력에 대한 평균 결과를 생성하는 경향이 있다.  
예를 들어, image colorization에서 학습된 G는 평균화 효과로 인해 때때로 desaturated 결과를 생성한다.  
이미지  super-resolution에서는 p-norm이 각 픽셀을 독립적으로 보기 때문에 생성기가 구조와 세부 정보를 합성하지 못한다.  

이미지 유사성에 대한 인간의 인식과 더 잘 일치하는 학습 목표를 설계하기 위해, 최근 연구는 사전 학습된  
이미지 classifier F(예: VGG)에 의해 추출된 deep feature representation 사이의 거리 측정을 제안한다.  
이러한 loss는 'p-norm'이 각 픽셀의 품질을 독립적으로 평가하는 반면, deep feature representation은  
전체 이미지를 전체적으로 요약하기 때문에 'pnorm'에 비해 유리하다.  
G는 다음과 같은 feature matching 목표를 최소화하도록 학습된다.  
![image](https://user-images.githubusercontent.com/40943064/145592173-2aabccb5-6d79-430c-9d15-ece1f645ef52.png)
F(t) :T개 레이어를 가진 사전 학습된 F의 t번째 레이어에서 feature extracter  
Nt : layer t의 총 feature 수  
Δt : 각 layer의 weight  
위의 거리는 종종 "perceptual disctance"라는 신조어로 만들어지지만, 네트워크가 원래 이미지 합성 작업보다는  
이미지 분류 작업을 위해 훈련되었기 때문에 다단계 deep feature space에서 일치하는 통계가 인간의 인식과 일치하고  
고품질 결과 합성에 도움이 될 수 있는 이유는 흥미롭다.  
최근의 한 연구는 강력한 분류자가 학습한 풍부한 특징이 기존의 수작업 지각 메트릭스를 능가하는  
인간의 지각 작업에 유용한 표현을 제공한다는 것을 시사한다.  

### 4.2.3. Learning with Conditional GANs
그러나 출력과 실측값 사이의 거리를 최소화한다고 해서 실제처럼 보이는 출력이 보장되지는 않는다.  
그들은 또한 small distance and photorealism이 서로 상충한다는 것을 증명한다.  
따라서 거리 최소화 대신 deep generative 모델은 distribution matching, 즉 생성된 결과의 분포를 학습 데이터의 분포에 일치시키는 데 초점을 맞춘다.  
많은 유형의 생성 모델 중에서 GAN은 많은 CV 작업에 대해 유망한 결과를 보여주었다.  
Goodfellow의 원본 작업에서 GAN 생성기 G : z → y는 저차원 무작위 벡터 z에서 출력 이미지 y로의 매핑을 학습한다.  
일반적으로 입력 벡터는 다변량 가우스 또는 균등 분포에서 샘플링한다.  
G는 적대적으로 훈련된 D에 의해 "실제"와 구별할 수 없는 출력을 생성하도록 학습된다.  
D는 G에 의해 생성된 합성 이미지를 감지하도록 훈련된다.  
얼굴이나 차량과 같은 객체 범주에 대해 훈련된 GAN은 객체의 고품질 인스턴스 합성을 배우지만, 일반적으로 합성된 배경은 품질이 낮다.  
최근 논문은 전체 scene의 생성 모델을 학습하여 이 문제를 완화하려고 한다.  
조건부 정보를 입력으로 추가하기 위해, cGAN은 관찰된 입력 x와 출력 이미지 y에 무작위로 샘플링된 벡터 z로부터 매핑 G : {x,z} → y를 학습한다.  
관찰된 입력 x는 또한 D로 전달되어 이미지 pair {x, y}의 real/fake 여부를 모델링한다.  
cGAN에서 입력 x는 모델이 생성해야 하는 객체 범주를 control하는 categorical label이다.  
pix2pix와 같은 이미지 조건부 GAN의 경우, G는 semantic label map과 같은 입력 x를 사실적으로 보이는  
출력 이미지로 변환하는 것을 목표로 하는 반면 D는 real/fake를 구별하는 것을 목표로 한다.  
모델은 해당 입력 xi와 출력 이미지 yi로 구성된 쌍 데이터 세트 {xi , yi} N i=1로 학습된다.  
cGAN은 다음 미니맥스 게임을 통해 입력이 주어진 출력의 조건부 분포와 일치한다.
![image](https://user-images.githubusercontent.com/40943064/145593823-5b721158-b603-439a-aa52-ff02338116f8.png)  
일반적으로 목적함수는 아래와 같이 정의된다.  
![image](https://user-images.githubusercontent.com/40943064/145593852-d0ed3476-4d15-4bad-b6df-2c43c7cf32de.png)  

초기 cGAN 구현에서는 노이즈 벡터가 주입되지 않으며, 학습 중에 네트워크에 의해 무시되는 경향이 있기 때문에 매핑이 결정적이다.  
보다 최근의 연구는 multi-modal image synthesis를 가능하게 하기 위해 latent vector z를 사용한다.  
학습을 안정화시키기 위해, cGANs 기반 방법은 픽셀당 'l1 loss Lrecon(G) (Equation (1)) 및 pereceptual distance loss Lperc(G) (Equation (2))도 채택한다.  
학습 중에 D는 real/fake를 구분하는 능력을 향상시키려고 하는 반면 G는 D를 속이는 능력을 향상시키려고 한다.  
pix2pix 방법은 G의 아키텍처로 U-Net을 채택하고 D로 patch 기반 FCN을 사용한다.  

개념적으로 perceptual distance와 cGAN은 둘 다 더 나은 G를 학습하기 위한 효과적인 학습 목표를 정의하기 위해  
보조 네트워크(F 또는 D)를 사용하기 때문에 관련이 있다.  
High-level 추상화에서 G(x)의 품질을 평가하기 위한 정확한 CV 모델(F 또는 D)은 neural rendering 문제를 해결하는 데 크게 도움이 될 수 있다.  
그러나 두 가지 중요한 차이가 있다.  
첫째, perceptual distance는 fake와 real 사이의 불일치를 측정하는 것을 목표로 하는 반면,  
cGAN은 real/fake의 조건부 분포의 근접성을 측정한다.  
둘째, perceptual distance의 경우 feature extractor F는 사전 학습되고 고정되는 반면,  
cGAN은 G에 따라 D를 즉시 학습한다.  
실제로, 두 가지 방법은 상호 보완적이며, 많은 neural rendering 애플리케이션은 두 손실을 동시에 사용한다.  
GAN 외에도 최근 VAE, auto regressive NN(예: PixelCNN, PixelRNN), invertible density 모델 등 많은 유망한 연구 방향이 등장했다.  
StarGAN은 서로 다른 도메인을 가진 여러 데이터 세트를 기반으로 i2i 변환을 위한 단일 모델을 학습할 수 있다.  
논의를 간결하게 유지하기 위해 여기서는 GAN에 초점을 맞춘다.  
우리는 독자들이 deep generative model의 완전한 그림을 위해 튜토리얼과 코스 노트를 검토할 것을 촉구한다.  
  
  
### 4.2.4. Learning without Paired Dat
위의 목표를 가진 G를 학습하려면 수백에서 수백만 개의 pair 학습 데이터가 필요하다.  
많은 실제 애플리케이션에서 pair 학습 데이터는 수집이 어렵고 비용이 많이 든다.  
분류 작업을 위해 이미지에 레이블을 지정하는 것과 달리  annotator는 이미지 합성 작업을 위해  
모든 픽셀에 레이블을 지정해야 한다.  
예를 들어, semantic segmentation과 같은 작업을 위한 몇 개의 작은 데이터 세트만 존재한다.  
Artistic stylization과 같은 그래픽 작업을 위한 입력-출력 쌍을 얻는 것은 종종 예술적 저작이 필요하고  
때로는 잘 정의되지도 않기 때문에 훨씬 더 어려울 수 있다.  
이 설정에서 모델에는 source domain {xi} N i=1(xi x X)과 target domain {y j}j=1(y j y Y)이 지정된다.  
우리가 아는 것은 출력 G(x)가 어느 대상 도메인에서 와야 하는지이다. 즉, 도메인 Y의 이미지와 같다.  
그러나 특정 입력이 주어지면 출력이 어떤 대상 이미지여야 하는지 알 수 없다.  
X에서 Y까지의 이미지를 projection하기 위해 무한히 많은 mapping이 있을 수 있다.  
따라서 우리는 추가적인 제약이 필요하다.  
Bijective mapping 적용을 위한 cyclic consistency loss, pixel space 또는 feature embeeding 공간에서  
출력이 입력 이미지에 근접하도록 장려하기 위한 distance preserving loss,  
도메인 간 공유 표현을 학습하기 위한 weight 공유 전략 등을 포함한 여러 제약 조건이 제안되었다.  
위의 방법은 조건부 GAN의 적용 범위를 넓히고 객체 변환, 도메인 전송 및 CG2real과 같은 많은 그래픽 애플리케이션을 가능하게 한다.  


## 5. Neural Rendering

고품질의 장면 사양을 고려할 때, 고전 렌더링 방법은 다양한 복잡한 실제 현상에 대해 사실적인 이미지를 렌더링할 수 있다.  
또한 렌더링을 통해 장면요소(카메라 시점, 조명, 기하학 및 재료)를 명시적으로 편집할 수 있다.  
그러나 특히 이미지에서 직접 고품질 장면 모델을 구축하는 데는 상당한 수작업이 필요하며  
이미지에서 자동화된 장면 모델링은 해결과제이다.  
반면 심층 생성 네트워크는 이제 무작위 노이즈 또는 장면 분할 및 레이아웃과 같은 특정 사용자 사양에 따라  
조정된 시각적으로 매력적인 이미지와 비디오를 생성하기 시작했다.  
장면 외관에 대한 fine control은 어려우며 장면 속성 간의 복잡한 non-local 3D interation을 항상 처리할 수는 없다.  
NR은 입력 이미지/비디오에서 새로운 이미지를 control 가능한 고품질 합성을 가능하게 하기 위해 이러한 접근 방식을 결합할 가능성을 가진다.  
NR 기술은 다양하며 장면 외관, 필요한 입력, 생성하는 출력 및 사용하는 네트워크 구조에 따라 다르다.  
일반적인 NR 접근법은 특정 장면 조건(예: 시점, 조명, 레이아웃 등)에 해당하는 입력 영상으로 사용하고,  
그것으로부터 "neural" 장면 표현을 구축하고, 새로운 장면 속성으로 이 표현을 "렌더링"하여 새로운 영상을 합성한다.  
학습된 장면 표현은 단순한 장면 모델링 근사치에 의해 제한되지 않으며 고품질의 새로운 이미지에 최적화될 수 있다.  
동시에, NR 접근법은 입력 기능, 장면 표현 및 네트워크 아키텍처의 형태로 고전적인 그래픽의 아이디어를 통합하여  
학습 작업을 더 쉽게 하고 출력을 더 쉽게 control할 수 있도록 한다.  

중요하게 생각하는 축을 따라 신경 렌더링 접근법의 분류법을 제안한다.  
  
• Control : Control하고자 하는 것은 무엇이며 어떻게 조절하는가?  
• CG Modules : 어떤 CG 모듈이 사용되며 NR 파이프라인에 어떻게 통합되는가?  
• Explicit or Implicit Control : 매개변수에 대한 explicit 통제를 제공하는가, 아니면 출력으로 얻을 것으로 예상되는 것의 예를 보여줌으로써 implicit으로 수행되는가?  
• Multi-modal Synthesis : 특정 입력이 주어졌을 때 여러 선택적 출력을 출력하도록 학습되었는가?  
• Generality: 렌더링 접근 방식이 여러 장면/객체에 걸쳐 일반화되었는가?  
  
다음에서는 현재의 SOTA 방법을 분류하는 데 사용하는 이러한 축에 대해 논의한다(표 1 참조).  
  
![image](https://user-images.githubusercontent.com/40943064/145702883-d1103a34-f839-4e9d-8b5d-f1be35e57a31.png)  
  
![image](https://user-images.githubusercontent.com/40943064/145702900-f1cc7e7e-2aea-42d5-b493-e7177f3290b5.png)  

### 5.1. Control
NR은 사용자가 지정한 장면 조건에서 고품질 이미지를 렌더링하는 것을 목표로 한다.  
현재의 방법은 novel view synthesis, relighting under novel lighting, animating faces/bodies(under novel expressions and poses)이 있다.  
새로운 표현과 자세에서 얼굴과 몸을 애니메이션화하는 것과 같은 특정 하위 문제를 다룬다.  
이러한 접근 방식이 서로 다른 주요 축은 제어 신호가 네트워크에 제공되는 방식에 있다.  
한 가지 전략은 장면 매개 변수를 첫 번째 또는 중간 네트워크 계층에 입력으로 직접 전달하는 것이다.  
관련 전략은 입력 이미지의 모든 픽셀에 걸쳐 장면 매개 변수를 타일링하거나 내부 네트워크 계층의 활성화에 연결하는 것이다.  
또 다른 접근법은 영상의 공간 구조에 의존하고 i2i 변환 네트워크를 사용하여  
"guide image" 또는 "conditioning image"에서 출력으로 매핑하는 것이다.  
예를 들어, 그러한 접근법은 semantic mask에서 출력 이미지에 매핑하는 방법을 배울 수 있다.  
다음에서 설명하는 또 다른 옵션은 제어 매개 변수를 그래픽 레이어의 입력으로 사용하는 것이다.  
