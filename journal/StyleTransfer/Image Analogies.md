# Image Analogies

## Abstract
'Image analogies'라 불리는 이미지 처리 프레임워크를 설명한다.  

두가지 stage가 있다.  
1) Design Phase : 한 이미지가 다른 이미지의 "필터링된" 버전이 되도록하는 한쌍의 이미지 (학습데이터로 표현)  
2) Application Phase : 학습된 필터가 일부 새로운 대상 이미지에 적용되어 "필터링된" 결과 생성  

Image Analogies는 최근 텍스쳐 합성의 결과에 우선적으로 영감을 받은 단순 다중스케일 자기회귀를 기반으로 한다.  
여러 타입의 source 쌍들을 입력으로 선택해서, 프레임웍은 광범위한 'image filter' 효과를 지원한다.  
1) Traditional image filters : 블러링 / 엠보싱  
2) Improved texture synthesis : 과거 방법론보다 더 향상된 품질의 질감합성  
3) Duper-resolution : 저해상도 source로부터 고해상도 이미지 추론  
4) Texture transfer : 이미지에 대한 임의의 source 질감 적용  
5) Artistic filters : 실세계 사례를 기반하여 합성된 다양한 drawing/painting 스타일  
6) Texture-by-numbers : 단순한 페인팅 인터페이스를 사용하여 다양한 질감으로 구성된 사실적 장면 생성  

## 1 Introduction
**a'nal'o'gy** n.  
_대상 구조의 개체 간의 관계와 속성을 추론하기 위해 소스 구조의 개체 간의 관계와 속성을 사용하는 구조 간의 체계적인 비교  
유추를 인식하는 타고난 재능은... 천부적 천재성의 으뜸가는 사실._  
**William James, 1890**
Analogy는 기본 추론 과정으로 문제를 풀거나 예시를 제공하거나 예측을 할때 꽤 일반적으로 사람으로써 사용하는 것이다.  
본 논문에서, 우리는 analogy를 복잡한 이미지 필더들을 생성하기 위한 수단으로써 탐험한다.  
![image](https://user-images.githubusercontent.com/40943064/146486372-60c1c561-9547-4c58-aad4-c0b2f7faa1cb.png)
특히, 다음의 문제 해결을 시도한다. :  
  
Problem (“IMAGE ANALOGIES”):  
이미지 A와 A' 쌍이(각각 original/filtered source image)이 주어지면 필터링되지 않은 일부 추가 대상 이미지 B와 함께  
다음과 같이 새로운 필터링된 대상 이미지 B'를 합성한다.  
  
A : A' :: B : B'  
  
즉, A가 A'와 관련된 "같은 방식"으로 B와 관련된 "유사한" 이미지 B'를 찾는다.  
일반적으로 이것은 해결하기 매우 어려운 문제이다.  
이 논문에서 많은 경우에 잘 작동하는 접근 방식을 설명한다.  
Image analogies의 장점은 이미지 변환을 지정하는 매우 자연스러운 수단을 제공한다는 것이다.  
무수히 많은 다양한 필터와 해당 설정 중에서 선택하는 대신 사용자는 적절한 예시(필터링되지 않은 해당 소스 이미지와 함께)를 제공하고  
사실상 "이렇게 보이게 하세요"라고 말할 수 있다.  
이상적으로는 이미지 유추를 통해 매우 복잡하고 비선형적인 이미지 필터를 배울 수 있어야 한다.  
이러한 스타일의 실질적(실생활의) 렌더링과 유사하다.  
또한 이러한 다양한 유형의 필터를 개별적으로 발명하거나 명시적으로 프로그래밍할 필요가 없다.  
이상적으로는 이와 같이 매우 다양한 효과를 제공하기 위해 동일한 일반 메커니즘을 대신 사용할 수 있다.  
이미지 유추는 분명히 바람직한 목표이지만 어떻게 달성할 수 있는지는 명확하지 않다.  
  
우선, image analogies 문제 진술의 중요한 측면은 필터링되지 않은 각 이미지와 각각의 필터링된 버전 간의 관계뿐만 아니라 전체적으로 보았을 때  
소스 쌍과 대상 쌍 간의 관계도 측정하는 데 사용되는 유사성의 정의이다.  
이 문제는 까다롭다.  
A에서 A'까지 원본 이미지 필터의 인식 가능한 특징을 보존할 수 있는 메트릭을 사용하는 동시에 완전히 다른 대상 이미지 B에도 적용할 수 있다.  
게다가, 학습 쌍의 어떤 특징이 필터의 "스타일"을 구성하는지 명확하지 않다:  
원칙적으로, 한 쌍의 이미지로부터 무한한 수의 다른 변환을 추론할 수 있다.  
본 논문에서, 우리는 원시 픽셀 값과 선택적으로 조정 가능한 필터 응답을 사용하여 마르코프 무작위 필드 모델의 근사치를 기반으로 하는 유사성 메트릭을 사용한다.  
소스와 대상 이미지 쌍 사이의 관계를 측정하기 위해 섹션 3에서 논의하는 것처럼 이미지 내의 작은 이웃의 공동 통계를 샘플링한다.
  
추가로, 우리는 필터링된 대상 이미지 B의 합성이 합리적인 속도로 진행되기를 바란다.  
따라서 유사성 메트릭을 사용하여 다양한 이미지 A, A' 및 B를 색인화하고 효율적으로 검색하여 B → B'를 합성하는 데 있어  
변환 A → A'의 적절한 부분을 선택할 수 있는 방법이 필요하다.  
우리는 주로 Wei와 Levoy및 Ashikh에 의한 텍스처 합성에 대한 최근 연구를 기반으로 한 autoregression 알고리즘을 사용한다.  
실제로, 우리의 접근 방식은 단일 텍스처가 아니라 해당 이미지 쌍의 상황에 대한 일반화와 함께 이 두 가지 접근 방식의 조합으로 생각할 수 있다.  
  
마지막으로 이미지 A의 통계를 완전히 다른 색상의 이미지 B에 적용하기 위해 섹션 3.3 및 3.4에 자세히 설명된 대로 사전 처리된 휘도 공간에서 작동하기도 한다.  
  
실제 사용에서는 두 단계를 포함하는 image analogies을 구상한다.  
설계(또는 학습) 단계에서 디자이너(예: 전문가)는 학습 이미지 A 및 A(예: 스캔된 이미지)를 선택하고, 원하는 경우 이미지에 주석을 달며,  
이미지 유추에서 다양한 유형의 이미지 기능이 가중되는 방식을 제어하는 매개 변수를 선택하여(직간접적으로) 필터를 생성한다.  
그런 다음 필터는 라이브러리에 따로 저장할 수 있다.  
나중에 애플리케이션 단계에서 사용자(아마도 이미지 필터를 만드는 데 전문 지식이 전혀 없는 사람)가 일부 대상 이미지 B에 필터를 적용한다.  
분명히, 우리는 우리의 이미지 아날로그 프레임워크가 특히 단일 훈련 쌍에서 가능한 모든 이미지 필터를 학습하고 시뮬레이션하는 데 있어  
완벽한 역할을 하기를 기대할 수는 없다.  
게다가, 우리가 우리의 틀이 배울 수 있기를 바라는 많은 필터들은 사실, 심지어 인간들도 숙달하기 매우 어렵다.  
그럼에도 불구하고, 우리는 섹션 4에서 설명된 바와 같이 다양한 상황에서 우리의 image analogies 프레임워크가 상당히 잘 작동한다는 것을 발견했다.  
여기에는 다음이 포함된다.  

• traditional image filters : 블러링, 엠보싱 (4.1)  
• improved texture synthesis : 과거 방식 보다 고품질의 텍스처 합성 (4.2)  
• super-resolution : 저차원 소스로부터 고차원 추론 (4.3)  
• texture transfer : 임의 소스 질감 적용 (4.4)  
• artistic filters : 다양한 drawing/painting 스타일(oil/watercolor/line art rendering) (4.5)  
• texture-by-numbers, : 단순한 페인팅 인터페이스를 사용하여 다양한 질감으로 구성된 사실적 장면 생성 (4.6)  

이 모든 경우에 다양한 효과를 생성하는 것은 주로 단지 입력으로 다른 유형의 소스 이미지 쌍을 제공하는 문제이다.  
예를 들어, 블러 필터는 이미지와 그 블러를 (A, A') 쌍으로 제공함으로써 학습된다.  
마찬가지로, 오일 페인팅 스타일은 입력 쌍으로서 이미지와 오일 페인팅된 동등한 이미지를 제공하여 학습된다.  
일반적인 텍스처 합성은 필터링되지 않은 이미지 A와 B가 null이고 분석/합성이 A와 B에서만 수행되는 image analogies의 특별한 경우로 볼 수 있다.  
예를 들어, A의 한 단색은 A의 "하늘 질감"에 해당하고, 다른 색은 "풀 질감"에 해당하며, 다른 색은 "풀 질감"에 해당한다.  
이 같은 색들을 B에 칠하면 비슷한 질감으로 사실적인 새로운 풍경 B를 만들 수 있다.  
  
마지막으로, 우리는 또한 디지털 페인팅 도구의 풋프린트 아래에 image analogies를 제공하는 데 사용할 수 있는 알고리즘의 실시간 대화형 버전을 설명한다(섹션 5).  
텍스처 바이 넘버는 이 도구에 자연스럽게 적용되지만, 어떤 유형의 이미지 유추에도 사용할 수 있다.  
  
여러 면에서 성공적이긴 하지만 이미지 쌍의 낮은 수준의 통계만 모델링하려고 하기 때문에 모든 경우에 image analogies가 작동하는 것은 아니다.  
따라서 광범위하고 일관된 브러시 스트로크와 같은 높은 수준의 특징이 항상 효과적으로 포착되거나 전달되는 것은 아니다.   
6절에서는 접근법의 한계를 논의하고 향후 연구 영역을 제시한다.  
 
## 2 Related work
## 3 Image analogies
Image analogies를 support 하기 위해 데이터구조와 알고리즘 셋을 설명한다.   
### 3.1 Definitions and data structures

이미지3개를 입력(A, A', B)으로 사용하여 하나의 출력(B')을 생성한다.  

두개의 source 이미지가 등록되는것을 가정한다. 즉, A 이미지의 p 픽셀과 그 주변의 색상은 A'의 p 픽셀과 그주변의 색상에 해당한다.  
그러무로 우리는 A와 A'의 동일 픽셀을 명시하는것으로 인덱스 p를 사용한다.  
타겟쌍 B와 B'의 픽셀은 q로 명시한다.  

이 설명을 위해 다양한 이미지에 RGB 색상뿐 아니라 luminance 및 다양한 필터 응답과 같은 추가 정보 채널도 포함되어 있다고 가정한다.  
함께 이러한 모든 채널(RGB 포함)은 각 픽셀 p에 대한 feature vector를 구성한다.  
A(p)(or A'(p))를 사용하여 픽셀 p에서 A(or A')의 완전한 feature vector를 나타내고 유사하게 B(q)(or B'(q))를 사용하여 feature를 지정한다.  
픽셀 q의 벡터 A 및 B 이미지에 사용된 feature가 A' 및 B' 이미지와 동일할 필요는 없다.  
우리가 사용하는 특정 feature는 아래 섹션 3.3에 자세히 설명되어 있다.  
(그러나 대체 또는 추가 feature를 실험하는 것은 확실히 미래 연구를 위한 풍부한 영역이다)  
앞으로 살펴보겠지만, 이러한 feature는 A'에서 B' 합성에 사용할 가장 적합한 픽셀을 선택하는 데 도움이 되도록 매칭 프로세스를 안내하는 데 사용된다.  
  
마지막으로, 우리의 알고리즘은 대상의 픽셀 q에 복사된 소스 픽셀의 위치 p를 추적해야 한다.  
따라서, 우리는 추가적인 데이터 구조 s(·)("소스"의 경우)를 저장할 것인데, 이것은 q로 색인화되고 s(q) = p 속성을 갖는다.  
  
요약하면, 우리의 알고리즘은 다음의 데이터 구조를 유지하는데, 그 중 A(p), A'(p), B(q)의 RGB 채널은 입력이고,  
B'(q)의 RGB 채널은 출력이며, s(q)뿐만 아니라 A, A', B, B'의 다른 채널은 합성 과정에서 중간 계산된 결과이다.

![image](https://user-images.githubusercontent.com/40943064/146557285-f5033286-98c0-4679-bbc1-39210b4c9f97.png)

여기서 SourcePoint와 TargetPoint는 각각 소스와 대상 쌍의 2D 픽셀 위치이다.  
우리는 실제로 알고리즘에서 이러한 5가지 수량의 멀티스케일 표현을 사용할 것이다.  
따라서 일반적으로 첨자를 사용하여 이러한 어레이를 다중 스케일 수준 l로 인덱싱한다.  
예를 들어, A(l)가 주어진 해상도에서 소스 이미지 A를 나타내면, A(l-1)는 각 차원에서  
절반의 픽셀을 가진 다음 조잡한 수준에서 해당하는 저해상도 이미지를 나타낸다.  
L을 사용하여 이미지의 최대 레벨, 즉 최고 해상도 버전의 레벨을 나타낸다.
  
### 3.2 The algorithm
이러한 표기법을 고려할 때 image analogies 알고리즘은 쉽게 설명할 수 있다.  
첫째, 초기화 단계에서 A, A' 및 B의 다중 스케일 표현은 feature vector와 일치 프로세스의 속도를 높이기 위해  
사용되는 일부 추가 인덱스(예를 들어, 아래에서 설명하는 것처럼 근사 근린 검색)와 함께 구성된다.  
그런 다음 합성은 한 번에 한 단계씩 B'의 다중 스케일 표현을 계산하면서 가장 거친 해상도에서 가장 뛰어난 해상도로 진행된다.  
각 수준에서 대상 쌍의 각 픽셀 q에 관련된 통계를 소스 쌍의 모든 픽셀 p에 대한 통계와 비교하고 "최상의" 일치를 찾는다.  
그런 다음 형상 벡터 B'l(q)를 가장 근접하게 일치하는 픽셀 p에 대한 형상 벡터 A'l(p)로 설정하고 가장 잘 일치하는 픽셀을 sl(q)로 기록한다.
  
![image](https://user-images.githubusercontent.com/40943064/146557914-1261cb5c-4f99-407a-96d6-346e210c8557.png)   
  
영상 아날로그 알고리즘의 핵심은 BESTMATCH 서브루틴이다.  
이 루틴은 부분적으로 합성된 B', 소스 정보 s, 레벨 l, 픽셀 q와 함께 완전한 이미지 A, A' 및 B를 입력한다.  
p, q 및 그 이웃의 feature vector에 따라 가장 근접한 일치 픽셀을 효율적으로 찾는다. 
그리고 인접한 합성 픽셀과의 일관성을 유지하려고 시도하는 Ashikhmin의 접근법을 기반으로 일관성 검색을 한다.  
일반적으로 후자의 접근법은 feature vector와 관련하여 근접하게 일치하는 픽셀을 반환하지 않는다.  
그러나 L2-노름은 지각적 유사성의 불완전한 측정이기 때문에 일관된 픽셀은 종종 L2 하에서 가장 잘 일치하는 픽셀보다 더 잘 보일 것이다.  
따라서 우리는 두 선택 사항을 비교할 때 인위적으로 더 크게 만들기 위해 일관성 매개 변수 ,에 따라 근사 검색 거리를 재조정한다.  
따라서 γ 값이 클수록 합성된 이미지의 정확도보다 일관성이 더 선호된다.  
일관성 항을 서로 다른 척도에서 일관되게 유지하기 위해,  
더 거친 척도의 픽셀 위치가 더 미세한 척도에서보다 더 멀리 떨어져 있기 때문에 2^(l-L)의 배수로 감쇠한다.  
(어떤 의미에서 2^(l-L) repres는 수준 l에서 텍스트온 [29]의 척도 추정치를 나타낸다.)  
일반적으로 색상 비사실적 필터에는 2 ≤ 25 25, 라인 아트 필터에는 κ = 1, 텍스처 합성에는 0.5 5 ≤ 5를 사용한다.  
  
이 알고리즘에 대한 보다 정확한 설명은 다음과 같다.
![image](https://user-images.githubusercontent.com/40943064/146558439-4bd02475-f300-47b9-82b7-f68a9c6db730.png)  
  
여기서, 우리는 Fl(p)를  
현 해상도 레벨 l과 더 coarser한 해상도수준 l-1에서 source 이미지 A와 A'의 일부 neighborhood N(p)  
안에서의 모든 feature vector에 대한 concate으로 명시한다.  
5X5 neighborhood를 fine level로 그리고 3X3 neighborhood를 coarse level로 사용했다.  
![image](https://user-images.githubusercontent.com/40943064/146559023-4f149c66-34b5-4827-86c6-4b14e570b512.png)  
유사하게, Fl(q)를  
(필터링된 대상 이미지 B'의 경우 가장 높은 해상도의 주변에는 이미 합성된 이미지의 부분만 포함하지만)
B와 B'에 대한 동일한 concat으로 명시한다.  
  
(F(·)는 우리의 표기법에서 overloaded되었다는 점에 유의; 지수 p 또는 q는 특정 F(·)가 소스/대상 이웃 피쳐 벡터인지 결정하는 데 사용된다.)  
각 케이스에서 노름 ||Fl(p)-Fl(q)||^2은 가우스 커널을 사용하여 feature vector F(p)와 F(q)에 대한 가중 거리로 계산되므로  
p와 q에서 멀리 떨어진 픽셀의 특징 벡터의 차이는 p와 q의 차이에 비해 더 작은 가중치를 갖는다.  

BESTAPPROXIMATEMATCH 절차의 경우, feature vector에 대해 동일한 표준을 사용하여  
approximate-nearest-neighbor search(ANN)과 tree structured vector quantization(TSVQ)를 모두 사용하려고 시도했다.  
우리의 경험에 따르면 ANN은 일반적으로 동일한 계산 시간에 대해 더 정확한 결과를 제공하지만 메모리를 더 많이 사용한다.  
이 백서에 표시된 모든 예에 ANN을 사용했습니다. PCA는 feature vector의 차원을 줄이는 데 사용할 수 있어 검색 속도가 상당히 빨라진다.  
일반적으로 분산의 99%를 유지하므로 차원이 약 10배 감소할 수 있다.  
그러나 PCA를 사용하면 일부 간단한 경우에 결과의 품질이 저하될 수 있다.  
Feature vector 크기가 큰 경우에 가장 유용하다(예: 조정 가능한 필터가 사용되는 경우).  
  
The BESTCOHERENCEMATCH는 s(r*) +(q − r*)를 return한다.  
이때, ![image](https://user-images.githubusercontent.com/40943064/146573109-a481f5be-e4e3-4747-a11c-e1b89022e0cd.png)  
N(q)는 Bl'에서 q에 인접한 이미 합성된 픽셀의 이웃이다.  
이 공식은 기본적으로 Ashikhmin 방법의 핵심 통찰력인 q에 인접한 Bl'의 이미 합성된 부분과 일치하는 최상의 픽셀을 반환한다.  
  
### 3.3 Features
Feature selection과 representation은 ML에서 큰 미해결 문제이며 활발한 연구 영역이다.  
현재 feature vector에 대해 몇 가지 다른 구성 요소를 실험했다.  
RGB 채널을 사용하는 것 자체가 가장 확실한 선택이며, 가장 먼저 시도했다.  
그러나 일부 필터의 경우 source 쌍에 RGB 색상을 사용하여 target 쌍과 잘 일치하는 데이터가 충분하지 않다는 것을 발견했다.  
이것은 잘 알려진 "차원의 저주" 때문이다: RGB 이미지의 주변 공간은 그레이스케일 이미지보다 훨씬 더 크므로  
단일 이미지 쌍은 그레이스케일보다 RGB에 대한 공간의 상응하는 희소 샘플을 제공한다.  
따라서 A의 주변 히스토그램이 여전히 B와 제대로 일치하지 않을 수 있지만 그레이스케일 이미지의 경우에는 문제가 되지 않는다.  
본 논문에서 제시된 많은 결과를 생성하는 데 사용한 대안은 각 픽셀에서 휘도를 계산하고 저장하여 거리 메트릭에서 RGB 대신 사용하는 것이다.  
휘도는 여러 가지 방법으로 계산할 수 있다.  
즉, YIQ 색 공간의 Y 채널을 사용한다. 여기서 I 및 Q 채널은 "색상 차이" 성분이다.  
이 접근방식은 시각 과학에 의해 동기 부여된다.  
우리는 색상 차이 채널의 변화보다 휘도 채널의 변화에 훨씬 더 민감하다.  
휘도 공간에서 처리 후 입력 B 이미지의 I, Q 채널을 합성 B 이미지로 복사한 후 RGB로 다시 변환하는 것만으로 컬러를 복구할 수 있다.  
이 접근법의 추가적인 이점은 1/3의 컬러 채널로 매칭과 합성을 수행하는 데 내재된 속도 향상이다.  
그러나 단점은 유추 필터의 색상 의존성이 사라진다는 것이다.  
본 논문에서는 블러 필터, 초해상도 필터, 예술적 필터 예시를 위한 휘도 공간에서 작업했다.  
매칭의 지각 결과를 개선하는 또 다른 방법은 방향 유도 필터의 다중 척도를 계산하는 것이다.  
이를 위해 A와 B의 휘도에 대한 조정 가능한 피라미드[45]를 계산하고 이러한 이미지에 대한 형상 벡터에 대한 필터 응답을 연결할 수 있다.  
거리 메트릭은 휘도 유사성과 필터링되지 않은 이미지 영역 사이의 방향 유사성의 가중치 조합이 된다.  
라인 아트 예제를 합성하기 위해 4개의 필터 커널로 구성된 3차 파생 스티어블 필터를 사용했다(그림 8).  
  
![image](https://user-images.githubusercontent.com/40943064/146574006-c0b3760c-68b8-436e-b0be-05d4c8a1fc2f.png)  
  
다른 실험의 경우 차이가 거의 없거나 전혀 없다는 것을 발견했다.  

## 3.4 Luminance remapping

이미지를 luminance로 변환해도 이미지 주변 히스토그램 간에 겹침이 심할 수 있다.  
예를 들어, 밝은 A는 어두운 B를 처리할 때 거의 사용되지 않는다.  
전제 조건화 단계로, 우리는 히스토그램을 대응시키는 휘도 변환을 발견하고자 한다.  
한 가지 표준 접근 방식은 히스토그램 일치이다.  
그러나 바람직하지 않은 부작용이 있는 평활하지 않은 매핑을 사용한다는 것을 발견했다.  
대신 우리의 접근 방식은 휘도 분포의 평균과 분산에 일치하는 선형 맵을 적용하는 것이다.  
보다 구체적으로, Y (p)가 이미지 A에서 픽셀의 휘도라면, 우리는 그것을 다음과 같이 다시 매핑한다.  
![image](https://user-images.githubusercontent.com/40943064/146574190-27624d50-20e7-417a-9208-d718f0a5e966.png)  
  
여기서 ΔA와 ΔB는 평균 광도이며, ΔA와 ΔB는 각각 A와 B의 휘도 분포와 관련하여 취해진 광도의 표준 편차이다.  
훈련 쌍을 일관되게 유지하기 위해 동일한 선형 변환을 A 에 적용한다.  
본 논문에서 휘도 리맵핑은 컬러 아티스틱 필터에만 사용된다(섹션 4.5).  
이와 같은 접근방식은 상당히 간단한 방법으로 색상 분포에 일치하도록 확장할 수 있다[25].  
더 나은 결과를 제공하기 위해 휘도에서 합성을 발견했기 때문에 이 논문에서 사용되지 않는다.  
