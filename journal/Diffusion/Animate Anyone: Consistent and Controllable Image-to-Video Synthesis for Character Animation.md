## Abstract
문제: 캐릭터 애니메이션에서 "캐릭터의 자세한 정보를 시간적으로 일관되게 유지하는 것"은 큰 도전과제임  
제안: 디퓨전 모델의 장점을 활용하여 캐릭터 애니메이션을 위한 새로운 프레임워크를 제안  
방법:  
1) ReferenceNet 도입: 참조 이미지의 복잡한 외모 특성의 일관성을 유지하기 위해 spatial attention으로 세부 특성들을 통합  
2) PoseGuider: 캐릭터의 움직임을 효율적으로 지시  
3) Temporal Modeling: 효과적으로 비디오 프레임 간의 부드러운 전환을 보장  

학습 데이터를 확장함으로써 다양한 캐릭터 애니메이션 SotA 달성  

## Introduction
적용: 온라인 소매업, 엔터테인먼트 비디오, 예술 창작, 가상 캐릭터  
GAN의 한계:  
GANs이 나오고 연구자들은 이미지 애니메이션과 포즈 전환 분야에 몰두했지만 local distortion, blurred details, semantic inconsistency, temporal instability 등의 한계가 있었다.  
Diffusion의 등장:  
최근 DM은 고품질 이미지와 비디오 생성에서 그 우수성을 보여주었어. 연구자들은 디퓨전 모델의 구조와 강력한 생성 능력을 활용하여 인간 이미지에서 비디오로 전환하는 작업을 탐구했다.  
1) DreamPose: SD를 확장하며 이미지에서 **CLIP과 VAE 기능을 통합하는 adapter 모듈**을 제안했으나 일관된 결과를 보장하기 위해 입력 샘플에 대한 fine-tuning이 필요하며, 이로 인해 운영 효율성이 떨어졌다.  
2) DisCo: SD를 수정하고 **CLIP을 통해 캐릭터 특성을 통합하며 ControlNet을 통해 배경 특성을 통합**하지만 캐릭터 세부 사항을 보존하는 데 있어 부족하며, 프레임 간 떨림 문제를 겪는다.  

현재 캐릭터 애니메이션 연구는 주로 특정 작업과 벤치마크에 집중하고 있어서 일반화 능력이 제한적이다. 최근 텍스트-이미지 연구의 발전을 활용하여 비디오 생성(예: 텍스트-비디오, 비디오 편집)도 시각적 품질과 다양성 측면에서 주목할 만한 발전을 이루었어. 몇몇 연구들은 이미지에서 비디오로 전환하는 방법론을 확장하고 있지만, 이미지에서 복잡한 세부 사항을 포착하는 데 부족함을 보이며, 캐릭터 애니메이션에 적용될 때 캐릭터 외모의 미세한 세부사항에서 시간적 불안정을 보인다.  

### 제안방법:  
SD 구조와 사전 학습된 weight를 상속받고, 멀티 프레임 입력을 수용할 수 있도록 노이즈 제거 UNet을 수정한다.  

#### ReferenceNet
외모의 일관성을 유지하기 위해, 참조 이미지의 공간적 세부사항을 포착하기 위해 대칭적인 UNet 구조로 특별히 설계된 ReferenceNet을 도입한다. UNet 블록의 각 해당 레이어에서, spatial attention을 사용하여 ReferenceNet의 특성을 denoising UNet에 통합한다. 이를통해 reference 이미지와의 관계를 일관된 feature space에서 종합적으로 학습할 수 있게 하며 이는 appearance 세부사항 보존의 개선에 크게 기여한다.  

#### Pose Guider
자세 제어를 보장하기 위해, 노이즈 제거 과정에 자세 제어 신호를 효율적으로 통합하는 경량의 pose guider를 사용한다.   

#### Temporal Modeling
시간적 안정성을 위해, 다중 프레임 간의 관계를 모델링하는 시간적 레이어를 도입하는데 시각적 품질에서 고해상도 세부사항을 보존하면서 연속적이고 부드러운 시간적 운동 과정을 시뮬레이션한다.  

우리의 모델은 5K 캐릭터 비디오 클립으로 구성된 내부 데이터셋에서 훈련된다. 이전 방법들과 비교할 때, 몇 가지 주목할 만한 장점이 있다.  
1) 비디오에서 캐릭터 외모의 공간적 및 시간적 일관성을 효과적으로 유지 
2) 시간적 떨림이나 깜박임과 같은 문제 없이 고화질 비디오를 생성
3) 특정 도메인에 구애받지 않고 어떤 캐릭터 이미지도 비디오로 애니메이션

대규모 데이터에서 훈련된 일반적인 이미지에서 상대적으로 우수한 능력을 보여준다. 또한 캐릭터 비디오 창작을 위한 기초적인 해결책으로서 역할을 하고 더욱 혁신적이고 창의적인 응용 분야의 발전을 영감줄 것으로 기대한다.   

## Related Works

### 2.1. Diffusion Model for Image Generation
T2I 연구에서, DM 기반은 우수한 생성 결과를 달성해서, 연구의 주류가 되었다. 복잡성을 줄이기 위해 SD은 latent space에서의 노이즈 제거를 제안하여 효과성과 효율성 사이의 균형을 맞춘다.  
ControlNet & T2I-Adapter: 추가 인코딩 레이어를 통합하여 pose, mask, edge, depth 등 다양한 조건에서 시각적 생성의 제어 가능성을 탐구한다.  
IP-Adapter: DM이 주어진 이미지 프롬프트에 의해 지정된 내용을 포함하는 이미지 결과를 생성할 수 있게 해준다.  
ObjectStitch & Paint-by-Example: CLIP을 활용해 주어진 이미지 조건에 기반한 디퓨전 기반 이미지 편집 방법을 제안한다.  
TryonDiffusion: 디퓨전 모델을 가상 의류 시도 작업에 적용하고 Parallel-UNet 구조를 도입한다.  

### 2.2. Diffusion Model for Video Generation
많은 연구들은 비디오 생성을 위해 T2I 모델의 기초 위에 프레임 간 주의 모델링을 확장하는 것을 탐구한다. 일부 작업들은 사전 훈련된 T2I 모델들을 비디오 생성기로 전환하기 위해 tempopral 레이어를 삽입한다.  
1) Video LDM: 이미지로 모델을 사전 훈련한 다음 비디오에서 temporal 레이어를 학습하는것을 제안한다.  
2) AnimateDiff: 대규모 비디오 데이터에서 훈련된 모션 모듈을 제시하는데, 이는 특정 조정 없이 대부분의 개인화된 T2I 모델에 주입한다. 우리 방식은 이러한 방법들로부터 temporal modeling에 대한 영감을 얻는다.  

일부 연구들은 T2V 기능을 이미지에서 비디오로 확장한다.  
1) VideoComposer: 조건부 제어로서 훈련 중 디퓨전 입력에 이미지를 통합한다.
2) AnimateDiff: 노이즈 제거 중 이미지 잠재 변수와 랜덤 노이즈의 가중 혼합을 수행한다.
3) VideoCrafter: CLIP에서 얻은 텍스트와 시각적 특성을 cross-attention 입력으로 통합한다.  
그러나 이러한 접근 방식들은 여전히 안정적인 인간 비디오 생성을 달성하는 데 도전을 겪고 있으며, 이미지 조건 입력을 통합하는 탐구는 계속해서 조사가 필요한 영역이다.  
