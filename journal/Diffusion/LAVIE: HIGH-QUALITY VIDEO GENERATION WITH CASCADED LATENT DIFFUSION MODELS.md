## Abstract
### 목표
T2I 모델을 basis로 이용하여 고품질 T2V 생성모델 제안  

### Problem
두가지 문제가 존재함: 1) 사실적이고 시간적으로 일관된 비디오 생성 2) 사전학습된 T2I 모델의 강력한 창조적 생성 특성 보존  

### Propose
Casecased Video latent diffusion model(T2V -> temporal interpolation -> SR)  

### Insight  
1) Rotary positional encoding과 결합된 간단한 temporal self-attention의 통합이 비디오 데이터에 내재된 temporal correlations를 적절히 포착한다는 사실을 확인.  
2) 이미지와 비디오의 공동 미세 조정 프로세스가 고품질의 창의적인 결과물을 만들어내는 데 중추적인 역할을 한다는 것을 확인  

## 1 Introduction
### Diffusion Models의 이미지 합성 분야 돌파구
DM은 이미지 합성 분야에서 주목할만한 돌파구를 이루었다. T2I 기술이 중심 무대를 차지하고 있다 이러한 성공을 바탕으로, T2V에 대한 관심이 증가하고 있으며, 영화 제작, 비디오 게임, 예술 창작과 같은 분야에서의 잠재적 응용 가능성에 의해 주도된다.  

### T2V 시스템의 도전과 대안 접근법
T2V 시스템을 처음부터 학습하는 것은 spatio-temporal joint distribution을 학습하기 위한 전체 네트워크 최적화에 막대한 계산 자원이 필요하기 때문에 상당한 도전을 안고 있다. 대안적인 접근법은 학습된 T2I 모델로부터 공간적 지식을 활용하여 동영상 데이터에 더 빨리 적응하고, 학습 과정을 가속화하여 고품질 결과를 효율적으로 달성하려는 목표를 가지고 있다. 그러나 실제로 (동영상 품질, 학습 비용, 모델 구성) 사이의 적절한 균형을 찾는 것은 여전히 어려우며, 이는 모델 아키텍처, 학습 전략 및 고품질 텍스트-비디오 데이터 세트 수집의 세심한 설계가 필요하다.  

### LaVie: 통합 동영상 생성 프레임워크
LaVie(3B 매개변수)라고 하는 통합 동영상 생성 프레임워크를 소개한다. 학습된 T2I Stable Diffusion을 기반으로 구축된 T2V 기초 모델로, 현실적이고 시간적으로 일관성 있는 동영상을 합성하는 것을 목표로 하며, 학습된 T2I 모델의 강력한 창의적 생성 특성을 보존한다.

#### Insight:  
1) 단순한 temporal self-attention과 RoPE(Rotary Positional Encoding)의 결합이 동영상 데이터에 내재된 시간적 상관 관계를 적절히 포착한다.(더 복잡한 아키텍처 설계는 생성된 결과물에 대한 시각적 개선을 미미하게만 가져온다.)
2) 공동 이미지-비디오 미세 조정은 고품질이고 창의적인 결과물을 생성하는 데 핵심적인 역할을 한다. 비디오 데이터 세트에서 직접 미세 조정을 하면 모델의 개념 혼합 능력이 심각하게 저하되어, 학습된 사전 지식이 점차 사라지는 catastrophic forgetting으로 이어진다. 또한, 공동 이미지-비디오 미세 조정은 이미지에서 동영상으로 대규모 지식 전달을 용이하게 하며, 이는 장면, 스타일, 캐릭터를 포함한다.  

## 2 Related Works

### Unconditional Video Generation  
학습 데이터 세트의 기저 분포를 포괄적으로 학습함으로써 동영상을 생성하는 것을 목표로 한다. 이전 연구들은 GANs, VAEs 및 VQ 기반 모델을 활용했다. 최근에는 DMs의 등장으로 동영상 생성 분야에서 주목할 만한 발전이 관찰되었다. 이 모델들은 이미지 합성에서 상당한 진전을 보였으며 최근 몇몇 연구 DMs를 동영상 생성에 적용하는 것을 탐구했다. 이 연구들은 이미지 기반 모델에 spatio-temporal 연산을 통합하여 복잡한 동영상 분포를 모델링하는 DMs의 유망한 능력을 보여주며, 동영상 품질 면에서 이전 접근법을 능가한다. 그러나 동영상 데이터 세트의 전체 분포를 조건 없이 학습하는 것은 여전히 매우 도전적이다. 공간적 및 시간적 내용의 결합은 어려움을 일으키며, 만족스러운 결과를 얻는 것은 여전히 어렵다.

### Text-to-video generation
텍스트-동영상 생성은 조건부 동영상 생성의 한 형태로, 텍스트 설명을 조건 입력으로 사용하여 고품질의 동영상을 합성하는 데 초점을 맞춘다. 기존 접근법들은 주로 텍스트-이미지 모델을 확장하여, 동영상 프레임 간의 시간적 상관관계를 설정하기 위해 temporal convolutions, temporal attention과 같은 시간적 모듈을 통합한다. 특히, Make-A-Video와 Imagen Video는 각각 DALL·E2와 Imagen을 기반으로 개발되었다. PYoCo는 noise prior 접근 방식을 제안하고 사전 학습된 eDiff-I를 초기화로 활용했다. 반면, 다른 연구들은 사전 학습된 모델의 접근성 때문에 Stable Diffusion을 기반으로 구축한다. 학습 전략 측면에서 하나의 접근 방법은 전체 모델을 이미지와 비디오 데이터 모두에 대해 처음부터 학습하는 것이다. 이 방법은 이미지와 비디오 분포 모두에서 학습함으로써 고품질의 결과를 얻을 수 있지만, 상당한 계산 자원을 요구하며 긴 최적화 과정이 필요하다. 또 다른 접근 방법은 사전 학습된 Stable Diffusion을 기반으로 T2V 모델을 구축하고, 이를 비디오 데이터에 대해 전체적으로 또는 부분적으로 미세 조정하는 것이다. 이러한 접근법은 대규모 사전 학습된 T2I 모델의 이점을 활용하여 수렴을 가속화하려는 목표를 가지고 있다. 그러나 우리는 비디오 데이터에만 전적으로 의존하는 것이 비디오와 이미지 데이터 세트 간의 상당한 분포 격차로 인해 만족스러운 결과를 얻지 못할 수 있으며, catastrophic forgetting과 같은 도전을 초래할 수 있다고 주장한다. 이전 연구와 대비하여, 우리의 접근 방식은 사전 학습된 SD에 효율적인 temporal module을 추가하고, 전체 모델을 이미지 및 비디오 데이터 세트 모두에 대해 공동으로 미세 조정함으로써 차별화된다.
