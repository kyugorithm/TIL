## Abstract
### 목표
Pretraiend T2I 모델을 이용하여 고품질 T2V 생성모델 제안  

### Problem
1) 사실적이고 시간적으로 일관된 비디오 생성  
2) Pre-trained T2I 모델의 강력한 창조적 생성 특성 보존  

### Propose
Casecased Video latent diffusion model(T2V - temporal interpolation - SR)  

### Insight  
1) Rotary positional encoding과 결합된 간단한 temporal self-attention의 통합이 비디오 데이터에 내재된 시간적 상관성을 적절히 포착  
2) 이미지와 비디오의 공동 미세 조정 프로세스가 고품질의 창의적인 결과물을 만들어내는 데 중요  

## 1 Introduction
### Diffusion Models의 이미지 합성 분야 돌파구
DM은 이미지 합성 분야에서 주목할만한 돌파구를 이루었다. T2I 기술이 중심 무대를 차지하고 있다 이러한 성공을 바탕으로, T2V에 대한 관심이 증가하고 있으며, 영화 제작, 비디오 게임, 예술 창작과 같은 분야에서의 잠재적 응용 가능성에 의해 주도된다.  

### T2V 시스템의 도전과 대안 접근법
T2V 시스템을 처음부터 학습하는 것은 spatio-temporal joint distribution을 학습하기 위한 전체 네트워크 최적화에 막대한 계산 자원이 필요하기 때문에 상당한 도전을 안고 있다. 대안적인 접근법은 학습된 T2I 모델로부터 공간적 지식을 활용하여 동영상 데이터에 더 빨리 적응하고, 학습 과정을 가속화하여 고품질 결과를 효율적으로 달성하려는 목표를 가지고 있다. 그러나 실제로 (동영상 품질, 학습 비용, 모델 구성) 사이의 적절한 균형을 찾는 것은 여전히 어려우며, 이는 모델 아키텍처, 학습 전략 및 고품질 텍스트-비디오 데이터 세트 수집의 세심한 설계가 필요하다.  

### LaVie: 통합 동영상 생성 프레임워크
LaVie(3B 매개변수)라고 하는 통합 동영상 생성 프레임워크를 소개한다. 학습된 T2I Stable Diffusion을 기반으로 구축된 T2V 기초 모델로, 현실적이고 시간적으로 일관성 있는 동영상을 합성하는 것을 목표로 하며, 학습된 T2I 모델의 강력한 창의적 생성 특성을 보존한다.

#### Insight:  
1) 단순한 temporal self-attention과 RoPE(Rotary Positional Encoding)의 결합이 동영상 데이터에 내재된 시간적 상관 관계를 적절히 포착한다.(더 복잡한 아키텍처 설계는 생성된 결과물에 대한 시각적 개선을 미미하게만 가져온다.)
2) 공동 이미지-비디오 미세 조정은 고품질이고 창의적인 결과물을 생성하는 데 핵심적인 역할을 한다. 비디오 데이터 세트에서 직접 미세 조정을 하면 모델의 개념 혼합 능력이 심각하게 저하되어, 학습된 사전 지식이 점차 사라지는 catastrophic forgetting으로 이어진다. 또한, 공동 이미지-비디오 미세 조정은 이미지에서 동영상으로 대규모 지식 전달을 용이하게 하며, 이는 장면, 스타일, 캐릭터를 포함한다.  

## 2 Related Works

### Unconditional Video Generation  
학습 데이터 세트의 기저 분포를 포괄적으로 학습함으로써 동영상을 생성하는 것을 목표로 한다. 이전 연구들은 GANs, VAEs 및 VQ 기반 모델을 활용했다. 최근에는 DMs의 등장으로 동영상 생성 분야에서 주목할 만한 발전이 관찰되었다. 이 모델들은 이미지 합성에서 상당한 진전을 보였으며 최근 몇몇 연구 DMs를 동영상 생성에 적용하는 것을 탐구했다. 이 연구들은 이미지 기반 모델에 spatio-temporal 연산을 통합하여 복잡한 동영상 분포를 모델링하는 DMs의 유망한 능력을 보여주며, 동영상 품질 면에서 이전 접근법을 능가한다. 그러나 동영상 데이터 세트의 전체 분포를 조건 없이 학습하는 것은 여전히 매우 도전적이다. 공간적 및 시간적 내용의 결합은 어려움을 일으키며, 만족스러운 결과를 얻는 것은 여전히 어렵다.

### Text-to-video generation
텍스트-동영상 생성은 조건부 동영상 생성의 한 형태로, 텍스트 설명을 조건 입력으로 사용하여 고품질의 동영상을 합성하는 데 초점을 맞춘다. 기존 접근법들은 주로 텍스트-이미지 모델을 확장하여, 동영상 프레임 간의 시간적 상관관계를 설정하기 위해 temporal convolutions, temporal attention과 같은 시간적 모듈을 통합한다. 특히, Make-A-Video와 Imagen Video는 각각 DALL·E2와 Imagen을 기반으로 개발되었다. PYoCo는 noise prior 접근 방식을 제안하고 pre-trained eDiff-I를 초기화로 활용했다. 반면, 다른 연구들은 pre-trained된 모델의 접근성 때문에 Stable Diffusion을 기반으로 구축한다. 학습 전략 측면에서 하나의 접근 방법은 전체 모델을 이미지와 비디오 데이터 모두에 대해 처음부터 학습하는 것이다. 이 방법은 이미지와 비디오 분포 모두에서 학습함으로써 고품질의 결과를 얻을 수 있지만, 상당한 계산 자원을 요구하며 긴 최적화 과정이 필요하다. 또 다른 접근 방법은 pre-trained Stable Diffusion을 기반으로 T2V 모델을 구축하고, 이를 비디오 데이터에 대해 전체적으로 또는 부분적으로 미세 조정하는 것이다. 이러한 접근법은 대규모 pre-trained T2I 모델의 이점을 활용하여 수렴을 가속화하려는 목표를 가지고 있다. 그러나 우리는 비디오 데이터에만 전적으로 의존하는 것이 비디오와 이미지 데이터 세트 간의 상당한 분포 격차로 인해 만족스러운 결과를 얻지 못할 수 있으며, catastrophic forgetting과 같은 도전을 초래할 수 있다고 주장한다. 이전 연구와 대비하여, 우리의 접근 방식은 pre-trained SD에 효율적인 temporal module을 추가하고, 전체 모델을 이미지 및 비디오 데이터 세트 모두에 대해 공동으로 미세 조정함으로써 차별화된다.

## 3 Preliminary of diffusion models 
Pass

## 4 Out Approach
제안된 프레임워크인 LaVie는 텍스트 설명을 기반으로 하는 V-LDMs로 구성된 연속적인 프레임워크로 세 개의 구별된 네트워크로 구성된다:  
1) 기본 T2V 모델: 짧고 저해상도의 key-frame을 생성한다.
2) Temporal Interpolation (TI): 짧은 비디오를 보간하고 fps를 증가  
3) Video Super Resolution (VSR): 저해상도 비디오에서 고화질 결과를 합성  

이 모델들은 각각 텍스트 입력을 조건 정보로 사용하여 개별적으로 학습된다. 추론 단계에서, 일련의 잠재적인 노이즈와 텍스트 프롬프트가 주어지면, LaVie는 전체 시스템을 활용하여 1280×2048 픽셀의 공간 해상도를 가진 61 프레임의 비디오를 생성한다.  

![image](https://github.com/kyugorithm/TIL/assets/40943064/031f814a-26d0-4df0-b062-a950fc39fea9)  

4.1 Base T2V Model
- 데이터 세트: T-프레임 비디오(v ∈ RT×3×H×W)는 비디오 데이터 세트(pvideo) 분포를 따르고, 이미지(x ∈ R3×H×W)는 이미지 데이터 세트(pimage) 분포를 따른다.  
- 2D UNet에서 3D 변환: 원래 LDM은 2D UNet으로 설계되어 이미지 데이터만 처리할 수 있다. 시공간 분포를 모델링하기 위해 두 가지 주요 수정을 도입:  
1) 각 2D 컨볼루션 레이어를 시간 차원을 포함하는 pseudo-3D conv. 레이어로 확장한다. 이는 입력 텐서 크기를 B×C×H×W에서 B×C×1×H×W로 변환  
2) 원래 트랜스포머 블록을 각 공간 레이어 뒤에 temporal attention 레이어가 포함된 Spatio-Temporal Transformer (ST-Transformer)로 확장  
- Rotary Positional Encoding (RoPE)의 도입: 최근 LLM에서 나온 RoPE 개념을 도입하여 temporal attention 레이어를 통합  
- 간소화된 접근법: 복잡한 시간적 모듈 설계(예: spatio-temporal attention, temporal causal attention)는 결과를 약간 향상시키지만 모델 크기와 학습 시간을 크게 증가 시키므로 간단한 네트워크를 유지하여 320×512 해상도의 16 프레임 비디오를 생성  

기본 모델의 주요 목표는 고품질의 key 프레임을 생성하면서도 다양성을 유지하고 비디오의 구성적 특성을 포착하는 것이다. 우리는 모델이 Cinematic shot of Van Gogh’s selfie”과 같은 창의적인 프롬프트와 일치하는 비디오를 합성할 수 있도록 하고자 한다. 그러나 pre-trained LDM에서 초기화하더라도 비디오 데이터 세트에서만 미세 조정을 하는 것은 몇 epoch만 학습한 후 이전 지식이 빠르게 잊혀지는 catastrophic forgetting 현상으로 인해 이 목표를 달성하지 못한다. 따라서, 이 문제를 해결하기 위해 이미지와 비디오 데이터를 모두 사용하는 공동 미세 조정 접근법을 적용한다. 실제로, 우리는 M개의 이미지를 시간 축을 따라 연결하여 T-프레임 비디오를 형성하고 전체 기본 모델을 T2I 및 T2V 작업의 목표를 최적화하도록 학습한다(그림 4 (c)에서 보여짐). 결과적으로, 우리의 학습 목표는 두 가지 구성 요소로 이루어진다: 비디오 손실 LV와 이미지 손실 LI. 전체 목표는 다음과 같이 수식화 될 수 있다.  
![image](https://github.com/kyugorithm/TIL/assets/40943064/d3bf2340-384d-46c9-9f77-07e0e622f841)  
cV와 cI는 각각 비디오와 이미지에 대한 텍스트 설명을 나타내며, α는 두 손실을 균형있게 조절하는 데 사용되는 계수다. 이미지를 미세 조정 과정에 통합함으로써 비디오 품질이 크게 향상됨을 관찰한다. 또한, 그림 2에서 보여지는 것처럼, 우리의 접근 방식은 이미지에서 비디오로 다양한 개념을 성공적으로 전달한다. 이에는 다른 스타일, 장면, 캐릭터가 포함된다. 우리 방법의 추가적인 장점은 LDM의 아키텍처를 수정하지 않고 이미지와 비디오 데이터 모두에 대해 공동으로 학습하기 때문에, 결과적으로 생성된 기본 모델은 T2I와 T2V 작업 모두를 처리할 수 있으며, 이는 우리가 제안한 설계의 일반화 가능성을 보여준다.

### 4.2 Temporal Interpolation
기본 T2V 모델을 기반으로, 우리는 생성된 비디오의 부드러움을 향상시키고 더 풍부한 시간적 세부 사항을 합성하기 위해 TI 네트워크를 도입한다. 이를 위해 기본 비디오의 프레임 속도를 4배로 늘리는 데 특별히 설계된 diffusion UNet을 학습한다. 이 네트워크는 16 프레임의 기본 비디오를 입력으로 받아 61 프레임으로 구성된 업샘플링된 출력을 생성한다. 학습 단계에서, 우리는 목표 프레임 속도에 맞추기 위해 기본 비디오 프레임을 복제하고 이를 노이즈가 있는 고속 프레임 프레임과 연결한다. 이 결합된 데이터는 diffusion UNet에 공급된다. 우리는 노이즈가 없는 고속 프레임 프레임을 재구성하는 목표를 사용하여 UNet을 학습시키며, 이를 통해 denoising 과정과 보간된 프레임을 생성하는 과정을 학습한다. 추론 시에는 기본 비디오 프레임을 무작위로 초기화된 가우스 노이즈와 연결한다. Diffusion UNet은 denoising 과정을 통해 이 노이즈를 점차 제거하여 61개의 보간된 프레임을 생성한다. 특히, 우리의 접근 방식은 전통적인 비디오 프레임 보간 방법과 다르다. 보간을 통해 생성된 각 프레임은 해당 입력 프레임을 대체한다. 다시 말해, 출력의 모든 프레임은 새롭게 합성되며, 입력 프레임이 보간 중에 변경되지 않는 기술과는 다른 접근 방식을 제공한다. 또한, 우리의 diffusion UNet은 텍스트 프롬프트에 조건을 부여하여 시간적 보간 과정에 추가적인 지침을 제공하며, 생성된 비디오의 전체 품질과 일관성을 향상시킨다.

### 4.3 Video Super Resolution
시각적 품질을 더욱 향상시키고 공간 해상도를 높이기 위해, VSR 모델을 파이프라인에 통합한다. 이를 위해 비디오 해상도를 1280×2048으로 증가시키도록 특별히 설계된 LDM 업샘플러를 학습시킨다. 4.1절에서 설명한 기본 모델과 유사하게, pre-trained diffusion 기반 이미지 ×4 업스케일러를 사전 지식으로 활용한다. 네트워크 아키텍처를 3D에서 비디오 입력을 처리하도록 적응시키기 위해, 우리는 추가적인 시간 차원을 도입하여 diffusion UNet 내에서 시간 처리를 가능하게 한다. 이 네트워크 내에서, 우리는 기존 공간 레이어와 함께 시간적 레이어, 즉 temporal attention 및 3D conv. 레이어를 도입한다. 이 시간적 레이어들은 생성된 비디오의 시간적 일관성을 향상시키는 데 기여한다. Latent space 내에서 저해상도 입력 프레임을 연결함으로써, diffusion UNet은 추가적인 텍스트 설명과 노이즈 수준을 조건으로 고려하며, 이를 통해 향상된 출력의 질감과 품질에 대해 더 유연한 제어를 가능하게 한다.  
  
사전 학습된 업스케일러의 공간 레이어는 고정된 상태로 유지되며, 우리의 초점은 V-LDM에 삽입된 시간적 레이어의 미세 조정에 있다. CNN 기반 SR 방법에 영감을 받아, 우리의 모델은 320×320 패치에서 패치 단위 학습을 거친다. 저해상도 비디오를 강력한 조건으로 활용함으로써, 우리의 업스케일러 UNet은 본질적인 conv. 특성을 효과적으로 보존한다. 이를 통해 패치에서 효율적인 학습을 진행하면서 임의 크기의 입력을 처리할 수 있는 능력을 유지한다. VSR 모델의 통합을 통해, 우리의 LaVie 프레임워크는 2K 해상도(1280×2048)의 고품질 비디오를 생성하여 최종 출력물에서 시각적 우수성과 시간적 일관성을 모두 보장한다.  

## 5 Experiments
1. Dataet 및 implementation detail을 포함하는 실험 설정을 제시  
2. Zero-shot T2V 작업에서 SOTA와 비교평가  
3. 공동 이미지-비디오 미세 조정의 효과에 대해 심층적으로 분석  
4. 우리 방법의 두 가지 응용 사례인 긴 비디오 생성과 개인화 비디오 합성  
5. 현재 접근 방식을 개선하기 위한 제한 사항과 잠재적인 해결책에 대해 논의  

### 5.1 Datasets
모델 학습 데이터: Webvid10M & Laion5B  
단, WebVid10M을 고해상도 비디오 생성에 사용하는 데 있어 비디오 해상도, 다양성, 미적 감각 측면에서 제한적이므로 T2V 품질 향상을 위해 Vimeo25M이라는 새로운 데이터 세트 생성.  
해상도와 미적 점수를 기준으로 엄격한 필터링 기준을 적용하여 2천만 개의 비디오와 4억 개의 이미지를 학습 목적으로 획득  

### Vimeo25M dataset
Vimeo25M: 고화질, 와이드스크린, 워터마크가 없는 형식의 2천5백만 개의 텍스트-비디오 쌍으로 구성. Videochat (Li 등, 2023)을 사용하여 자동으로 생성. 원래 비디오는 Vimeo에서 출처를 얻었으며 광고 및 상업, 애니메이션, 브랜드 콘텐츠, 코미디, 다큐멘터리, 실험, 음악, 서사, 스포츠, 여행 등 열 가지 카테고리로 분류된다. 예시 비디오는 그림 5에 나타나 있다.  

#### 데이터세트 획득
1. PySceneDetect를 사용하여 주요 비디오의 장면 detection 및 segment 진행
2. 자막의 품질을 보장하기 위해 세 단어 미만의 자막을 필터링하고 16 프레임 미만의 비디오 세그먼트를 제외
3. 총 2천5백만 개의 개별 비디오 세그먼트 획득.
4. Vimeo25M 데이터 세트의 통계, 비디오 카테고리 분포, 비디오 세그먼트의 지속 시간 및 자막 길이를 포함한 것은 그림 6에 나타나 있다. 데이터 세트는 다양한 범위의 카테고리를 보여주며, 대부분의 카테고리에서 상대적으로 균형 잡힌 양을 나타낸다. 또한, 데이터 세트의 대부분의 비디오는 약 10단어로 구성된 자막을 가진다.  
  
#### Vimeo25M 품질
Vimeo25M와 WebVid10M 데이터 간의 미적 점수를 비교 하면 그림 7(a)에서 보여지는 것처럼, Vimeo25M의 비디오 중 약 16.89%가 높은 미적 점수(6 이상)를 받았으며, 이는 WebVid10M의 7.22%를 능가한다. 4에서 6 사이의 점수 범위에서 Vimeo25M은 79.12%를 달성했으며, 이는 WebVid10M의 72.58%보다 우수하다. 마지막으로, 그림 7(b)는 Vimeo25M과 WebVid10M 데이터 세트 간의 공간 해상도 비교를 나타낸다. Vimeo25M 데이터 세트의 대부분의 비디오가 WebVid10M의 것보다 높은 해상도를 가지고 있음이 분명하며, 이는 생성된 결과가 향상된 품질을 보장한다.  

<img src="https://github.com/kyugorithm/TIL/assets/40943064/912891ab-f19c-4241-9970-a432e9bb2f7c" width=400>

### 5.2 Implementation Detail
1. 모델 초기화: 기본 T2V 모델의 Autoencoder와 LDM은 사전 학습된 Stable Diffusion 1.4에서 초기화된다.  
2. 비디오 전처리 및 학습: 각 비디오를 320×512 해상도로 전처리하고 비디오 클립당 16 프레임을 사용하여 학습한다. 공동 이미지-비디오 미세 조정을 위해 각 비디오에 4개의 이미지를 연결한다.  
3. 커리큘럼 학습 적용: 커리큘럼 학습을 적용하여, 초기에는 내용이 상대적으로 간단한 WebVid10M과 Laion5B를 사용하고, 점차 복잡한 Vimeo25M으로 학습 범위를 확장한다.  
4. TI: 사전 학습된 기본 T2V 모델에서 초기화되며, 아키텍처는 추가적인 conv. 레이어를 포함하여 확장된다. 학습 중에는 주로 WebVid10M을 사용하며, 후반부에는 Vimeo25M을 도입한다.  
5. VSR: 공간 레이어는 사전 학습된 이미지 ×4 업스케일러에서 초기화되며, 새로운 시간적 레이어만 학습된다. WebVid10M과 Laion5B 데이터 세트를 사용하여 공동 이미지-비디오 학습을 수행한다.  
6. 학습 및 추론 과정: 학습은 320×320 크기의 패치로 수행되며, 훈련된 모델은 추론 시 320×512 해상도의 비디오를 처리할 수 있다.  

### 5.3 Qualitative
(그림 2)동물, 영화 캐릭터, 다양한 객체 등 다양한 콘텐츠로 비디오를 합성하는 능력을 보여준다. 특히, 우리 모델은 "요다가 기타를 연주하는" 것과 같은 행동을 합성함으로써 공간적 & 시간적 개념을 결합하는 강력한 능력을 보여준다. 이러한 결과는 모델이 단순히 학습 데이터를 기억하는 것이 아니라 underlying distribution을 포착하여 서로다른 컨셉을 조합하는 방법을 배운다는 것을 나타낸다.  
![image](https://github.com/kyugorithm/TIL/assets/40943064/b4bdf82e-4b24-46ac-934b-33093e6c3d33)

(그림8) 생성결과를 세 개의 SOTA와 비교한다. LaVie는 시각적 충실도 측면에서 Make-A-Video를 능가한다. "반 고흐 스타일" 합성과 관련하여, LaVie가 다른 두 접근법보다 스타일을 더 효과적으로 포착한다는 것을 관찰한다. 이는 두 가지 요인에 기인한다:  
1) 사전 학습된 LDM에서 초기화하는 것은 시공간적 합동 분포 학습을 촉진하고,
2) 공동 이미지-비디오 미세 조정은 Video LDM에서 관찰된 대재앙적 망각을 완화하고 이미지에서 비디오로의 지식 전달을 더 효과적으로 가능하게 한다. 그러나 다른 두 접근법에 대한 테스트 코드가 사용할 수 없기 때문에 체계적이고 공정한 비교를 수행하는 것은 어렵다.  

![image](https://github.com/kyugorithm/TIL/assets/40943064/965c5abc-613a-438c-ba8b-244814caa986)

### 5.4 Quantitative
벤치마크 데이터 세트, UCF101와 MSR-VTT에서 zero-shot 정량평가를 수행하여 기존 방법과 비교한다. 그러나 고화질 비디오(예: 약 10000개)를 대규모로 샘플링하는 것은 시간이 많이 소요되기 때문에, 평가는 기본 모델에서 사용되는 비디오로 제한하여 계산 시간을 줄인다. 또한 현재 평가 지표 FVD가 생성된 비디오의 실제 품질을 완전히 포착하지 못할 수 있다는 것을 관찰했다. 따라서, 포괄적인 평가를 제공하기 위해 대규모 human evaluation를 실시한다.  

UCF101 Zero-shot 평가: FVD를 사용, 사전 학습된 I3D 모델을 백본으로 사용하는 TATS 방법을 따른다. Video LDM에서 제안된 방법론과 유사하게, 클래스 이름을 텍스트 프롬프트로 사용하고 각 클래스당 100개의 샘플을 생성하여 총 10,100개의 비디오를 생성한다. 비디오 샘플링 및 평가 중에는 각 비디오당 16 프레임을 320×512 해상도로 생성한다. 각 프레임은 중앙에서 270×270 크기의 정사각형으로 자른 다음 224×224로 크기를 조정하여 I3D 모델 입력 요구 사항에 맞춘다.

결과(Tab.1에서 제시)에 따르면, Make-A-Video를 제외한 모든 기준 방법을 능가한다. 그러나 우리는 Make-A-Video가 사용하는 WebVid10M과 HD-VILA-100M에 비해 더 작은 학습 데이터 세트(WebVid10M+Vimeo25M)를 사용한다는 점을 주목해야 한다. 또한, Make-A-Video는 각 클래스에 대해 수동으로 문장 템플릿을 디자인하는 반면, 우리는 Video LDM의 접근 방식을 따라 클래스 이름을 직접 텍스트 프롬프트로 사용한다. 동일한 실험 설정을 사용하는 방법을 고려할 때, 우리의 접근 방식은 Video LDM의 최신 결과보다 24.31만큼 우수하다는 것을 보여주며, 우리 방법의 우수성을 강조하고 zero-shot 비디오 생성을 위한 제안된 데이터 세트의 중요성을 강조한다.  

MSR-VTT에서의 Zero-shot 평가: MSR-VTT 데이터 세트에서는 공식 테스트 세트에서 비디오당 하나의 캡션을 무작위로 선택하여 총 2,990개의 비디오를 평가한다. GODIVA (Wu 등, 2021)를 따라 텍스트-비디오 의미적 유사성을 CLIPSIM 지표를 사용하여 평가한다. CLIPSIM을 계산하기 위해, 주어진 텍스트 프롬프트를 고려하여 각 프레임에 대한 클립 텍스트-이미지 유사성을 계산한 다음 평균 점수를 계산한다. 이 평가에서는 이전 연구(Blattmann 등, 2023)에서 설명된 방법론을 따라 ViT-B-32 클립 모델을 백본으로 사용하여 공정한 비교를 보장한다. 우리의 실험 설정과 세부 사항은 이전 연구와 일관되다. 결과는 LaVie가 최신 기술과 비교하여 우수하거나 경쟁력 있는 성능을 달성함을 보여주며, 제안된 학습 계획 및 Vimeo25M 데이터 세트의 활용의 효과성을 강조한다. 이러한 발견들은 우리의 접근 방식이 텍스트-비디오 의미적 유사성을 포착하는 데 효과적임을 강조한다.

인간 평가: 일반적인 비디오 품질을 평가하는 데 중점을 둔 이전 방법들과 달리, 다양한 관점에서 생성된 비디오를 포괄적으로 평가하기 위해 더 미묘한 평가가 필요하다고 주장한다. 이를 위해, 우리는 테스트 플랫폼의 접근성을 활용하여 VideoCrafter와 ModelScope와 같은 두 가지 기존 접근 방식과 비교한다. 철저한 평가를 수행하기 위해 30명의 인간 평가자를 동원하고 두 가지 평가 유형을 사용한다. 첫째, 평가자들에게 세 가지 다른 시나리오에서 비디오 쌍을 비교하도록 요청한다: 우리의 것 대 ModelScope, 우리의 것 대 VideoCrafter, ModelScope 대 VideoCrafter. 평가자들은 전체 비디오 품질을 평가하여 어떤 비디오가 더 나은 품질인지 투표하도록 지시받는다. 둘째, 평가자들에게 각 비디오를 개별적으로 평가하도록 요청하며, 이 때 움직임의 부드러움, 움직임의 합리성, 주제 일관성, 배경 일관성, 얼굴, 몸, 손의 품질과 같은 다섯 가지 사전 정의된 지표를 사용한다. 평가자들은 각 지표에 대해 "좋음", "보통", "나쁨" 중 하나의 레이블을 할당해야 한다. 모든 인간 연구는 시간 제한 없이 수행된다.

Tab. 3과 Tab. 4에서 제시된 바와 같이, 우리의 제안된 방법은 다른 두 접근 방식을 능가하여 인간 평가자들 사이에서 가장 높은 선호도를 얻는다. 그러나 "움직임의 부드러움" 측면에서 만족스러운 점수를 얻는 데 모든 세 가지 접근 방식이 어려움을 겪고 있음을 주목할 가치가 있다. 이는 일관되고 현실적인 움직임을 생성하는 것이 지속적인 도전임을 나타낸다. 또한, 고품질의 얼굴, 몸, 손 시각물을 생성하는 것은 여전히 어려운 과제이다.

### 6. Limitation

#### Multi-subject generation  
"알버트 아인슈타인이 스파이더맨과 학술 논문에 대해 토론하는" 것과 같이 두 명 이상의 주제가 포함된 장면을 생성할 때 어려움을 겪는다. 모델이 알버트 아인슈타인과 스파이더맨의 외모를 혼합하는 경향이 있는데, 이는 Stable Diffusion에서도 흔히 발견되는 문제다. 이를 개선하기 위한 잠재적 해결책으로는 현재 언어 모델인 CLIP을 T5와 같은 더 강력한 언어 이해 모델로 대체하는 것이 있다. 이러한 대체는 모델이 복잡한 언어 설명을 더 정확하게 이해하고 표현하는 능력을 향상시켜, 다중 인물 시나리오에서 주제들의 혼합을 완화할 수 있다.  

#### Hands generation  
고품질의 손을 가진 인간 몸을 생성하는 것은 여전히 어려운 과제다. 모델은 종종 올바른 수의 손가락을 정확하게 묘사하는 데 어려움을 겪어, 덜 현실적인 손 표현을 만들어낸다. 이 문제를 해결하기 위한 잠재적 해결책으로는 인간 얼굴이 포함된 비디오가 있는 더 크고 다양한 데이터 세트에서 모델을 학습시키는 것이 있다. 모델이 다양한 손 모양과 변형의 폭넓은 범위를 경험함으로써, 더 현실적이고 해부학적으로 정확한 손을 생성하는 방법을 배울 수 있다.

